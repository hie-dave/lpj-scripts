#!/usr/bin/env bash
#
# BARPA processing script for precipitation data.
#

# Exit immediately if:
# - Any command fails
# - Any part of a pipeline fails
# - We reference any unbound variables
set -euo pipefail

# If environment variable DEBUG is set to 1, run in debug mode.
if [ "${DEBUG:-}" = 1 ]
then
  echo "Running in debug mode. Set DEBUG to 0 to disable."
  set -x
fi

# Get directory containing this script.
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd -P)"

source "${DIR}/barpa_config"
source "${DIR}/barpa_helpers"

OUT_DIR="${OUT_DIR}/1deg"
mkdir -p "${OUT_DIR}"

# Grid file used with cdo remapcon for downscaling to 1-degree resolution.
# Ensure this is in the home directory; otherwise it may not be accessible from
# the compute nodes.
GRID_FILE="${DIR}/barpa_1deg.grid"

VAR="pr"

# Precipitation requires a units conversion from kg m-2 s-1 (ie millimeters per
# second) to mm. This requires us to multiply by the timestep size in seconds,
# which is a parameter defined in barpa_config.
SECONDS_PER_MINUTE=60
MINUTES_PER_HOUR=60
SECONDS_PER_HOUR=$((SECONDS_PER_MINUTE * MINUTES_PER_HOUR))
SECONDS_PER_TIMESTEP=
if [ ${FREQ} = 1hr ]
then
	SECONDS_PER_TIMESTEP=${SECONDS_PER_HOUR}
elif [ ${FREQ} = 3hr ]
then
	SECONDS_PER_TIMESTEP=$((SECONDS_PER_HOUR * 3))
elif [ ${FREQ} = 6hr ]
then
	SECONDS_PER_TIMESTEP=$((SECONDS_PER_HOUR * 6))
elif [ ${FREQ} = day ]
then
	SECONDS_PER_TIMESTEP=$((SECONDS_PER_HOUR * 24))
else
	# Monthly input not supported.
	die "Unknown/unsupported timestep frequency: ${FREQ}. Need to add number of seconds per timestep into barpa_processing_pr in order to convert from kg m-2 s-1 to mm"
fi

# VARIABLE_ID is required by the barpa_helpers functions.
VARIABLE_ID="${VAR}"

# The directory into which intermediate output files will be written.
WORK_DIR="${OUT_DIR}/working/${VAR}"
mkdir -p "${WORK_DIR}"

# Colon-separated IDs of all PBS jobs for unpacking this variable's files.
DEPS=""

# Start and end date of the entire timeseries in YYYYMM format.
START_DATE=""
END_DATE=""

# First we need to unpack all input files and perform a units conversion.
# The unpacking and units conversion of each input file will occur in its
# own separate PBS job, which will all run in parallel. There are about 55
# input files per variable, so this shouldn't be an issue (max jobs in queue
# per user is 1000).
for INPUT_FILE in $(enumerate_files)
do
	if [ ! -f "${INPUT_FILE}" ]
	then
		die "Input file does not exist: ${INPUT_FILE}"
	fi

	# File name (without path) of this input file.
	FILE_NAME="$(basename "${INPUT_FILE}")"

	# Full path to the output file corresponding to this input file.
	OUT_FILE="${WORK_DIR}/${FILE_NAME}"

	# Name of the processing job for this particular input file.
	JOB_NAME="barpa_unpack_${FILE_NAME/.nc/}"

	# Processing script for this job.
	SCRIPT_FILE="${SCRIPT_DIR}/${JOB_NAME}"

	# Log file for this job.
	LOG_FILE="${LOG_DIR}/${JOB_NAME}.log"

	# Get the start date and end date of this input file.
	START_DATE="$(min "${START_DATE}" "$(get_start_date "${FILE_NAME}")")"
	END_DATE="$(max "${END_DATE}" "$(get_end_date "${FILE_NAME}")")"

	# Generate script for this file.
	cat <<EOF > "${SCRIPT_FILE}"
#!/usr/bin/env bash
#PBS -l ncpus=1
#PBS -l walltime=02:00:00
#PBS -l mem=4GB
#PBS -q normal
#PBS -l wd
#PBS -j oe
#PBS -m a
#PBS -M ${PBS_EMAIL}
#PBS -P ${PBS_PROJECT}
#PBS -p ${PBS_PRIORITY}
#PBS -l storage=${PBS_STORAGE}
#PBS -N ${JOB_NAME}
#PBS -o ${LOG_FILE}

set -euo pipefail

module purge
module load netcdf cdo nco

IN_FILE="${INPUT_FILE}"
OUT_FILE="${OUT_FILE}"
GRID_FILE="${GRID_FILE}"

cdo -L -O -v mulc,${SECONDS_PER_TIMESTEP} -unpack -remapcon,"\${GRID_FILE}" "\${IN_FILE}" "\${OUT_FILE}"

# Alternate version without spatial downscaling:
# cdo -L -O -v mulc,${SECONDS_PER_TIMESTEP} -unpack "\${IN_FILE}" "\${OUT_FILE}"

# Convert to mm.
ncatted -O -a "units,${VAR},o,c,mm" "${OUT_FILE}" "${OUT_FILE}"

EOF
	chmod u+x "${SCRIPT_FILE}"
	JOB_ID="$(submit "${SCRIPT_FILE}")"
	if [ -z "${DEPS}" ]
	then
		DEPS="${JOB_ID}"
	else
		DEPS="${DEPS}:${JOB_ID}"
	fi
done # End of loop generating unpack jobs

# Now we generate a job which runs mergetime on all of the unpacked files.

JOB_NAME="barpa_mergetime_${VAR}"
OUT_FILE="${WORK_DIR}/$(get_barpa_file_name "${START_DATE}" "${END_DATE}")"

# Submit the mergetime job with a dependency on all previous jobs.
JOB_ID="$(generate_mergetime_script "${WORK_DIR}" "${OUT_FILE}" "${JOB_NAME}" "${DEPS}")"

# Now we generate a job which rechunks the file, reorders dimensions, etc.

# The input file for this job is the output file for the previous job.
IN_FILE="${OUT_FILE}"
OUT_FILE="${OUT_DIR}/$(basename "${IN_FILE}")"
JOB_NAME="barpa_rechunk_${VAR}"

# Submit the rechunk job with a dependency on the mergetime job.
generate_rechunk_script "${IN_FILE}" "${OUT_FILE}" "${JOB_NAME}" "${JOB_ID}"
