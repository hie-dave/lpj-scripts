#!/usr/bin/env python3
#
# This script will alter the dimension order and chunk size of a netcdf file.
#

import ozflux_common
import math, tempfile, string
import traceback

from ozflux_logging import *
from ozflux_netcdf import *
from argparse import ArgumentParser
from sys import argv
from netCDF4 import Dataset, Dimension
from typing import Callable
from ozflux_parallel import JobManager
from os import path

from mpi4py import MPI

# Tag for work transmissions used in MPI comms.
_TAG_WORK = 42

# Rank of the master node in MPI mode.
_RANK_MASTER = 0

class ReshapeOptions:
	"""
	Class for storing CLI arguments from the user.

	@param dimension_order: Desired order of dimensions in the output file.
	@param chunk_sizes: Chunk size of each dimension. Unspecified dimensions
                        will be left as-is.
    @param compression_level: Compression level in the output file.
	@param min_chunk_size: Minimum chunk size to use when copying data.
	"""
	def __init__(self, dimension_order: list[str]
	    , chunk_sizes: list[tuple[str, int]], compression_level: int
		, min_chunk_size: int):
		self.dimension_order = dimension_order
		self.chunk_sizes = chunk_sizes
		self.compression_level = compression_level
		self.min_chunk_size = min_chunk_size

class Options:
	"""
	Class for storing CLI arguments from the user.

	@param log: Log level.
	@param in_files: Input files.
	@param out_dir: Output directory.
	@param prog: True to write progress messages, false otherwise.
	@param dim_order: Desired order of dimensions in the output file.
	@param chunk_sizes: Chunk size of each dimension. Unspecified dimensions
                        be left as-is.
	@param parallel: Should files be processed in parallel?
	"""
	def __init__(self, log : LogLevel, in_files: list[str], out_dir: str
		, prog: bool, parallel: bool, reshape_options: ReshapeOptions):

		self.log_level = log
		self.in_files = in_files
		self.out_dir = out_dir
		self.report_progress = prog
		self.reshape_options = reshape_options
		self.parallel = parallel

class ReshapeFile:
	def __init__(self, in_file: str, out_file: str, opts: ReshapeOptions):
		self.in_file = in_file
		self.out_file = out_file
		self.opts = opts
	def exec(self, pcb: Callable[[float], None]):
		log_information("Processing '%s'..." % self.in_file)
		with open_netcdf(self.out_file, True) as nc_out:
			with open_netcdf(self.in_file) as nc_in:
				reshape(nc_in, nc_out, self.opts, pcb)

def parse_args(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Formatting ozflux data into a format suitable for consumption by LPJ-Guess")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default 3)", nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.INFORMATION)
	parser.add_argument("files", nargs = "+", help = "Input .nc files to be processed")
	parser.add_argument("-o", "--out-dir", help = "Directory into which output files will be saved.")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	parser.add_argument("-P", "--parallel", action = "store_true", help = "Process files in parallel")
	parser.add_argument("-c", "--chunk-size", help = "Chunk sizes for each dimension. This is optional, and if omitted, the values in the input file will be kept (not recommended!). This should be specified in the same format as for nco. E.g. lat/1,lon/1,time/365", default = "")
	parser.add_argument("-C", "--compression-level", type = int, help = "Compression level in the output file (0 = off) (default: same as input file)")
	parser.add_argument("-d", "--dimension-order", help = "Dimension names should be specified separated by commas. E.g. `-d lat,lon,time`")
	parser.add_argument("-m", "--min-chunk-size", type = int, help = "Minimum chunk size to use when copying data. 0 means use the chunk size in the netcdf file. Higher values result in better performance but higher memory usage. (default: 0)")
	parser.add_argument("--version", action = "version", version = "%(prog)s " + ozflux_common.VERSION)

	p = parser.parse_args(argv[1:])

    # lat/1,lon/1,time/365
	chunks = [x for x in str.split(p.chunk_size, ",")]
	chunks = [(dim, int(chunk)) for (dim, chunk) in [str.split(c, "/") for c in chunks]]

	# lat,lon,time
	dim_order = str.split(p.dimension_order, ",")
	opts = ReshapeOptions(dim_order, chunks, p.compression_level, p.min_chunk_size)
	return Options(p.verbosity, p.files, p.out_dir, p.show_progress, p.parallel, opts)

def get_chunk_size(nc: Dataset, dim: str, chunk_sizes: list[tuple[str, int]]) -> int:
	"""
	Get the chunk size for the specified dimension.

	@param nc: The input netcdf file. Used to ensure the chunk size doesn't exceed dimension size.
	@param dim: The dimension.
	@param chunk_sizes: List of tuples of dimension name and chunk size.
	"""
	dimension = nc.dimensions[dim]
	for (chunk_dim, chunk_size) in chunk_sizes:
		if chunk_dim == dim:
			if chunk_size > dimension.size:
				log_warning(f"Attempted to use chunk sizes {chunk_size} for dimension {dim}. This exceeds the total dimension length of {dimension.size}. The dimension length will be used as chunk size, but you should consider rethinking your chunking strategy if you see this message.")
				return dimension.size
			return chunk_size

	# I'm not entirely comfortable with warnings here, but some netcdf files are
	# setup in strange ways, so I'll leave it like this for now.

	default_chunking = nc.variables[dim].chunking()
	if default_chunking == "contiguous":
		log_warning(f"No chunk size specified for contiguous dimension {dim}. This code pathway is untested. Godspeed!")
		return None

	log_warning(f"No chunking specified for dimension {dim}. Falling back to the chunk size in the input file, which is {default_chunking[0]}")
	return default_chunking[0]

def init_outfile(nc_in: Dataset, nc_out: Dataset, reshape_options: ReshapeOptions):
	"""
	Initialise the output file by creating dimensions and variables as required.

	@param nc_out: The output file.
	@param in_file: Path to the input file.
	@param reshape_options: The parameters defining how the file will be reshaped.
	"""
	# Create dimensions in output file by copying them directly from the input
	# file.
	log_information("Creating dimensions in output file...")
	for name in nc_in.dimensions:
		dim = nc_in.dimensions[name]
		size = dim.size
		create_dim_if_not_exists(nc_out, name, size)

	# Create variables in output file, with changes based on user inputs.
	log_information("Creating variables in output file...")
	for name in nc_in.variables:
		var = nc_in.variables[name]
		dims = var.dimensions
		fmt = get_nc_datatype(var.datatype)

		# Compression level and type.
		ct = ""
		cl = 0
		if reshape_options.compression_level <= 0:
			# Use same as input file.
			filters = var.filters()
			cl = filters['complevel']
			ct = get_compression_type(filters)
		else:
			cl = reshape_options.compression_level
			ct = "zlib"

		# Dimension order.
		if len(reshape_options.dimension_order) == len(dims):
			# User-specified dimension order.
			dims = reshape_options.dimension_order

		# Chunk sizes.
		cs = None
		if len(reshape_options.chunk_sizes) > 0:
			cs = tuple([get_chunk_size(nc_in, dim, reshape_options.chunk_sizes) for dim in dims])
		else:
			# Chunking from input file.
			chunking = var.chunking()
			cs = None if chunking == "contiguous" else tuple(chunking)

		# Create the variable with these options.
		create_var_if_not_exists(nc_out, name, fmt, dims, cl, ct, cs)

		# Copy all attributes for this variable.
		copy_attributes(var, nc_out.variables[name])

	# Copy all file-level attributes from the input file to the output file.
	copy_attributes(nc_in, nc_out)

def var_weight(nc_in: Dataset, nc_out: Dataset, name: str) -> int:
	"""
	Get a weighting for the specified variable which is a proxy for the time
	complexity of copying it into the output file.

	@param nc_in: The input netcdf file.
	@param nc_out: The output netcdf file..
	@param name: Name of the variable.
	"""
	var = nc_in.variables[name]
	dim = var.dimensions
	weight = 1
	for dim in var.dimensions:
		weight *= nc_in.dimensions[dim].size
	if not consistent_dimensionality(var, nc_out.variables[name]):
		weight *= 2
	return weight
		

def reshape(nc_in: Dataset, nc_out: Dataset, opts: ReshapeOptions
	, pcb: Callable[[float], None]):
	"""
	Reshape the input file according to specified parameters.

	@param nc_in: Input netcdf file.
	@param nc_out: Output netcdf file.
	@param opts: Configuration settings which define how the file should be reshaped.
	@param pcb: Progress callback function.
	"""

	# Initialise the output file: create dimensions, variables, etc.
	init_outfile(nc_in, nc_out, opts)

	step_start = 0.0
	step_sizes = [var_weight(nc_in, nc_out, name) for name in nc_in.variables]
	step_sizes /= numpy.sum(step_sizes)

	# Copy all data, transposing as necessary.
	for (name, step_size) in zip(nc_in.variables, step_sizes):
		vpcb = lambda p: pcb(step_start + step_size * p)
		copy_variable(nc_in, nc_out, name, opts.min_chunk_size, vpcb)
		step_start += step_size

def reshape_file(in_file: str, out_dir: str, opts: ReshapeOptions, pcb: Callable[[float], None]):
	"""
	Reshape the specified file.
	"""
	out_file = os.path.join(out_dir, os.path.basename(in_file))
	if os.path.exists(out_file):
		os.remove(out_file)

	with open_netcdf(in_file) as nc_in:
		with open_netcdf(out_file, True) as nc_out:
			reshape(nc_in, nc_out, opts, pcb)

def main(opts: Options):
	"""
	Main function.

	@param opts: CLI arguments from the user.
	"""
	if not os.path.exists(opts.out_dir):
		os.makedirs(opts.out_dir)

	job_manager = JobManager()
	for in_file in opts.in_files:
		out_file = os.path.join(opts.out_dir, os.path.basename(in_file))
		job_manager.add_job(ReshapeFile(in_file, out_file, opts.reshape_options), os.path.getsize(in_file))
	if opts.parallel:
		job_manager.run_parallel()
	else:
		job_manager.run_single_threaded()

def mpi_init() -> list[str]:
	"""
	If running in MPI mode, distribute the list of input files equally across
	the MPI workers. This function will return the list of files that this node
	should process.
	"""
	# TODO: refactor this whole mess so that it's encapsulated by the JobManager
	# class of ozflux_parallel.py.
	comm = MPI.COMM_WORLD
	if comm.size <= 1:
		log_diagnostic("Running outside of MPI, or in MPI with world size of 1")
		return opts.in_files

	log_diagnostic(f"Running in MPI environment with world size of {comm.size}")

	if comm.rank == _RANK_MASTER:
		# Master node: divide work between workers.
		log_debug(f"[master]: splitting work list into {comm.size} chunks")
		work = numpy.array_split(opts.in_files, comm.size)
		log_diagnostic(f"[master] Transmitting work list to slaves...")
		for i in range(1, comm.size):
			log_debug(f"[master] Transmitting work list to node {i}")
			comm.send(work[i], i, tag = _TAG_WORK)
		log_diagnostic("[master] Work list successfully transmitted")
		log_diagnostic(f"[master] Master node has {len(work[0])} files to process")
		return work[0]
	else:
		log_diagnostic(f"[slave {comm.rank}] Waiting for work list")
		in_files = comm.recv(source = _RANK_MASTER, tag = _TAG_WORK)
		log_diagnostic(f"[slave {comm.rank}] Received work list of {len(in_files)} files")
		return in_files

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(argv)

	set_log_level(opts.log_level)
	set_show_progress(opts.report_progress)

	# TODO: this should be handled by the job manager class.
	opts.in_files = mpi_init()

	try:
		# Actual logic is in main().
		main(opts)
		print("\nAll files reshaped successfully!")
	except BaseException as error:
		# Basic error handling.
		log_error(traceback.format_exc())
		exit(1)
