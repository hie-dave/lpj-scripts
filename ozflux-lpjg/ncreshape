#!/usr/bin/env python3
#
# This script will alter the dimension order and chunk size of a netcdf file.
#

import ozflux_common
import math, tempfile, string
import traceback

from ozflux_logging import *
from ozflux_netcdf import *
from argparse import ArgumentParser
from sys import argv
from netCDF4 import Dataset, Dimension
from typing import Callable
from ozflux_parallel import JobManager
from os import path

class ReshapeOptions:
	"""
	Class for storing CLI arguments from the user.

	@param dimension_order: Desired order of dimensions in the output file.
	@param chunk_sizes: Chunk size of each dimension. Unspecified dimensions
                        will be left as-is.
    @param compression_level: Compression level in the output file.
	"""
	def __init__(self, dimension_order: list[str]
	    , chunk_sizes: list[tuple[str, int]], compression_level: int):
		self.dimension_order = dimension_order
		self.chunk_sizes = chunk_sizes
		self.compression_level = compression_level

class Options:
	"""
	Class for storing CLI arguments from the user.

	@param log: Log level.
	@param in_file: Input file.
	@param out_file: Output file.
	@param prog: True to write progress messages, false otherwise.
	@param dim_order: Desired order of dimensions in the output file.
	@param chunk_sizes: Chunk size of each dimension. Unspecified dimensions
                        be left as-is.
	"""
	def __init__(self, log : LogLevel, in_file: str, out_file: str, prog: bool,
		reshape_options: ReshapeOptions):

		self.log_level = log
		self.in_file = in_file
		self.out_file = out_file
		self.report_progress = prog
		self.reshape_options = reshape_options

def parse_args(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Formatting ozflux data into a format suitable for consumption by LPJ-Guess")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default 3)", nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.INFORMATION)
	parser.add_argument("-i", "--in-file", required = True, help = "Input file")
	parser.add_argument("-o", "--out-file", help = "Path to the output file. If this is the same as the input file, output will be written to a temporary file in the same directory, and the input file will be deleted and overwritten only if processing finishes successfully.")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	parser.add_argument("-c", "--chunk-size", help = "Chunk sizes for each dimension. This is optional, and if omitted, the values in the input file will be kept (not recommended!). This should be specified in the same format as for nco. E.g. lat/1,lon/1,time/365", default = "")
	parser.add_argument("-C", "--compression-level", type = int, help = "Compression level in the output file (0 = off) (default: same as input file)")
	parser.add_argument("-d", "--dimension-order", help = "Dimension names should be specified separated by commas. E.g. `-d lat,lon,time`")
	parser.add_argument("--version", action = "version", version = "%(prog)s " + ozflux_common.VERSION)

	p = parser.parse_args(argv[1:])

    # lat/1,lon/1,time/365
	chunks = [x for x in str.split(p.chunk_size, ",")]
	chunks = [(dim, int(chunk)) for (dim, chunk) in [str.split(c, "/") for c in chunks]]
	opts = ReshapeOptions(p.dimension_order, chunks, p.compression_level)
	return Options(p.verbosity, p.in_file, p.out_file, p.show_progress, opts)

def init_outfile(nc_out: Dataset, in_file: str
	, chunk_sizes: list[tuple[str, int]], dimension):
	"""
	Initialise the output file by creating dimensions and variables as required.

	@param nc_out: The output file.
	@param first_infile: The first input file. All other input files must have
	identical variables and dimensionality to this file.
	@param chunk_size: Minimum chunk size to be used when copying data.
	@param units: Desired output units (empty string means same as input units).
	"""
	if len(infiles) < 1:
		raise ValueError("No input files")

	first_infile = infiles[0]
	with open_netcdf(first_infile) as nc_in:
		log_information("Creating dimensions in output file...")
		for name in nc_in.dimensions:
			dim = nc_in.dimensions[name]
			size = dim.size
			if name == DIM_TIME:
				# Make time an unlimited dimension
				size = 0
			create_dim_if_not_exists(nc_out, name, size)
		log_information("Creating variables in output file...")
		for name in nc_in.variables:
			var = nc_in.variables[name]
			dims = var.dimensions
			fmt = get_nc_datatype(var.datatype)
			filters = var.filters()
			# Compression level
			cl = filters['complevel']

			# Compression type
			ct = get_compression_type(filters)

			# Chunk size
			chunking = var.chunking()
			cs = None if chunking == "contiguous" else tuple(chunking)
			create_var_if_not_exists(nc_out, name, fmt, dims, cl, ct, cs)
			var_out = nc_out.variables[name]
			for attr in var.ncattrs():
				if not attr[0] == "_":
					value = getattr(var, attr)
					if attr == ATTR_UNITS and len(var.dimensions) == 3 and units != "":
						value = units
					setattr(var_out, attr, value)
		lat = get_var_from_std_name(nc_in, STD_LAT)
		copy_1d(nc_in, nc_out, lat.name, chunk_size, lambda p: ...)

		lon = get_var_from_std_name(nc_in, STD_LON)
		copy_1d(nc_in, nc_out, lon.name, chunk_size, lambda p: ...)

def reshape(nc_in: Dataset, nc_out: Dataset, dimension_order: list[str]
	, chunk_sizes: list[tuple[str, int]], compression_level: int):
	...

def main(opts: Options):
	"""
	Main function.
	"""
	out_file = opts.out_file
	temp_file = False
	if opts.in_file == out_file:
		temp_file = True
		pfx = os.path.basename(opts.in_file)
		dir = os.path.dirname(opts.in_file)
		out_file = tempfile.mkstemp(prefix = pfx, dir = dir)
	with open_netcdf(opts.in_file) as nc_in:
		with open_netcdf(out_file, True) as nc_out:
			reshape(nc_in, nc_out, opts)
	if temp_file:
		# os.remove(opts.in_file)
		# os.rename(out_file, opts.out_file)
		...

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(argv)

	set_log_level(opts.log_level)
	set_show_progress(opts.report_progress)

	try:
		# Actual logic is in main().
		main(opts)
		print("\nFiles merged successfully!")
	except BaseException as error:
		# Basic error handling.
		log_error(traceback.format_exc())
		exit(1)
