#!/usr/bin/env python

#######################################################################
# Imports
#######################################################################

import requests, encodings, re, sys, os, typing
from argparse import ArgumentParser
from ozflux_logging import *
from ozflux_common import VERSION
from typing import Callable

#######################################################################
# Constants/Inputs
#######################################################################

CATALOG_URL_BASE = "https://dap.tern.org.au/thredds/catalog/ecosystem_process/ozflux"
DOWNLOAD_URL_BASE = "https://dap.tern.org.au/thredds/fileServer/ecosystem_process/ozflux"

# Minimum number of chunks to use when downloading. Increasing this may reduce
# throughput.
MIN_NUM_CHUNKS = 2

#######################################################################
# CLI Options
#######################################################################

class Options:
	def __init__(self, verbosity: LogLevel, prog: bool, op: str, annual: bool, daily: bool):
		"""
		Create a new Options instance.

		@param verbosity: Logging verbosity.
		@param prog: True to write progress messages, false otherwise.
		@param op: Output path.
		@param annual: True to download annual data, false otherwise.
		@param daily: True to download daily data, false otherwise.
		"""
		self.verbosity = verbosity
		self.show_progress = prog
		self.out_dir = op
		self.annual = annual
		self.daily = daily

def parse_cli(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Extract ozflux site metadata")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default %d)" % LogLevel.INFORMATION, nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.INFORMATION)
	parser.add_argument("-o", "--out-path", required = True, help = "Output directory")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")

	g = parser.add_mutually_exclusive_group()
	g.add_argument("-a", "--annual", action = "store_true", help = "Download annual flux data (if not enabled will download smallest timestep available)")
	g.add_argument("-d", "--daily", action = "store_true", help = "Download daily flux data (if not set, will download smallest timestep available)")

	parser.add_argument("--version", action = "version", version = "%(prog)s " + VERSION)

	parsed = parser.parse_args(argv[1:])
	return Options(parsed.verbosity, parsed.show_progress, parsed.out_path
		, parsed.annual, parsed.daily)

#######################################################################
# Main Program
#######################################################################

def request_string(url: str) -> str:
	"""
	Make a request to the given URL and return the body of the response
	decoded as utf8.
	"""
	resp = requests.get(url).content
	return encodings.utf_8.decode(resp)[0]

def get_page_list_contents(url: str) -> list[str]:
	"""
	Get the contents of the main directory displayed on the page.
	"""
	body = request_string(url)
	pattern = r"<a href=['\"]([^\/]+)\/catalog.html['\"]><code>\1<\/code><\/a>"
	return re.findall(pattern, body)

def get_sites() -> list[str]:
	"""
	Return names of all sites.
	"""
	url = "%s/catalog.html" % CATALOG_URL_BASE
	return get_page_list_contents(url)

def get_versions(site: str) -> list[str]:
	url = "%s/%s/catalog.html" % (CATALOG_URL_BASE, site)
	return get_page_list_contents(url)

def get_latest_version(site: str) -> str:
	"""
	Get the name of the latest available version of data for the
	specified site.
	"""
	versions = get_versions(site)
	if len(versions) < 1:
		raise ValueError("No versions of site '%s' are available" % site)
	return versions[len(versions) - 1]

def get_latest_processing_level(site: str, version: str) -> str:
	"""
	Return latest processing level for the specified version of a site.
	Throw if none found.
	"""
	url = "%s/%s/%s/catalog.html" % (CATALOG_URL_BASE, site, version)
	elems = get_page_list_contents(url)
	if len(elems) < 1:
		raise ValueError("No processing levels defined for site %s version %s" % (site, version))
	last = elems[len(elems) - 1]
	if last.lower() == "plots" and len(elems) > 1:
		last = elems[len(elems) - 2]
	return last

def get_latest_version_and_level(site: str) -> tuple[str, str]:
	"""
	Get the latest available processing level and version for a site.

	Return a tuple of (version, level).
	"""
	versions = get_versions(site)
	for i in range(len(versions)):
		version = versions[len(versions) - 1 - i]
		try:
			level = get_latest_processing_level(site, version)
			return (version, level)
		except:
			# No processing level, and therefore no data, found.
			if (i == len(versions) - 1):
				return (None, None)
			# Otherwise, ignore error and try next version
			pass

def get_file_suffix(annual: bool, daily: bool):
	if annual and daily:
		raise ValueError(f"Cannot set annual and daily both to true")
	if annual:
		return "_Annual"
	if daily:
		return "_Daily"
	return ""

def get_available_files(site: str, version: str, level: str, annual: bool
						, daily: bool) -> list[str]:
	"""
	Get a list of all files available for the specified site, version,
	and processing level. Returns a list of URLs.
	"""
	download_page_url = "%s/%s/%s/%s/default/catalog.html" % (CATALOG_URL_BASE, site, version, level)
	download_page = request_string(download_page_url)

	sfx = get_file_suffix(annual, daily)
	pattern = r"<a href=['\"][^'\"]+(%s[A-Za-z0-9]*_L\d+_\d+_\d+%s\.nc)" % (site, sfx)
	files = re.findall(pattern, download_page)
	if len(files) == 0:
		# Site name may not be in download name. E.g. site "CalperumChowilla" but filenames are "Calperum_L6_*"
		pattern = r"<a href=['\"][^'\"]+([A-Za-z0-9]+_L\d+_\d+_\d+%s\.nc)" % "_Annual" if annual else ""
		files = re.findall(pattern, download_page)
	return ["%s/%s/%s/%s/default/%s" % \
		(DOWNLOAD_URL_BASE, site, version, level, file) \
		for file in files]

def get_download_url(site: str, annual: bool, daily: bool) -> str:
	"""
	Get the download URL to the data file for the latest version of the
	data available for the specified site.
	"""
	(version, level) = get_latest_version_and_level(site)
	files = get_available_files(site, version, level, annual, daily)

	# Length should be 1.
	if len(files) < 1:
		return None

	return files[-1]

def download(site: str, out_dir: str, annual: bool, daily: bool
			 , pcb: Callable[[float],None]):
	"""
	Download latest version of the data for the specified site into the
	specified directory.

	@param site: The site to download.
	@param out_dir: Output directory.
	@param annual: True to download annual data. False to download smallest available timestep.
	@param pcb: Progress callback function.
	"""
	# Get URL of the file to be downloaded for this site.
	url = get_download_url(site, annual, daily)
	if url is None:
		log_warning("Skipping %s as there is no available download" % site)
		return

	# Determine output file name - use URL file name.
	file_name = url.split("/")[-1]
	if (file_name == ""):
		log_error(f"Unable to determine file name for site {site}")
	out_file = "%s/%s" % (out_dir, file_name)

	# Don't download it if it already exists.
	# TODO: verify checksum? To replace partially-downloaded files.
	if os.path.exists(out_file):
		log_information("Skipping %s as it already exists" % site)
		return

	log_information("Downloading %s..." % site)
	pcb(0)

	resp = requests.get(url, stream = True)

	# Get file size.
	total = int(resp.headers.get("Content-Length"))

	# Determine chunk size to be used for the download.
	chunk_size = min(8192, total // MIN_NUM_CHUNKS)

	# Download the file.
	with open(out_file, "wb") as file:
		downloaded = 0
		for data in resp.iter_content(chunk_size = chunk_size):
				downloaded += len(data)
				file.write(data)
				progress = 1.0 * downloaded / total
				pcb(progress)

def main(out_dir: str, annual: bool, daily: bool):
	"""
	Main CLI entrypoint method. Discover and download all available sites.

	@param out_dir: Output path to which files will be downloaded.
	@param annual: True to download annual data, false to download smallest available timestep.
	"""
	# Discover all sites available on the DAP.
	sites = get_sites()

	if len(sites) == 0:
		log_warning(f"No sites discovered. Has the THREDDS server moved?")
		return

	# Create output directory if it doesn't already exist.
	if not os.path.exists(out_dir):
		os.makedirs(out_dir)

	# Download the files.
	for i in range(len(sites)):
		download(sites[i], out_dir, annual, daily
		   , lambda p: log_progress( (i + p) / len(sites)))

if __name__ == "__main__":
	# Parse CLI arguments.
	opts = parse_cli(sys.argv)

	# Initialise logging environment.
	set_log_level(opts.verbosity)
	set_show_progress(opts.show_progress)

	# Actual download logic.
	main(opts.out_dir, opts.annual, opts.daily)
