#!/usr/bin/env python
from argparse import ArgumentParser
import datetime, numpy, os, ozflux_common, re, threading, time
import multiprocessing, multiprocessing.connection
import traceback, sys
from ozflux_logging import *
from netCDF4 import Dataset, Variable
from enum import IntEnum
from ozflux_common import LsmVariables
from sys import argv
from typing import Callable
from ozflux_netcdf import *

#######################################################################
# Constants related to .nc format/processing
#######################################################################

def lsm_var(forcing: LsmVariables, out_name: str, out_units: str) -> ForcingVariable:
	return ForcingVariable(ozflux_var_names[forcing]
		, out_name
		, out_units
		, indata_aggregators[forcing]
		, forcing_bounds[forcing][0]
		, forcing_bounds[forcing][1])

# Name of the single variable created in the output file.
OUT_VARIABLE_LSM = "forcing_data"

# Data format in output file.
FORMAT_FLOAT = "f8"

# Name of the forcings dimension in the output file.
OUT_DIM_NAME_FORCINGS = "forcings"

# Name of the mean temperature variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_TEMP = "tav"

# Name of the insol variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_INSOL = "insol"

# Name of the prec variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_PREC = "prec"

# Name of the co2 variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_CO2 = "co2"

# Name of the VPD variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_VPD = "vpd"

# Name of the tmax variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_TMAX = "tmax"

# Name of the tmin variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_TMIN = "tmin"

# Name of the pressure variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_PS = "ps"

# Atmospheric pressure lower bound (Pa). todo: is this too conservative?
LBOUND_PS = 50000

# Atmospheric pressure upper bound (Pa). todo: is this too conservative?
UBOUND_PS = 150000

# Variable names for the output file.
lsm_var_names_out = {
	LsmVariables.SWDOWN: "swdown",
	LsmVariables.PARDF: "pardf",
	LsmVariables.PARDR: "pardr",
	LsmVariables.LWDOWN: "lwdown",
	LsmVariables.PRECLS: "precls",
	LsmVariables.PRECCO: "precco",
	LsmVariables.TAIR: "tair",
	LsmVariables.UAIR: "uair",
	LsmVariables.VAIR: "vair",
	LsmVariables.QAIR: "qair",
	LsmVariables.PSURF: "psurf"
}

# Variable names for the ozflux input file.
ozflux_var_names = {
	LsmVariables.SWDOWN: "Fsd",
	LsmVariables.PARDF: "zero",
	LsmVariables.PARDR: "zero",
	LsmVariables.LWDOWN: "Fld",
	LsmVariables.PRECLS: "Precip",
	LsmVariables.PRECCO: "zero",
	LsmVariables.TAIR: "Ta",
	LsmVariables.UAIR: "Ws",
	LsmVariables.VAIR: "zero",
	LsmVariables.QAIR: "SH",
	LsmVariables.PSURF: "ps"
}

# From DMB:
# > Radiation (SWDOWN, PARDF, PARDR, LWDOWN)-> W/m^2
# > Precipitation (PRECLS, PRECCO) -> kg/m^2/s
# > Air temperature (TAIR) -> K
# > Wind speed (UAIR, VAIR) -> m/s (u = eastward, v = northward)
# > Air specific humidity (QAIR) -> kg/kg (unitless)
# > Pressure (PSURF) -> Pa
forcing_units = {
	LsmVariables.SWDOWN: "W/m2",
	LsmVariables.PARDF: "W/m2",
	LsmVariables.PARDR: "W/m2",
	LsmVariables.LWDOWN: "W/m2",
	LsmVariables.PRECLS: "kg/m2/s",
	LsmVariables.PRECCO: "kg/m2/s",
	LsmVariables.TAIR: "K",
	LsmVariables.UAIR: "m/s",
	LsmVariables.VAIR: "m/s",
	LsmVariables.QAIR: "",
	LsmVariables.PSURF: "Pa"
}

# Min and max values for each forcing variable, in output units.
forcing_bounds = {
	LsmVariables.SWDOWN: (0, 1e5),
	LsmVariables.PARDF: (0, 1e5),
	LsmVariables.PARDR: (0, 1e5),
	LsmVariables.LWDOWN: (0, 1e5),
	LsmVariables.PRECLS: (0, 1e4),
	LsmVariables.PRECCO: (0, 1e3), # Always zero, not used.
	LsmVariables.TAIR: (200, 373), # Kelvins
	LsmVariables.UAIR: (0, 100),
	LsmVariables.VAIR: (0, 1), # Always zero, not used.
	LsmVariables.QAIR: (0, 0.2),
	LsmVariables.PSURF: (3e4, 1.5e5)
}

# These are the functions used to aggregate data temporally, when converting
# from one timestep to another.
indata_aggregators = {
	LsmVariables.SWDOWN: numpy.mean,
	LsmVariables.PARDF: numpy.mean,
	LsmVariables.PARDR: numpy.mean,
	LsmVariables.LWDOWN: numpy.mean,
	LsmVariables.PRECLS: numpy.sum, # precip in mm
	LsmVariables.PRECCO: numpy.sum, # precip in mm
	LsmVariables.TAIR: numpy.mean,
	LsmVariables.UAIR: numpy.mean,
	LsmVariables.VAIR: numpy.mean,
	LsmVariables.QAIR: numpy.mean,
	LsmVariables.PSURF: numpy.mean,
}

#######################################################################
# End of .nc constants
#######################################################################

class Options:
	"""
	Class for storing CLI arguments from the user.

	@param log: Log level.
	@param files: Input files.
	@param odir: Output directory.
	@param prog: True to write progress messages, 0 otherwise.
	@param parallel: True to process files in parallel.
	@param timestep: Desired output timestep, in minutes.
	@param compression_level: Compression quality for output file [0, 9]. 0 = none, 1 = fastest compression, largest filesize, 9 = slowest compression, smallest filesize.
	@param compression_type: Compression algorithm to be used (default 'zlib').
	"""
	def __init__(self, log : LogLevel, files: list[str], odir: str, \
		prog: bool, parallel: bool, timestep: int, out_type: OutputType,
		compression_level: int, compression_type: str):
		self.log_level = log
		self.files = files
		self.out_dir = odir
		self.report_progress = prog
		self.parallel = parallel
		self.timestep = timestep
		self.compression_level = compression_level
		self.compression_type = compression_type
		self.out_type = OutputType(out_type)

def six_digit_string(x: float) -> str:
	"""
	Return a number, encoded as a 6 digit string.
	"""
	# Round x to 2 digits.
	res = str(abs(int(round(x, 2) * 1000)))
	pad = "0" * (6 - len(res))
	return "%s%s" % (pad, res)

def get_output_filename(infile: str) -> str:
	"""
	Get the expected output file name for the given input .nc file.
	"""
	with Dataset(infile, "r", format=ozflux_common.NC_FORMAT) as nc:
		# Read latitude/longitude from input .nc file.
		lon = float(nc.longitude)
		lat = float(nc.latitude)

		return ozflux_common.get_met_filename(lon, lat)

def write_data_lsm(variable: Variable, data: list[float], forcing: LsmVariables, \
	progress_callback: Callable[[float], None]):
	"""
	Write data to the output file (LSM mode).

	@param variable: Variable in the output file, to which data will be written.
	@param data: A column of data to be written.
	@param forcing: Column of the variable to which the data will be written.
	@progress_callback: Called to report progress.
	"""
	n = len(data)

	# Always use at least this many chunks.
	MIN_NUM_CHUNKS = 10

	chunk_size = min(CHUNK_SIZE, n / MIN_NUM_CHUNKS)
	for i in range(0, n, chunk_size):
		row = i
		upper = min(n, i + chunk_size)
		col = forcing
		variable[row:upper, col] = data[i:upper]
		progress_callback(i / n)

	log_debug("Successfully wrote %d items of %s" % (n, forcing))

def write_data_dg(nc_out: Dataset, var_name: str, data: list[float], lon: float
	, lat: float, progress_cb: Callable[[float], None]):
	"""
	Write data to the output file (dailygrass mode).

	@param variable: Variable in the output file to which data will be written.
	@param data: A timeseries of data for a variable for a gridcell.
	@param lon: Longitude of the data.
	@param lat: Latitude of the data.
	@param progress_cb: Called to report progress.
	"""
	n = len(data)
	chunk_size = get_steps_per_year(opts.timestep)
	for i in range(0, n, chunk_size):
		upper = min(n, i + chunk_size)
		(ilon, ilat) = get_coord_indices(nc_out, lon, lat)
		# Dimension order is time,lat,lon
		nc_out.variables[var_name][ilat, ilon, i:upper] = data[i:upper]
		progress_cb(upper / n)

def change_start_minute(datetime: str, minute: int) -> str:
	"""
	Change the minute value of a date/time string.
	"""
	if minute < 0 or minute > 59:
		m = "Unable to set start minute: minute value %d must be in range [0, 59]"
		raise ValueError(m % minute)

	pattern = r"(\d{4}-\d{1,2}-\d{1,2} \d{1,2}:)\d{1,2}(:\d{1,2})"
	matches = re.findall(pattern, datetime)
	if len(matches) < 2:
		m = "Invalid datetime format; expected yyyy-MM-dd hh:mm:ss format, but was '%s'"
		raise ValueError(m % datetime)
	return "%s%d%s" % (matches[0], minute, matches[1])

def copy_data_lsm(in_file: Dataset, variable: Variable, timestep: int, \
	progress_callback: Callable[[float], None]):
	"""
	Copy all data from the input file to the output variable.

	@param input file: Opened .nc file.
	@param variable: The newly-created and initialised forcing variable.
	@param timestep: Desired output timestep in minutes.
	@param progress_callback: Progress reporting function.
	"""
	# 0. Get variable IDs.
	i = 0
	for forcing in Forcing:
		if forcing == LsmVariables.NFORCINGS:
			break
		out_name = lsm_var_names_out[forcing]
		log_diagnostic("----- %s -----" % out_name)
		log_diagnostic("Reading %s data" % out_name)
		read_proportion = 0.66
		write_proportion = 1 - read_proportion

		var = lsm_var(forcing, out_name, forcing_units[forcing])
		
		data = get_data(in_file, var, timestep, lambda p: \
			progress_callback((i + p * read_proportion) / LsmVariables.NFORCINGS))

		# 3. Write data to out file.
		log_diagnostic("Writing %s data" % out_name)
		write_data_lsm(variable, data, forcing, lambda p: progress_callback( \
			(i + read_proportion + write_proportion * p) / LsmVariables.NFORCINGS))
		i += 1

def copy_data_dailygrass(nc_in: Dataset, nc_out: Dataset, timestep: int
	, progress_cb: Callable[[float], None]):
	"""
	Copy data to the output file in dailygrass format.

	@param nc_in: Input NetCDF file.
	@param nc_out: Output NetCDF file.
	@param timestep: Output timestep length in minutes.
	@param progress_cb: Progress callback function.
	"""
	i = 0

	# Output units for temperature variables.
	temp_out_units = "degC"

	# Lower and upper bounds for temperature variables.
	temp_min_degc = -100
	temp_max_degc = 100

	# Variables to be added to output file.
	variables = [
	ForcingVariable(ozflux_var_names[LsmVariables.TAIR]
		, DG_OUT_VARIABLE_NAME_TEMP, temp_out_units, numpy.mean, temp_min_degc
		, temp_max_degc)
	, lsm_var(LsmVariables.SWDOWN, DG_OUT_VARIABLE_NAME_INSOL, "W/m2")
	, lsm_var(LsmVariables.PRECLS, DG_OUT_VARIABLE_NAME_PREC, "mm")
	, ForcingVariable("CO2", DG_OUT_VARIABLE_NAME_CO2, "ppm", numpy.mean, 250
		, 900)
	, ForcingVariable("VPD", DG_OUT_VARIABLE_NAME_VPD, "kPa", numpy.mean, 0, 100)
	, ForcingVariable("ps", DG_OUT_VARIABLE_NAME_PS, "Pa", numpy.mean, LBOUND_PS
		, UBOUND_PS)
	]

	step_width = opts.timestep / MINUTES_PER_HOUR

	# If generating a daily file, need to include tmax/tmin variables in output.
	if step_width == 24:
		name_tav = ozflux_var_names[LsmVariables.TAIR]
		variables.append(ForcingVariable(name_tav, DG_OUT_VARIABLE_NAME_TMAX, temp_out_units
			, numpy.amax, temp_min_degc, temp_max_degc))
		variables.append(ForcingVariable(name_tav, DG_OUT_VARIABLE_NAME_TMIN, temp_out_units
			, numpy.amin, temp_min_degc, temp_max_degc))

	# 95% of time to read
	global global_read_prop, global_write_prop

	lon = float(nc_in.longitude)
	lat = float(nc_in.latitude)

	n = 0
	for var in variables:
		if opts.parallel:
			global cd_tp_lock
			cd_tp_lock.acquire()
		read_prop = global_read_prop
		write_prop = global_write_prop
		if opts.parallel:
			cd_tp_lock.release()

		log_diagnostic("----- %s -----" % var.in_name)
		log_diagnostic("Reading %s data")

		read_start = time.time()
		data = get_data(nc_in, var, timestep
		, lambda p: progress_cb( (i + p * read_prop) / len(variables)))
		read_time = time.time() - read_start

		n = max(n, len(data))

		log_diagnostic("Writing %s data")
		write_start = time.time()
		write_data_dg(nc_out, var.out_name, data, lon, lat, lambda p: \
		progress_cb( (i + read_prop + write_prop * p) / len(variables)))
		write_time = time.time() - write_start

		# Bookkeeping.
		total_time = read_time + write_time
		if opts.parallel:
			cd_tp_lock.acquire()
		global_read_prop = read_time / total_time
		global_write_prop = write_time / total_time
		log_debug("Read: %.2f%%, Write: = %.2f%%" % \
			(global_read_prop, global_write_prop))
		if opts.parallel:
			cd_tp_lock.release()

		# Copy a few attributes.f
		in_var = nc_in.variables[var.in_name]
		out_var = nc_out.variables[var.out_name]

		out_var.units = var.out_units
		if 'standard_name' in in_var.ncattrs():
			out_var.standard_name = in_var.standard_name
		else:
			out_var.standard_name = var.in_name
		out_var.long_name = in_var.long_name

		i += 1

	# Copy data into time variable. This is # of hours since start date.
	# So it starts at 0 and increments by timestep width (in hours).
	# Write data in chunks to speed things up a bit.
	timesteps = [i * step_width for i in range(n)]
	for i in range(0, n, CHUNK_SIZE):
		up = min(n, i + CHUNK_SIZE)
		nc_out.variables[OUT_DIM_NAME_TIME][i:up] = timesteps[i:up]

def create_dailygrass_variables(nc: Dataset):
	"""
	Create the variables required for dailygrass mode.

	@param nc: The output netcdf file.
	"""
	dims = (OUT_DIM_NAME_LAT, OUT_DIM_NAME_LON, OUT_DIM_NAME_TIME)
	chunksizes = (1, 1, get_steps_per_year(opts.timestep))
	format = FORMAT_FLOAT

	create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_TEMP, format, dims, chunksizes)
	create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_INSOL, format, dims, chunksizes)
	create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_PREC, format, dims, chunksizes)
	create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_CO2, format, dims, chunksizes)
	create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_VPD, format, dims, chunksizes)
	create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_PS, format, dims, chunksizes)

	# When generating daily outputs, we need to include tmax/tmin.
	if opts.timestep == 1440:
		create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_TMAX, format, dims, chunksizes)
		create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_TMIN, format, dims, chunksizes)

def create_dim_if_not_exists(nc: Dataset, name: str, size: int = 0):
	"""
	Create an unlimited dimension in the NetCDF file if it does not already
	exist.

	@param nc: The NetCDF file.
	@param name: Name of the dimension.
	"""
	if not name in nc.dimensions:
		nc.createDimension(name, size)

def create_var_if_not_exists(nc: Dataset, name: str, format: str
	, dims: tuple[str], chunksizes: tuple[int] = None):
	"""
	Create a variable in the NetCDF file if it does not already exist.

	@param nc: The NetCDF file.
	@param name: Name of the variable.
	@param format: Format of the variable.
	@param dims: Dimensions of the variable.
	"""
	compression = None if opts.compression_level == 0 else opts.compression_type
	log_diagnostic("Compression = %s L%d"%(compression, opts.compression_level))
	if not name in nc.variables:
		nc.createVariable(name, format, dims, compression = compression
		, complevel = opts.compression_level
		, chunksizes = chunksizes)

def prepare_outfile(nc: Dataset, out_type: OutputType):
	"""
	Create the variables in the output NetCDF file. The variables created
	will depend to some degree on the output type.
	"""
	# 0. Create dimensions in output file.
	log_debug("Creating output dimensions")
	if (out_type == OutputType.LSM):
		# LSM mode needs a forcings dimension.
		nc.createDimension(OUT_DIM_NAME_FORCINGS, LsmVariables.NFORCINGS)
	else:
		# Dailygrass mode needs lon/lat dimensions (as it's gridded).
		# Chunk size for geographic (ie lat/lon) variables.
		geo_chunk_size = (1,)
		# todo: writing all data into single file? If so, will need to set
		# dim_size to len(files).
		dim_size = 0 if UNLIMITED_DIMS else 1 #len(opts.files)
		create_dim_if_not_exists(nc, OUT_DIM_NAME_LON, dim_size)
		create_dim_if_not_exists(nc, OUT_DIM_NAME_LAT, dim_size)
		create_var_if_not_exists(nc, OUT_DIM_NAME_LON
			, FORMAT_FLOAT, (OUT_DIM_NAME_LON), geo_chunk_size)
		create_var_if_not_exists(nc, OUT_DIM_NAME_LAT
			, FORMAT_FLOAT, (OUT_DIM_NAME_LAT), geo_chunk_size)
		var_lon = nc.variables[OUT_DIM_NAME_LON]
		var_lat = nc.variables[OUT_DIM_NAME_LAT]
		var_lon.long_name = "longitude"
		var_lat.long_name = "latitude"
		var_lon.standard_name = "longitude"
		var_lat.standard_name = "latitude"
		var_lon.units = "degree_east"
		var_lat.units = "degree_north"

	# Time dimension is needed in both LSM and daily_grass mode.
	create_dim_if_not_exists(nc, OUT_DIM_NAME_TIME)

	if out_type == OutputType.DAILY_GRASS:
		create_var_if_not_exists(nc, OUT_DIM_NAME_TIME, FORMAT_FLOAT
			, (OUT_DIM_NAME_TIME), (get_steps_per_year(opts.timestep),))

	# 1. Create variables in output file.
	log_debug("Creating output variables")
	if (out_type == OutputType.LSM):
		# LSM mode only requires a single variable.
		nc.createVariable(OUT_VARIABLE_LSM \
		, FORMAT_FLOAT \
		, (OUT_DIM_NAME_TIME,OUT_DIM_NAME_FORCINGS), compression = 'zlib')
	else:
		# I've created a separate function for the dailygrass variables to
		# improve readability in this function.
		create_dailygrass_variables(nc)

def process_file(in_file: str, out_file: str, timestep: int \
	, out_type: OutputType, progress_callback: Callable[[float], None]):
	"""
	Read the input file and generate an output file at the specified
	path in LPJ-Guess (lsminput) format.

	@param in_file: Input file path
	@param out_file: Output file path
	@param timestep: Output timestep length in minutes.
	"""
	log_information("Processing %s..." % in_file)
	log_debug("Opening output file %s for writing" % out_file)
	with Dataset(out_file, "r+", format=ozflux_common.NC_FORMAT) as nc_out:
		log_debug("Opening input file %s for reading" % in_file)
		with Dataset(in_file, "r", format=ozflux_common.NC_FORMAT) as nc_in:
			# Quick sanity check of time step.
			instep = int(nc_in.time_step)
			if instep > timestep:
				m = "Invalid input timestep (%d). Must be <= output (%d)"
				raise ValueError(m % instep, timestep)
			if timestep % instep != 0:
				m = "Invalid input timestep: %d. Must be an integer multiple of output timestep (%d)"
				raise ValueError(m % (instep, timestep))

			prepare_outfile(nc_out, out_type)

			# 2. Copy data into this variable.
			log_debug("Migrating data")
			if out_type == OutputType.LSM:
				copy_data_lsm(nc_in, nc_out.variables[OUT_VARIABLE_LSM]
					, timestep, progress_callback)
			else:
				copy_data_dailygrass(nc_in, nc_out, timestep, progress_callback)

			# 3. Metadata for output file.
			nc_out.time_step = str(timestep) # in minutes.
			start_date = get_next_year(
				ozflux_common.parse_date(nc_in.time_coverage_start))
			nc_out.time_coverage_start = start_date.strftime(
				ozflux_common.DATE_FORMAT)
			nc_out.time_coverage_end = nc_in.time_coverage_end
			nc_out.latitude = nc_in.latitude
			nc_out.longitude = nc_in.longitude
			site_name = os.path.basename(in_file)
			site_name = os.path.splitext(site_name)[0]
			nc_out.site_name = site_name
			if opts.out_type == OutputType.DAILY_GRASS:
				var_time = nc_out.variables[OUT_DIM_NAME_TIME]
				var_time.calendar = "gregorian"
				var_time.long_name = OUT_DIM_NAME_TIME
				var_time.standard_name = OUT_DIM_NAME_TIME
				var_time.units = "hours since %s" % nc_out.time_coverage_start
			else:
				for forcing in LsmVariables:
					if forcing == LsmVariables.NFORCINGS:
						break
					oname = lsm_var_names_out[forcing]
					units = forcing_units[forcing]
					attr_name = "col_%d_%s_units" % (forcing, oname)
					setattr(nc_out, attr_name, units)

def parse_args(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Formatting ozflux data into a format suitable for consumption by LPJ-Guess")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default 3)", nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.INFORMATION)
	parser.add_argument("files", nargs = "+", help = "Input .nc files to be processed")
	parser.add_argument("-o", "--out-dir", required = True, help = "Path to the output directory. Processed files will be saved with the same file name into this directory")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	parser.add_argument("-P", "--parallel", action = "store_true", help = "Process files in parallel")
	parser.add_argument("-t", "--timestep", type = int, required = True, help = "Output timestep in minutes")
	parser.add_argument("-c", "--output-compatibility", type = int, required = True, help = "Output compatibility type (0 = daily_grass, 1 = LSM)")
	parser.add_argument("--compression-level", type = int, nargs = "?", default = 4, help = "Compression quality for output file [0, 9]. 0 = none, 1 = fastest compression, largest filesize, 9 = slowest compression, smallest filesize (default 4)")
	parser.add_argument("--compression-type", default = "zlib", help = "Compression algorithm to be used (default 'zlib')")
	parser.add_argument("--version", action = "version", version = "%(prog)s " + ozflux_common.VERSION)

	parsed = parser.parse_args(argv[1:])
	return Options(parsed.verbosity, parsed.files, parsed.out_dir
	, parsed.show_progress, parsed.parallel, parsed.timestep
	, parsed.output_compatibility, parsed.compression_level
	, parsed.compression_type)

def main(opts: Options):
	"""
	Main CLI entrypoint function.

	@param opts: Object containing parsed CLI arguments.
	"""
	# Create output directory if it doesn't exist.
	if not os.path.exists(opts.out_dir):
		os.makedirs(opts.out_dir)

	# overall_progress holds the progress [0, 1] of processing each
	# file.
	global overall_progress
	overall_progress = [0.0] * len(opts.files)

	# Relative time spent in each file (based on file size).
	file_weightings = [os.path.getsize(f) for f in opts.files]
	total_weight = sum(file_weightings)
	file_weightings = [x / total_weight for x in file_weightings]

	# These are only required in parallel mode.
	processes = []
	mutex = multiprocessing.BoundedSemaphore(1)

	def progress_reporter(progress: float, file_idx: int):
		"""
		Locally-scoped function which is called periodically by in order
		to report the progress of processing a particular file.

		@param progress: Progress of the file processing in range [0, 1].
		@param file_idx: Index of the file being processed.
		"""
		# Shortcut to avoid unnecessary computations.
		if not opts.report_progress:
			return

		global overall_progress

		weighted_progress = file_weightings[file_idx] * progress

		# No need to check if in parallel mode, as the mutex is easily
		# obtained when running in serial mode.
		with mutex:
			overall_progress[file_idx] = weighted_progress
			aggregate_progress = sum(overall_progress) / sum(file_weightings)
			log_progress(aggregate_progress)

	class ProcessingTask(multiprocessing.Process):
		"""
		A sub-process which will process a single file, with process reporting
		via a pipe back to the main process.

		Using multiple processes because the hdf5 library is not threadsafe.
		"""
		def __init__(self, infile: str, outfile: str, idx: int, timestep: int
		, output_type: OutputType
		, progress_writer: multiprocessing.connection.Connection):
			multiprocessing.Process.__init__(self)
			self.index = idx
			self.infile = infile
			self.outfile = outfile
			self.timestep = timestep
			self.output_type = output_type
			self.progress_writer = progress_writer
		def run(self):
			# Do main processing.
			process_file(self.infile, self.outfile, self.timestep
			, self.output_type
			, lambda p: self.progress_writer.send((p, self.index)))

			# Close progress reporter pipe.
			self.progress_writer.close()

	readers: list[multiprocessing.connection.Connection] = []

	for i in range(len(opts.files)):
		infile = opts.files[i]
		out_file_name = os.path.basename(infile)
		if opts.out_type == OutputType.LSM:
			out_file_name = get_output_filename(infile)

		outfile = os.path.join(opts.out_dir, out_file_name)
		if os.path.exists(outfile):
			os.remove(outfile)

		if opts.parallel:
			# Run task in a background process (hdf5/netcdf is not thread-safe).

			# Create a pipe for 1-way communication (progress reporting).
			reader, writer = multiprocessing.connection.Pipe(duplex = False)
			# Put the read pipe handle into the readers list.
			readers.append(reader)

			# Start the process.
			p = ProcessingTask(infile, outfile, i, opts.timestep, opts.out_type
				, writer)
			p.start()

			# Close the writable end of the pipe now, to be sure that p is the
			# only process which owns a handle for it. This ensures that when p
			# closes its handle for the writable end, wait() will promptly
			# report the readable end as being ready.
			writer.close()

			# Store the process handle for later use.
			processes.append(p)
		else:
			# Run task in the current thread.
			process_file(infile, outfile, opts.timestep, opts.out_type
			, lambda p: progress_reporter(p, i))

	if opts.parallel:
		while readers:
			for reader in readers:
				try:
					msg = None
					if reader.poll():
						msg = reader.recv()
						(progress, process_index) = msg
						progress_reporter(progress, process_index)
				except EOFError:
					readers.remove(reader)

		# Wait for processes to exit (actually, they should have all finished by
		# the time we get to here).
		for p in processes:
			p.join()

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(argv)

	set_log_level(opts.log_level)
	set_show_progress(opts.report_progress)

	try:
		# Actual logic is in main().
		main(opts)
	except BaseException as error:
		# Basic error handling.
		log_error(traceback.format_exc())
		exit(1)
