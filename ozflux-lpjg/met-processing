#!/usr/bin/env python
from argparse import ArgumentParser
import os, re, time
import ozflux_dave, ozflux_netcdf, ozflux_parallel
import traceback
from ozflux_logging import *
from netCDF4 import Dataset
from sys import argv
from typing import Callable
from ozflux_netcdf import *
from ozflux_common import *
from ozflux_dave import get_dailygrass_vars
import ozflux_lsm

class Options:
	"""
	Class for storing CLI arguments from the user.

	@param log: Log level.
	@param files: Input files.
	@param odir: Output directory.
	@param prog: True to write progress messages, 0 otherwise.
	@param parallel: True to process files in parallel.
	@param timestep: Desired output timestep, in minutes.
	@param compression_level: Compression quality for output file [0, 9]. 0 = none, 1 = fastest compression, largest filesize, 9 = slowest compression, smallest filesize.
	@param compression_type: Compression algorithm to be used (default 'zlib').
	@param single_outfile: Iff true, inputs will be written to a single output file.
	"""
	def __init__(self, log : LogLevel, files: list[str], odir: str, \
		prog: bool, parallel: bool, timestep: int, out_type: LpjVersion,
		compression_level: int, compression_type: str, single_outfile: bool):
		self.log_level = log
		self.files = files
		self.out_dir = odir
		self.report_progress = prog
		self.parallel = parallel
		self.timestep = timestep
		self.compression_level = compression_level
		self.compression_type = compression_type
		self.out_type = LpjVersion(out_type)
		self.single_outfile = single_outfile

class MetProcessingTask(ozflux_parallel.Task):
	def __init__(self, infile: str, outfile: str, timestep: int
	      , out_type: LpjVersion, compression_level: int
		  , compression_type: str):
		self.infile = infile
		self.outfile = outfile
		self.timestep = timestep
		self.out_type = out_type
		self.compression_level = compression_level
		self.compression_type = compression_type

	def exec(self, pcb: Callable[[float], None]):
		process_file(self.infile, self.outfile, self.timestep
		, self.out_type, self.compression_level, self.compression_type, pcb)

def six_digit_string(x: float) -> str:
	"""
	Return a number, encoded as a 6 digit string.
	"""
	# Round x to 2 digits.
	res = str(abs(int(round(x, 2) * 1000)))
	pad = "0" * (6 - len(res))
	return "%s%s" % (pad, res)

def get_output_filename(infile: str) -> str:
	"""
	Get the expected output file name for the given input .nc file.
	"""
	with Dataset(infile, "r", format=NC_FORMAT) as nc:
		# Read latitude/longitude from input .nc file.
		lon = float(nc.longitude)
		lat = float(nc.latitude)

		return get_met_filename(lon, lat)

def write_data_dg(nc_out: Dataset, var_name: str, data: list[float], lon: float
	, lat: float, progress_cb: Callable[[float], None]):
	"""
	Write data to the output file (dailygrass mode).

	@param variable: Variable in the output file to which data will be written.
	@param data: A timeseries of data for a variable for a gridcell.
	@param lon: Longitude of the data.
	@param lat: Latitude of the data.
	@param progress_cb: Called to report progress.
	"""
	n = len(data)
	chunk_size = get_steps_per_year(opts.timestep)
	for i in range(0, n, chunk_size):
		upper = min(n, i + chunk_size)
		(ilon, ilat) = get_coord_indices(nc_out, lon, lat)
		# Dimension order is time,lat,lon
		nc_out.variables[var_name][ilat, ilon, i:upper] = data[i:upper]
		progress_cb(upper / n)

def change_start_minute(datetime: str, minute: int) -> str:
	"""
	Change the minute value of a date/time string.

	@param datetime: Date/time string in format yyyy-MM-dd hh:mm:ss
	@param minute: The new value of the minute field.
	"""
	if minute < 0 or minute > 59:
		m = "Unable to set start minute: minute value %d must be in range [0, 59]"
		raise ValueError(m % minute)

	pattern = r"(\d{4}-\d{1,2}-\d{1,2} \d{1,2}:)\d{1,2}(:\d{1,2})"
	matches = re.findall(pattern, datetime)
	if len(matches) < 2:
		m = "Invalid datetime format; expected yyyy-MM-dd hh:mm:ss format, but was '%s'"
		raise ValueError(m % datetime)
	return "%s%d%s" % (matches[0], minute, matches[1])

def get_vars(version: LpjVersion, timestep: int) -> list[ForcingVariable]:
	"""
	Get the list of variables to be copied to the output file.

	@param version: The target lpj-guess version.
	@param timestep: Output timestep in minutes.
	"""
	if version == LpjVersion.LSM:
		return ozflux_lsm.get_lsm_vars()
	if version == LpjVersion.DAILY_GRASS:
		return get_dailygrass_vars(timestep)
	raise ValueError("Unknown output mode: %d" % version)

def copy_data_lsm(in_file: Dataset, out_file: Dataset, timestep: int, \
	pcb: Callable[[float], None]):
	"""
	Copy all data from the input file to the output variable.

	@param input file: Opened .nc file.
	@param variable: The newly-created and initialised forcing variable.
	@param timestep: Desired output timestep in minutes.
	@param pcb: Progress reporting function.
	"""
	# 0. Get variable IDs.
	i = 0
	vars = ozflux_lsm.get_lsm_vars()
	nvar = len(vars)
	n = len(vars)
	for var in vars:
		log_diagnostic("----- %s -----" % var.out_name)
		log_diagnostic("Reading %s data" % var.out_name)
		read_proportion = 0.66
		write_proportion = 1 - read_proportion

		data = get_data(in_file, var, timestep, lambda p: \
			pcb((i + p * read_proportion) / nvar))

		# 3. Write data to out file.
		start = i + read_proportion
		step = write_proportion
		log_diagnostic("Writing %s data" % var.out_name)
		reporter = lambda p: pcb((start + step * p) / n)
		ozflux_lsm.write_data(out_file, data, var, reporter)
		i += 1

def write_data(nc_in: Dataset, nc_out: Dataset, data: list[float] \
	, var: ForcingVariable, version: LpjVersion, pcb: Callable[[float], None]):
	"""
	Write the data to the output file.

	@param nc: The output netcdf file.
	@param data: The data to be written.
	@param var: The variable being copied.
	@param version: The lpj-guess version selected by the user.
	@param pcb: Progress callback function.
	"""
	if version == LpjVersion.LSM:
		ozflux_lsm.write_data(nc_out, data, var, pcb)
	elif version == LpjVersion.DAILY_GRASS:
		lon = float(nc_in.longitude)
		lat = float(nc_in.latitude)
		write_data_dg(nc_out, var.out_name, data, lon, lat, pcb)
	else:
		raise ValueError("Unknown lpj version %d" % version)

def copy_data(nc_in: Dataset, nc_out: Dataset, timestep: int
	, version: LpjVersion , pcb: Callable[[float], None]):
	"""
	Copy data to the output file in dailygrass format.

	@param nc_in: Input NetCDF file.
	@param nc_out: Output NetCDF file.
	@param timestep: Output timestep length in minutes.
	@param version: Output lpj-guess version selected by the user.
	@param progress_cb: Progress callback function.
	"""
	i = 0

	# Variables to be added to output file.
	variables = get_vars(version, timestep)

	# 95% of time to read
	global global_read_prop, global_write_prop

	nvar = len(variables)

	n = 0
	for var in variables:
		if opts.parallel:
			global cd_tp_lock
			cd_tp_lock.acquire()
		read_prop = global_read_prop
		write_prop = global_write_prop
		if opts.parallel:
			cd_tp_lock.release()

		log_diagnostic("----- %s -----" % var.in_name)
		log_diagnostic("Reading %s data")

		reporter = lambda p: pcb( (i + p * read_prop) / nvar)
		read_start = time.time()
		data = get_data(nc_in, var, timestep, reporter)
		read_time = time.time() - read_start

		n = max(n, len(data))

		log_diagnostic("Writing %s data")
		reporter = lambda p: pcb( (i + read_prop + write_prop * p) / nvar)
		write_start = time.time()
		write_data(nc_in, nc_out, data, var, version, reporter)
		write_time = time.time() - write_start

		# Bookkeeping.
		total_time = read_time + write_time
		if opts.parallel:
			cd_tp_lock.acquire()
		global_read_prop = read_time / total_time
		global_write_prop = write_time / total_time
		log_debug("Read: %.2f%%, Write: = %.2f%%" % \
			(global_read_prop, global_write_prop))
		if opts.parallel:
			cd_tp_lock.release()

		# Copy a few attributes.f
		in_var = nc_in.variables[var.in_name]
		out_var = nc_out.variables[var.out_name]

		out_var.units = var.out_units
		if 'standard_name' in in_var.ncattrs():
			out_var.standard_name = in_var.standard_name
		else:
			out_var.standard_name = var.in_name
		out_var.long_name = in_var.long_name

		i += 1

	# Copy data into time variable. This is # of hours since start date.
	# So it starts at 0 and increments by timestep width (in hours).
	# Write data in chunks to speed things up a bit.
	step_width = opts.timestep / MINUTES_PER_HOUR
	timesteps = [i * step_width for i in range(n)]
	for i in range(0, n, CHUNK_SIZE):
		up = min(n, i + CHUNK_SIZE)
		nc_out.variables[DIM_TIME][i:up] = timesteps[i:up]

def create_dimensions(nc: Dataset, version: LpjVersion, ngridcell: int):
	"""
	Create the required dimensions in the output file.
	"""
	log_debug("Creating output dimensions")
	if (version == LpjVersion.LSM):
		ozflux_lsm.create_dimensions(nc)
	else:
		ozflux_dave.create_dimensions(nc, ngridcell)

	# Time dimension is needed in both LSM and daily_grass mode.
	create_dim_if_not_exists(nc, DIM_TIME)

def create_variables(nc: Dataset, version: LpjVersion, timestep: int
	, clevel: int, ctype: str):
	"""
	Create the required variables in the output file.

	@param nc: The output .nc file.
	@param version: The lpj-guses version selected by the user.
	@param timestep: Output timestep in minutes.
	@param clevel: Compression level [1-9], 9 = slowest/smallest.
	@param ctype: Compression algorithm to be used (or None).
	"""
	log_debug("Creating output variables")
	if (version == LpjVersion.LSM):
		ozflux_lsm.create_variables(nc)
	else:
		# I've created a separate function for the dailygrass variables to
		# improve readability in this function.
		ozflux_dave.create_dailygrass_variables(nc, timestep, clevel, ctype)

def prepare_outfile(nc: Dataset, version: LpjVersion, ngridcell: int
	, timestep: int, clevel: int, ctype: str):
	"""
	Create the variables in the output NetCDF file. The variables created
	will depend to some degree on the output type.

	@param nc: Output .nc file.
	@param version: Output lpj-guess version selected by the user.
	@param ngridcell: The number of gridcells to be stored in this output file.
	@param timestep: Output timestep in minutes.
	@param clevel: Compression level [1-9], 9 = slowest/smallest.
	@param ctype: Compression algorithm to be used (or None).
	"""
	create_dimensions(nc, version, ngridcell)
	create_variables(nc, version, timestep, clevel, ctype)

def open_outfile(file: str) -> Dataset:
	log_debug("Opening output file %s for writing" % file)
	return Dataset(file, "r+", format = NC_FORMAT)

def process_files(in_files: list[str], out_file: str, timestep: int, out_type: \
	LpjVersion, pcb: Callable[[float], None]):
	"""
	Read the input files and write all data to the specified output file.

	@param in_files: Input files.
	@param out_file: Output file path.
	@param timestep: Output timestep in minutes.
	@param out_type: Target lpj-guess branch (LSM or daily-grass).
	@param pcb: Progress reporting function.
	"""
	read_prop = 0.9
	write_prop = 1 - read_prop
	with open_outfile(out_file) as nc_out:
		...

def write_metadata(nc: Dataset, timestep: int, version: LpjVersion):
	"""
	Write version-specific metadata.

	@param nc: Output .nc file.
	@param timestep: Output timestep in minutes.
	@param version: Output version selected by the user.
	"""
	ozflux_netcdf.write_common_metadata(nc, timestep)

	if version == LpjVersion.DAILY_GRASS:
		ozflux_dave.write_metadata(nc)
	elif version == LpjVersion.LSM:
		ozflux_lsm.write_metadata(nc)
	else:
		raise ValueError("Unknown output version %d" % version)

def process_file(in_file: str, out_file: str, timestep: int \
	, version: LpjVersion, clevel: int, ctype: str
	, pcb: Callable[[float], None]):
	"""
	Read the input file and generate an output file at the specified
	path in LPJ-Guess (lsminput) format.

	@param in_file: Input file path.
	@param out_file: Output file path.
	@param timestep: Output timestep length in minutes.
	@param version: Output version selected by the user.
	@param clevel: Compression level [1-9], 9 = slowest/smallest.
	@param ctype: Compression algorithm to be used (or None).
	@param pcb: Progress callback function.
	"""
	log_information("Processing %s..." % in_file)
	with open_outfile(out_file) as nc_out:
		log_debug("Opening input file %s for reading" % in_file)
		with Dataset(in_file, "r", format=NC_FORMAT) as nc_in:
			# Quick sanity check of time step.
			instep = int(nc_in.time_step)
			if instep > timestep:
				m = "Invalid input timestep (%d). Must be <= output (%d)"
				raise ValueError(m % instep, timestep)
			if timestep % instep != 0:
				m = "Invalid input timestep: %d. Must be an integer multiple of output timestep (%d)"
				raise ValueError(m % (instep, timestep))

			prepare_outfile(nc_out, version, 1, timestep, clevel, ctype)

			# 2. Copy data into this variable.
			log_debug("Migrating data")
			copy_data(nc_in, nc_out, timestep, version, pcb)

			# 3. Write site metadata.
			start_date = get_next_year(
				parse_date(nc_in.time_coverage_start))
			nc_out.time_coverage_start = start_date.strftime(
				DATE_FORMAT)
			nc_out.time_coverage_end = nc_in.time_coverage_end
			nc_out.latitude = nc_in.latitude
			nc_out.longitude = nc_in.longitude
			site_name = os.path.basename(in_file)
			site_name = os.path.splitext(site_name)[0]
			nc_out.site_name = site_name

			# Write additional global/common metadata.
			write_metadata(nc_out, timestep, opts.out_type)

def parse_args(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Formatting ozflux data into a format suitable for consumption by LPJ-Guess")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default 3)", nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.INFORMATION)
	parser.add_argument("files", nargs = "+", help = "Input .nc files to be processed")
	parser.add_argument("-o", "--out-dir", required = True, help = "Path to the output directory. Processed files will be saved with the same file name into this directory")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	parser.add_argument("-P", "--parallel", action = "store_true", help = "Process files in parallel")
	parser.add_argument("-t", "--timestep", type = int, required = True, help = "Output timestep in minutes")
	parser.add_argument("-c", "--output-compatibility", type = int, required = True, help = "Output compatibility type (0 = daily_grass, 1 = LSM)")
	parser.add_argument("--compression-level", type = int, nargs = "?", default = 4, help = "Compression quality for output file [0, 9]. 0 = none, 1 = fastest compression, largest filesize, 9 = slowest compression, smallest filesize (default 4)")
	parser.add_argument("--compression-type", default = "zlib", help = "Compression algorithm to be used (default 'zlib')")
	parser.add_argument("-s", "--single-outfile", action = "store_true", help = "Combine all inputs into a single output file")
	parser.add_argument("--version", action = "version", version = "%(prog)s " + VERSION)

	parsed = parser.parse_args(argv[1:])
	return Options(parsed.verbosity, parsed.files, parsed.out_dir
	, parsed.show_progress, parsed.parallel, parsed.timestep
	, parsed.output_compatibility, parsed.compression_level
	, parsed.compression_type, parsed.single_outfile)

def main(opts: Options):
	"""
	Main CLI entrypoint function.

	@param opts: Object containing parsed CLI arguments.
	"""
	# Create output directory if it doesn't exist.
	if not os.path.exists(opts.out_dir):
		os.makedirs(opts.out_dir)

	job_manager = ozflux_parallel.JobManager()

	for infile in opts.files:
		out_file_name = os.path.basename(infile)
		if opts.out_type == LpjVersion.LSM:
			out_file_name = get_output_filename(infile)

		outfile = os.path.join(opts.out_dir, out_file_name)
		if os.path.exists(outfile):
			os.remove(outfile)

		job = MetProcessingTask(infile, outfile, opts.timestep, opts.out_type
			  , opts.compression_level, opts.compression_type)

		job_manager.add_job(job, os.path.getsize(infile))

	if opts.parallel:
		job_manager.run_parallel()
	else:
		job_manager.run_single_threaded()

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(argv)

	set_log_level(opts.log_level)
	set_show_progress(opts.report_progress)

	try:
		# Actual logic is in main().
		main(opts)
	except BaseException as error:
		# Basic error handling.
		log_error(traceback.format_exc())
		exit(1)
