#!/usr/bin/env python
from argparse import ArgumentParser
import re, datetime, traceback
from construct import Switch
from netCDF4 import Dataset, Variable
from enum import IntEnum
from sys import argv, stdout, stderr
from typing import Callable

VERSION = "0.1"
CHUNK_SIZE = 1024
DATE_FORMAT = r"%Y-%m-%d %H:%M:%S"

class LogLevel(IntEnum):
	NONE = 0,
	ERROR = 1,
	WARNING = 2,
	INFORMATION = 3,
	DIAGNOSTIC = 4,
	DEBUG = 5

#######################################################################
# Constants related to .nc format/processing
#######################################################################

# Name of the single variable created in the output file.
OUT_VARIABLE_FORCING_NAME = "forcing_data"

# Data format in output file.
OUT_VARIABLE_FORCING_FORMAT = "f8"

# Name of the forcings dimension in the output file.
OUT_DIM_NAME_FORCINGS = "forcings"

# Name of the time dimension in the output file.
OUT_DIM_NAME_TIME = "time"

# NetCDF file format used for input and output.
NC_FORMAT = "NETCDF4"

# Forcing variable IDs (and column IDs too - this defines the order of
# the columns in the output file).
class Forcing(IntEnum):
	"""
	Indices of variables in the output .nc file.
	"""
	SWDOWN = 0,
	PARDF = 1,
	PARDR = 2,
	LWDOWN = 3,
	PRECLS = 4,
	PRECCO = 5,
	TAIR = 6,
	UAIR = 7,
	VAIR = 8,
	QAIR = 9,
	PSURF = 10
	# Insert new values here if required.
	NFORCINGS = 11 # This must always be the last/highest enum value

# Variable names for the output file.
outdata_forcing_names = {
	Forcing.SWDOWN: "swdown",
	Forcing.PARDF: "pardf",
	Forcing.PARDR: "pardr",
	Forcing.LWDOWN: "lwdown",
	Forcing.PRECLS: "precls",
	Forcing.PRECCO: "precco",
	Forcing.TAIR: "tair",
	Forcing.UAIR: "uair",
	Forcing.VAIR: "vair",
	Forcing.QAIR: "qair",
	Forcing.PSURF: "psurf"
}

# Variable names for the ozflux input file.
indata_forcing_names = {
	Forcing.SWDOWN: "Fsd",
	Forcing.PARDF: "zero",
	Forcing.PARDR: "zero",
	Forcing.LWDOWN: "Fld",
	Forcing.PRECLS: "Precip",
	Forcing.PRECCO: "zero",
	Forcing.TAIR: "Ta",
	Forcing.UAIR: "Ws",
	Forcing.VAIR: "zero",
	Forcing.QAIR: "SH",
	Forcing.PSURF: "ps"
}

# From DMB:
# > Radiation (SWDOWN, PARDF, PARDR, LWDOWN)-> W/m^2
# > Precipitation (PRECLS, PRECCO) -> kg/m^2/s
# > Air temperature (TAIR) -> K
# > Wind speed (UAIR, VAIR) -> m/s (u = eastward, v = northward)
# > Air specific humidity (QAIR) -> kg/kg (unitless)
# > Pressure (PSURF) -> Pa
forcing_units = {
	Forcing.SWDOWN: "W/m2",
	Forcing.PARDF: "W/m2",
	Forcing.PARDR: "W/m2",
	Forcing.LWDOWN: "W/m2",
	Forcing.PRECLS: "kg/m2/s",
	Forcing.PRECCO: "kg/m2/s",
	Forcing.TAIR: "K",
	Forcing.UAIR: "m/s",
	Forcing.VAIR: "m/s",
	Forcing.QAIR: "",
	Forcing.PSURF: "Pa"
}

units_synonyms = [
	["W/m2", "W/m^2", "Wm^-2"],
	["kg/m2/s", "kg/m^2/s", "kgm^-2s^-1"],
	["K", "k"],
	["m/s", "ms^-1"],
	["Pa", "pa"],
	["kg/kg", "", "mm/mm", "m/m"]
]

# Whether a particular variable's units are a rate (true) or a magnitude
# (false) in the input data. This is used when aggregating from one
# timestep to another.
indata_units_are_rate = {
	Forcing.SWDOWN: True,
	Forcing.PARDF: True,
	Forcing.PARDR: True,
	Forcing.LWDOWN: True,
	Forcing.PRECLS: False, # precip in mm
	Forcing.PRECCO: False, # precip in mm
	Forcing.TAIR: True,
	Forcing.UAIR: True,
	Forcing.VAIR: True,
	Forcing.QAIR: True,
	Forcing.PSURF: True,
}

# g to kg. This is not really used except as an example.
G_TO_KG = 1e-3

# KPa to Pa
KPA_TO_PA = 1e3

# Â°C to K
DEG_C_TO_K = 273.15

# This assumes an hourly output timestep.
SECONDS_PER_TIMESTEP = 60 * 60

# Recipes for unit conversions.
units_conversions = {
	("g", "kg"): lambda x: x * G_TO_KG, # (as an example)
	("mm", "kg/m2/s"): lambda x: x * SECONDS_PER_TIMESTEP,
	("degC", "K"): lambda x: x + DEG_C_TO_K,
	("kPa", "Pa"): lambda x: x * KPA_TO_PA
}

#######################################################################
# End of .nc constants
#######################################################################

class Options:
	"""
	Class for storing CLI arguments from the user.

	@param log_l: Log level.
	@param ifile: Input file.
	@param ofile: Output file.
	@param prog: True to write progress messages, 0 otherwise.
	"""
	def __init__(self, log_l : LogLevel, ifile: str, ofile: str, prog: bool):
		self.log_level = log_l
		self.in_file = ifile
		self.out_file = ofile
		self.report_progress = prog

# Global variable which tracks log level as set by the user.
global_log_level: LogLevel

def log(msg: str, log_level: LogLevel):
	"""
	Write a log message.
	"""
	# todo: custom log file as CLI arg?
	if log_level <= global_log_level:
		file = stderr if log_level == LogLevel.ERROR else stdout
		print(msg, file = file)

def log_error(msg: str):
	"""
	Write an error message.
	"""
	log(msg, LogLevel.ERROR)

def log_warning(msg: str):
	"""
	Write a warning message.
	"""
	log(msg, LogLevel.WARNING)

def log_information(msg: str):
	"""
	Write an information message.
	"""
	log(msg, LogLevel.INFORMATION)

def log_diagnostic(msg: str):
	"""
	Write a diagnostic message.
	"""
	log(msg, LogLevel.DIAGNOSTIC)

def log_debug(msg: str):
	"""
	Write a debug message.
	"""
	log(msg, LogLevel.DEBUG)

def dump_netcdf_file(filename):
	"""
	Print all variable names/descriptions in the .nc file.
	"""
	with open(filename) as nc_file:
		for (key, value) in nc_file.variables.items():
			print("%s: %s" % (key, value.long_name))

def get_units(var_id: Variable) -> str:
	"""
	Get the units for the specified variable in the .nc file.
	"""
	return var_id.units

def log_progress(progress: float):
	"""
	Report progress to the user.

	@param Progress: Current overall progress [0-1].
	"""
	if opts.report_progress:
		print("Working: %.2f%%\r" % (100.0 * progress), end = "")

def copy_data(in_file: Dataset, variable: Variable):
	"""
	Copy all data from the input file to the output variable.

	@param input file: Opened .nc file.
	@param variable: The newly-created and initialised forcing variable.
	"""
	# 0. Get variable IDs.
	i = 0
	for forcing in Forcing:
		if forcing == Forcing.NFORCINGS:
			break
		out_name = outdata_forcing_names[forcing]
		log_information("----- %s -----" % out_name)
		log_information("Reading %s data" % out_name)
		read_proportion = 0.66
		write_proportion = 1 - read_proportion
		data = get_data(in_file, forcing, lambda p: log_progress( \
			(i + p * read_proportion) / Forcing.NFORCINGS))

		# 3. Write data to out file.
		log_information("Writing %s data" % out_name)
		write_data(variable, data, forcing, lambda p: log_progress( \
			(i + read_proportion + write_proportion * p) / Forcing.NFORCINGS))
		i += 1

def write_data(variable: Variable, data: list[float], forcing: Forcing, \
	progress_callback: Callable[[float], None]):
	"""
	Write data to the output file.
	"""
	n = len(data)
	chunk_size = CHUNK_SIZE
	for i in range(n):
		row = i
		col = forcing
		# index = row * Forcing.NFORCINGS + col
		variable[row, col] = data[i]

		if i % chunk_size == 0:
			progress_callback(i / n)

	log_diagnostic("Successfully wrote %d items of %s" % (n, forcing))

def zeroes(n: int) -> list[float]:
	"""
	Create an array of zeroes of the specified length.
	"""
	x = [float] * n
	for i in range(n):
		x[i] = 0
	return x

def temporal_aggregation(in_file: Dataset, data: list[float], forcing: Forcing, \
	progress_callback: Callable[[float], None]) -> list[float]:
	"""
	Aggregate the data from 30m to 60m (hourly) timestep.
	"""
	# todo: support other timesteps (both source and target)?
	if in_file.time_step != "30":
		m = "Unsupported input timestep: %s"
		raise ValueError(m % in_file.time_step)

	n_start = len(data)

	start_minute = get_start_minute(in_file)
	if start_minute == 30:
		# Don't include the first timestep value if it lies on the half-
		# hour boundary. trim_to_start_year() relies on this assumption,
		# so it will need to be updated if we change this and vice versa.
		m = "Input data starts on half-hour boundary. First value will be removed."
		log_diagnostic(m)
		data = data[1:]
	elif start_minute != 0:
		m = "Strange start time: '%s'. Data must start on the hour or half-hour"
		raise ValueError(m % in_file.time_coverage_start)

	# Integer division to discard the remainder. If data has an odd
	# length, the last value will be appended later.
	n = len(data) // 2
	out = [float] * n

	m = "Values are %s and therefore we will take the %s over each timestep"
	unit_kind = "rate" if indata_units_are_rate[forcing] else "magnitude"
	agg_kind = "mean" if indata_units_are_rate[forcing] else "sum"
	log_diagnostic(m % (unit_kind, agg_kind))
	for i in range(n):
		# Use mean of this timestep and the next. For some variables and
		# units this won't make sense, but all of the current ozflux
		# variables are rates rather than magnitudes, so I think this
		# should be ok.
		x0 = data[2 * i]
		x1 = data[2 * i + 1]

		# If the value is a rate, we take the mean rate over the time period.
		if indata_units_are_rate[forcing]:
			out[i] = (x0 + x1) / 2
		else:
			# For a magnitude, we sum the values over the time period.
			out[i] = x0 + x1

		if i % CHUNK_SIZE == 0:
			progress_callback(i / n)

	if len(data) % 2 == 1:
		# odd # of elems - this means the data ends on the hour, which
		# means it's missing the last half hour's data. This aggregation
		# may be incorrect for some units. But I think it's ok for the
		# current ozflux variables/units.
		log_diagnostic("Data ends on the hour - we will take the last value")
		x = data[len(data) - 1]
		if not indata_units_are_rate[forcing]:
			# As we only have half of the data for the timestep, we assume
			# the rate was consistent across the timestep.
			x *= 2
		out.append(x)

	n_end = len(out)
	log_diagnostic("Temporal aggregation reduced array length by %d values" % (n_start - n_end))

	return out

def find_units_conversion(current_units: str, desired_units: str) \
	-> Callable[[float], float]:
	"""
	Find a conversion between two different units. Throw if not found.
	The return value is a function which takes and returns a float.
	"""
	# units_conversions is a dict mapping unit combos to a conversion.
	# units_conversions: dict[tuple[str, str], Callable[[float],float]]
	combination = (current_units, desired_units)
	if combination in units_conversions:
		return units_conversions[combination]
	m = "No unit conversion exists from '%s' to '%s'"
	raise ValueError(m % (current_units, desired_units))

def fix_units(data: list[float], current_units: str, desired_units: str, \
	progress_callback: Callable[[float], None]) -> list[float]:
	"""
	Convert data to the units required for the output file.
	This will modify the existing array.
	"""
	conversion = find_units_conversion(current_units, desired_units)
	n = len(data)
	for i in range(n):
		data[i] = conversion(data[i])
		if i % CHUNK_SIZE == 0:
			progress_callback(i / n)
	return data

def units_match(unit0: str, unit1: str) -> str:
	"""
	Check if the two units are equivalent.
	E.g. m/s and ms^-1 would return true, but not m and kg.
	"""
	for case in units_synonyms:
		if unit0 in case and unit1 in case:
			return True
	return False

def read_data(variable: Variable, progress_callback: Callable[[float], None]) \
	-> list[float]:
	"""
	Read all data for a variable from the .nc input file.
	"""
	arr = [float] * variable.size
	n = len(arr)
	for i in range(n):
		arr[i] = variable[i]
		if i % CHUNK_SIZE == 0:
			progress_callback(i / n)
	return arr

def get_start_minute(in_file: Dataset):
	"""
	Determine the minute at which the dataset starts (0-59).
	This is done by checking the time_coverage_start attribute of the dataset.
	"""
	start_time = in_file.time_coverage_start
	# yyyy-MM-dd hh:mm:ss
	pattern = r"\d{4}-\d{1,2}-\d{1,2} \d{1,2}:(\d{1,2}):\d{1,2}"
	matches = re.findall(pattern, start_time)
	if len(matches) < 1:
		m = "Unable to parse start time; expected yyyy-MM-dd hh:mm:ss format, but was '%s'"
		raise ValueError(m % in_file.time_coverage_start)
	return int(matches[0])

def change_start_minute(datetime: str, minute: int) -> str:
	"""
	Change the minute value of a date/time string.
	"""
	if minute < 0 or minute > 59:
		m = "Unable to set start minute: minute value %d must be in range [0, 59]"
		raise ValueError(m % minute)

	pattern = r"(\d{4}-\d{1,2}-\d{1,2} \d{1,2}:)\d{1,2}(:\d{1,2})"
	matches = re.findall(pattern, datetime)
	if len(matches) < 2:
		m = "Invalid datetime format; expected yyyy-MM-dd hh:mm:ss format, but was '%s'"
		raise ValueError(m % datetime)
	return "%s%d%s" % (matches[0], minute, matches[1])

def parse_date(datestr: str) -> datetime.datetime:
	"""
	Parse a date object from a string in yyyy-MM-dd hh:mm:ss format.
	"""
	return datetime.datetime.strptime(datestr, DATE_FORMAT)

def is_start_of_year(date: datetime.datetime) -> bool:
	"""
	Check if a given datetime object represents the start of a year.
	"""
	return date.second == 0 \
	and date.minute == 0 \
	and date.hour == 0 \
	and date.day == 1 \
	and date.month == 1

def get_next_year(start_date: datetime.datetime) -> datetime.datetime:
	"""
	Get a date representing the first valid date time in the first year
	on or after the given date. So if start_date lies on the exact start
	of a year, start_date will be returned. Otherwise the first day of
	the next year will be returned.
	"""
	if is_start_of_year(start_date):
		m = "get_next_year(): Start date %s lies on start of year"
		log_diagnostic(m % start_date)
		return start_date
	return datetime.datetime(start_date.year + 1, 1, 1, 0, 0, 0)

def trim_to_start_year(in_file: Dataset, data: list[float]) -> list[float]:
	"""
	Trim the given data to the start of the next year.
	"""
	start_date = parse_date(in_file.time_coverage_start)
	if (start_date.minute == 30):
		# temporal_aggregation() trims the first value if it lies on the
		# half-hour boundary. We need to do the same here.
		log_diagnostic("Data starts on 30m boundary: 1st value is ignored")
		start_date = start_date + datetime.timedelta(minutes = 30)
	next_year = get_next_year(start_date)

	delta = next_year - start_date
	n_trim = int(delta.total_seconds() / 3600)
	m = "Trimming %d values to get to start of %d"
	log_diagnostic(m % (n_trim, next_year.year))
	return data[n_trim:]

def get_data(in_file: Dataset, forcing: Forcing, \
	progress_callback: Callable[[float], None]) -> list[float]:
		"""
		Get all data from the input file and convert it to a format
		suitable for the output file.
		"""
		# 0. Get variable ID from variable name.
		in_name = indata_forcing_names[forcing]
		out_name = outdata_forcing_names[forcing]

		# Some variables are not translated directly into the output
		# file and can be ignored (ie filled with zeroes).
		if in_name == "zero":
			log_information("Using zeroes for %s" % out_name)
			data = zeroes(in_file.variables['time'].size // 2)
			return trim_to_start_year(in_file, data)
		if not in_name in in_file.variables:
			raise ValueError("Variable %s does not exist in input file" % in_name)

		var_id = in_file.variables[in_name]
		in_units = get_units(var_id)
		out_units = forcing_units[forcing]
		matching_units = units_match(in_units, out_units)

		# About 1/3 of time to fix units (if units need fixing).
		units_time_proportion = 0 if matching_units else 1 / 3

		# About 2/3 of remaining time to read the data from input file.
		read_time_proportion = (1 - units_time_proportion) * 2 / 3

		# Rest of time is spent aggregating over the timestep.
		aggregation_time_proportion = 1 - read_time_proportion - units_time_proportion

		# 1. Read data from netcdf file.
		data = read_data(var_id, lambda p: progress_callback(read_time_proportion * p))
		log_diagnostic("Successfully read %d values from netcdf" % len(data))

		# 2. Change timestep to something suitable for lpj-guess.
		log_information("Aggregating %s to hourly timestep" % out_name)
		data = temporal_aggregation(in_file, data, forcing, lambda p: progress_callback( \
			read_time_proportion + aggregation_time_proportion * p))
		log_diagnostic("Successfully aggregated to destination timestep")

		# 3. Trim data to the start of a year.
		data = trim_to_start_year(in_file, data)

		# 4. Ensure units are correct.
		if not matching_units:
			log_information("Converting %s from %s to %s" % \
				(out_name, in_units, out_units))
			data = fix_units(data, in_units, out_units, \
				lambda p: progress_callback( \
					read_time_proportion \
						+ aggregation_time_proportion \
						+ units_time_proportion * p))
		else:
			log_information("No units conversion required for %s" % out_name)

		# Done!
		return data

def process_file(in_file, out_file):
	"""
	Read the input file and generate an output file at the specified
	path in LPJ-Guess (lsminput) format.
	"""
	log_debug("Opening output file %s for writing" % out_file)
	with Dataset(out_file, "w", format=NC_FORMAT) as nc_out:
		log_debug("Opening input file %s for reading" % in_file)
		with Dataset(in_file, "r", format=NC_FORMAT) as nc_in:
			# Quick sanity check of time step.
			if nc_in.time_step != "30":
				m = "Unknown time step (%d) in input file %s. Only hourly is supported"
				raise ValueError(m % (nc_in.time_step, in_file))

			# 0. Create dimensions in output file.
			log_debug("Creating output dimensions")
			nc_out.createDimension(OUT_DIM_NAME_FORCINGS, Forcing.NFORCINGS)
			nc_out.createDimension(OUT_DIM_NAME_TIME, None)

			# 1. Create variables in output file.
			log_debug("Creating output variable")
			out_variable = nc_out.createVariable( \
				OUT_VARIABLE_FORCING_NAME \
				, OUT_VARIABLE_FORCING_FORMAT \
				, (OUT_DIM_NAME_TIME,OUT_DIM_NAME_FORCINGS))

			# 2. Copy data into this variable.
			log_debug("Migrating data")
			copy_data(nc_in, out_variable)

			# 3. Metadata for output file.
			nc_out.time_step = "60"
			start_date = get_next_year(parse_date(nc_in.time_coverage_start))
			nc_out.time_coverage_start = start_date.strftime(DATE_FORMAT)
			nc_out.time_coverage_end = nc_in.time_coverage_end
			for forcing in Forcing:
				if forcing == Forcing.NFORCINGS:
					break
				oname = outdata_forcing_names[forcing]
				units = forcing_units[forcing]
				attr_name = "col_%d_%s_units" % (forcing, oname)
				setattr(nc_out, attr_name, units)

def parse_args(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Formatting ozflux data into a format suitable for consumption by LPJ-Guess")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default 3)", nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.WARNING)
	parser.add_argument("-i", "--in-file", required = True, help = "Path to the input .nc file")
	parser.add_argument("-o", "--out-file", required = True, help = "Path to the output .nc file")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	parser.add_argument("--version", action = "version", version = "%(prog)s " + VERSION)

	parsed = parser.parse_args(argv[1:])
	return Options(parsed.verbosity, parsed.in_file, parsed.out_file, parsed.show_progress)

def main(opts: Options):
	"""
	Main CLI entrypoint function.

	@param opts: Object containing parsed CLI arguments.
	"""
	process_file(opts.in_file, opts.out_file)

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(argv)

	# todo: rethink logging
	global_log_level = opts.log_level
	try:
		# Actual logic is in main().
		main(opts)
	except BaseException as error:
		# Basic error handling.
		log_error(traceback.format_exc())
		exit(1)
