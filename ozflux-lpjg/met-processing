#!/usr/bin/env python
from argparse import ArgumentParser
import datetime, multiprocessing, numpy, os, ozflux_common, re, threading
import traceback
from ozflux_logging import *
from netCDF4 import Dataset, Variable
from enum import IntEnum
from ozflux_common import Forcing
from sys import argv
from typing import Callable

# Semantic version number.
VERSION = "1.0"

# Progress updates inside long loops are written every N iterations.
PROGRESS_CHUNK_SIZE = 1024

# Inputs/outputs will be read/written in chunks of this size.
# Increasing this will improve performance but increase memory usage.
CHUNK_SIZE = 16384

# Whether to aggregate timesteps in paralllel. This doesn't save a huge amount
# of time in the files that I've tested (which are fairly small), but it doesn't
# hurt performance, and should really help with larger files. I don't really see
# any reason to set this to false, which is why this isn't a CLI option.
PARALLEL_AGGREGATION = True

#######################################################################
# Constants related to .nc format/processing
#######################################################################

# Name of the single variable created in the output file.
OUT_VARIABLE_FORCING_NAME = "forcing_data"

# Data format in output file.
OUT_VARIABLE_FORCING_FORMAT = "f8"

# Name of the forcings dimension in the output file.
OUT_DIM_NAME_FORCINGS = "forcings"

# Name of the time dimension in the output file.
OUT_DIM_NAME_TIME = "time"

# Variable names for the output file.
outdata_forcing_names = {
	Forcing.SWDOWN: "swdown",
	Forcing.PARDF: "pardf",
	Forcing.PARDR: "pardr",
	Forcing.LWDOWN: "lwdown",
	Forcing.PRECLS: "precls",
	Forcing.PRECCO: "precco",
	Forcing.TAIR: "tair",
	Forcing.UAIR: "uair",
	Forcing.VAIR: "vair",
	Forcing.QAIR: "qair",
	Forcing.PSURF: "psurf"
}

# Variable names for the ozflux input file.
indata_forcing_names = {
	Forcing.SWDOWN: "Fsd",
	Forcing.PARDF: "zero",
	Forcing.PARDR: "zero",
	Forcing.LWDOWN: "Fld",
	Forcing.PRECLS: "Precip",
	Forcing.PRECCO: "zero",
	Forcing.TAIR: "Ta",
	Forcing.UAIR: "Ws",
	Forcing.VAIR: "zero",
	Forcing.QAIR: "SH",
	Forcing.PSURF: "ps"
}

# From DMB:
# > Radiation (SWDOWN, PARDF, PARDR, LWDOWN)-> W/m^2
# > Precipitation (PRECLS, PRECCO) -> kg/m^2/s
# > Air temperature (TAIR) -> K
# > Wind speed (UAIR, VAIR) -> m/s (u = eastward, v = northward)
# > Air specific humidity (QAIR) -> kg/kg (unitless)
# > Pressure (PSURF) -> Pa
forcing_units = {
	Forcing.SWDOWN: "W/m2",
	Forcing.PARDF: "W/m2",
	Forcing.PARDR: "W/m2",
	Forcing.LWDOWN: "W/m2",
	Forcing.PRECLS: "kg/m2/s",
	Forcing.PRECCO: "kg/m2/s",
	Forcing.TAIR: "K",
	Forcing.UAIR: "m/s",
	Forcing.VAIR: "m/s",
	Forcing.QAIR: "",
	Forcing.PSURF: "Pa"
}

# Min and max values for each forcing variable, in output units.
forcing_bounds = {
	Forcing.SWDOWN: (0, 1e5),
	Forcing.PARDF: (0, 1e5),
	Forcing.PARDR: (0, 1e5),
	Forcing.LWDOWN: (0, 1e5),
	Forcing.PRECLS: (0, 1e4),
	Forcing.PRECCO: (0, 1e3), # Always zero, not used.
	Forcing.TAIR: (200, 373), # Kelvins
	Forcing.UAIR: (0, 100),
	Forcing.VAIR: (0, 1), # Always zero, not used.
	Forcing.QAIR: (0, 0.2),
	Forcing.PSURF: (3e5, 1.5e6)
}

# Common alternative names for various units which we may reasonably encounter.
units_synonyms = [
	["W/m2", "W/m^2", "Wm^-2"],
	["kg/m2/s", "kg/m^2/s", "kgm^-2s^-1"],
	["K", "k"],
	["m/s", "ms^-1"],
	["Pa", "pa"],
	["kg/kg", "", "mm/mm", "m/m"] # debatable
]

# Whether a particular variable's units are a rate (true) or a magnitude
# (false) in the input data. This is used when aggregating from one
# timestep to another.
indata_units_are_rate = {
	Forcing.SWDOWN: True,
	Forcing.PARDF: True,
	Forcing.PARDR: True,
	Forcing.LWDOWN: True,
	Forcing.PRECLS: False, # precip in mm
	Forcing.PRECCO: False, # precip in mm
	Forcing.TAIR: True,
	Forcing.UAIR: True,
	Forcing.VAIR: True,
	Forcing.QAIR: True,
	Forcing.PSURF: True,
}

# g to kg. This is not really used except as an example.
G_TO_KG = 1e-3

# KPa to Pa
KPA_TO_PA = 1e3

# Â°C to K
DEG_C_TO_K = 273.15

# Number of seconds per minute.
SECONDS_PER_MINUTE = 60

# Number of minutes per hour.
MINUTES_PER_HOUR = 60

# Number of seconds per hour.
SECONDS_PER_HOUR = SECONDS_PER_MINUTE * MINUTES_PER_HOUR

# Recipes for unit conversions.
#
# The keys are a tuple of two strings:
#
# 1. The input units
# 2. The output units.
#
# The values are functions which convert a value from the input units into the
# output units. These functions take two arguments:
#
# x: The input value in input units.
# t: The timestep length (in seconds).
#
# Note that units are changed after timestep aggregation occurs. So the input
# variable will already be in the output timestep at this point in time.
units_conversions = {
	("g", "kg"): lambda x, _: x * G_TO_KG, # (as an example)
	("mm", "kg/m2/s"): lambda x, t: x / t, # Assuming mm means mm per timestep
	("degC", "K"): lambda x, _: x + DEG_C_TO_K,
	("kPa", "Pa"): lambda x, _: x * KPA_TO_PA
}

#######################################################################
# End of .nc constants
#######################################################################

class Options:
	"""
	Class for storing CLI arguments from the user.

	@param log: Log level.
	@param files: Input files.
	@param odir: Output directory.
	@param prog: True to write progress messages, 0 otherwise.
	@param parallel: True to process files in parallel.
	@param timestep: Desired output timestep, in minutes.
	"""
	def __init__(self, log : LogLevel, files: list[str], odir: str, \
		prog: bool, parallel: bool, timestep: int):
		self.log_level = log
		self.files = files
		self.out_dir = odir
		self.report_progress = prog
		self.parallel = parallel
		self.timestep = timestep

def six_digit_string(x: float) -> str:
	"""
	Return a number, encoded as a 6 digit string.
	"""
	# Round x to 2 digits.
	res = str(abs(int(round(x, 2) * 1000)))
	pad = "0" * (6 - len(res))
	return "%s%s" % (pad, res)

def get_output_filename(infile: str) -> str:
	"""
	Get the expected output file name for the given input .nc file.
	"""
	with Dataset(infile, "r", format=ozflux_common.NC_FORMAT) as nc:
		# Read latitude/longitude from input .nc file.
		lon = float(nc.longitude)
		lat = float(nc.latitude)

		return ozflux_common.get_met_filename(lon, lat)

def get_units(var_id: Variable) -> str:
	"""
	Get the units for the specified variable in the .nc file.
	"""
	return var_id.units

def copy_data(in_file: Dataset, variable: Variable, timestep: int, \
	progress_callback: Callable[[float], None]):
	"""
	Copy all data from the input file to the output variable.

	@param input file: Opened .nc file.
	@param variable: The newly-created and initialised forcing variable.
	@param timestep: Desired output timestep in minutes.
	@param progress_callback: Progress reporting function.
	"""
	# 0. Get variable IDs.
	i = 0
	for forcing in Forcing:
		if forcing == Forcing.NFORCINGS:
			break
		out_name = outdata_forcing_names[forcing]
		log_diagnostic("----- %s -----" % out_name)
		log_diagnostic("Reading %s data" % out_name)
		read_proportion = 0.66
		write_proportion = 1 - read_proportion
		data = get_data(in_file, forcing, timestep, lambda p: \
			progress_callback((i + p * read_proportion) / Forcing.NFORCINGS))

		# 3. Write data to out file.
		log_diagnostic("Writing %s data" % out_name)
		write_data(variable, data, forcing, lambda p: progress_callback( \
			(i + read_proportion + write_proportion * p) / Forcing.NFORCINGS))
		i += 1

def write_data(variable: Variable, data: list[float], forcing: Forcing, \
	progress_callback: Callable[[float], None]):
	"""
	Write data to the output file.

	@param variable: Variable in the output file, to which data will be written.
	@param data: A column of data to be written.
	@param forcing: Column of the variable to which the data will be written.
	@progress_callback: Called to report progress.
	"""
	n = len(data)
	for i in range(0, n, CHUNK_SIZE):
		row = i
		upper = min(n, i + CHUNK_SIZE)
		col = forcing
		variable[row:upper, col] = data[i:upper]
		progress_callback(i / n)

	log_debug("Successfully wrote %d items of %s" % (n, forcing))

def zeroes(n: int) -> list[float]:
	"""
	Create an array of zeroes of the specified length.
	"""
	return [0.0] * n

def aggregate(data: list[float], start: int, stop: int, chunk_size: int
	, aggregator: Callable[[list[float]], float]
	, progress_cb: Callable[[float], None]) -> list[float]:
	"""
	Aggregate the values in the specified dataset between the start and
	stop indices, using the specified aggregator function.

	@param data: The data.
	@param start: Aggregate only values after this index.
	@param stop: Aggregate only values before this index.
	@param chunk_size: Aggregate values in chunks of this size.
	@param aggregator: A function to aggregate a chunk of values.
	@param progress_cb: A function which handles progress reporting.
	"""
	n = len(data)
	if start < 0:
		raise ValueError("start cannot be less than 0 (is %d)" % start)
	if stop > n:
		m = "stop cannot be past end of list (is %d and n = %d)"
		raise ValueError(m % (stop, n))
	if start >= stop:
		m = "start cannot be greater than stop (start = %d, stop = %d)"
		raise ValueError(m % (start, stop))

	niter = ((stop - start) // chunk_size)
	result = [0.0] * niter
	for i in range(niter):
		lower = start + i * chunk_size
		upper = min(n, start + (i + 1) * chunk_size)
		result[i] = aggregator(data[lower:upper])
		if i % PROGRESS_CHUNK_SIZE == 0:
			progress_cb(i / niter)
	return result

def temporal_aggregation(in_file: Dataset, data: list[float], forcing: Forcing\
	, output_timestep: float
	, progress_callback: Callable[[float], None]) -> list[float]:
	"""
	Aggregate the input data to destination timestep.

	@param in_file: Input netcdf file.
	@param data: Data to be aggregated.
	@param forcing: The variable represented by the data.
	@param output_timestep: The desired output timestep.
	@param progress_callback: Function used for progress reporting.
	"""
	# todo: support other timesteps (both source and target)?
	input_timestep = int(in_file.time_step)
	if input_timestep == output_timestep:
		return data
	if output_timestep < input_timestep:
		m = "Invalid output timestep (%d); must be >= input timestep (%d)"
		raise ValueError(m % (output_timestep, input_timestep))
	if output_timestep % input_timestep != 0:
		m = "Invalid timestep; output (%d) must be an integer multiple of input\
timestep (%d)"
		raise ValueError(m % (output_timestep, input_timestep))

	# Record the amount of data at the start of the function. This is
	# written as a diagnostic later.
	n_start = len(data)

	start_minute = get_start_minute(in_file)
	if start_minute == 30 and input_timestep == 30:
		# Don't include the first timestep value if it lies on the half-
		# hour boundary. trim_to_start_year() relies on this assumption,
		# so it will need to be updated if we change this and vice versa.
		m = "Input data starts on 30m boundary. First value will be removed."
		log_debug(m)
		data = data[1:]
	elif start_minute != 0:
		m = "Strange start time: '%s'. Data must start on the hour or half-hour"
		raise ValueError(m % in_file.time_coverage_start)

	m = "Values are %s and therefore we will take the %s over each timestep"
	unit_kind = "rate" if indata_units_are_rate[forcing] else "magnitude"
	agg_kind = "mean" if indata_units_are_rate[forcing] else "sum"
	agg_func = numpy.mean if indata_units_are_rate[forcing] else numpy.sum
	# Guaranteed to be an integer division, given the above error checks.
	timestep_ratio = output_timestep // input_timestep
	log_debug(m % (unit_kind, agg_kind))

	out: list[float]
	out = []

	if PARALLEL_AGGREGATION:
		# This can be quite slow for large datasets, so I've parallelised it.

		# Get number of CPUs.
		ncpu = multiprocessing.cpu_count()

		# Number of timesteps to be processed by each thread.
		chunk_size = len(data) // ncpu

		# As we're aggregating every 2 timesteps together, we need to ensure
		# that each thread is processing an even number of values.
		if chunk_size % 2 == 1:
			chunk_size += 1

		global thread_progress
		thread_progress = [0.0] * ncpu
		global mutex
		mutex = multiprocessing.BoundedSemaphore(1)
		def update_progress(progress: float, tid: int):
			global thread_progress
			global mutex
			with mutex:
				thread_progress[tid] = progress
				progress_callback(numpy.mean(thread_progress))

		class Aggregator(threading.Thread):
			def __init__(self, data: list[float], start: int, stop: int
				, chunk_size: int, tid: int
				, aggregation_func: Callable[[list[float]], float]):
				threading.Thread.__init__(self)
				self.data = data
				self.istart = start
				self.stop = stop
				self.chunk_size = chunk_size
				self.thread_id = tid
				self.aggregation_func = aggregation_func
			def run(self):
				self.result = aggregate(self.data, self.istart, self.stop
				, self.chunk_size, self.aggregation_func
				, lambda p: update_progress(p, self.thread_id))

		# Create an array to hold the thread objects.
		threads = []
		for i in range(ncpu):
			# Calculate start and stop indices for this thread. Each thread
			# is operating on the same input data array, but they operate
			# on different parts (slices) of the array.
			start = i * chunk_size
			stop = len(data) if i == ncpu - 1 else start + chunk_size

			# Create the thread object.
			t = Aggregator(data, start, stop, timestep_ratio, i, agg_func)

			# Start the thread and store the object reference for later.
			t.start()
			threads.append(t)

		# Now we wait for all of the threads to finish, and retrieve their
		# results.
		for t in threads:
			t.join()
			out.extend(t.result)
	else:
		out = aggregate(data, 0, len(data), timestep_ratio, agg_func
		, progress_callback)

	n_end = len(out)
	m = "Temporal aggregation reduced array length by %d values"
	log_debug(m % (n_start - n_end))

	return out

def find_units_conversion(current_units: str, desired_units: str) \
	-> Callable[[float], float]:
	"""
	Find a conversion between two different units. Throw if not found.
	The return value is a function which takes and returns a float.
	"""
	# units_conversions is a dict mapping unit combos to a conversion.
	# units_conversions: dict[tuple[str, str], Callable[[float],float]]
	combination = (current_units, desired_units)
	if combination in units_conversions:
		return units_conversions[combination]
	m = "No unit conversion exists from '%s' to '%s'"
	raise ValueError(m % (current_units, desired_units))

def fix_units(data: list[float], current_units: str, desired_units: str, \
	timestep: int, progress_callback: Callable[[float], None]) -> list[float]:
	"""
	Convert data to the units required for the output file.
	This will modify the existing array.

	@param data: Input data.
	@param current_units: The input units.
	@param desired_units: The output units.
	@param timestep: The input timestep length in minutes.
	@param progress_callback: Function for progress reporting.
	"""
	conversion = find_units_conversion(current_units, desired_units)
	n = len(data)
	timestep *= SECONDS_PER_MINUTE
	for i in range(n):
		data[i] = conversion(data[i], timestep)
		if i % PROGRESS_CHUNK_SIZE == 0:
			progress_callback(i / n)
	return data

def units_match(unit0: str, unit1: str) -> str:
	"""
	Check if the two units are equivalent.
	E.g. m/s and ms^-1 would return true, but not m and kg.
	"""
	for case in units_synonyms:
		if unit0 in case and unit1 in case:
			return True
	return False

def read_data(variable: Variable, progress_callback: Callable[[float], None]) \
	-> list[float]:
	"""
	Read all data for a variable from the .nc input file.
	"""
	arr = [float] * variable.size
	n = len(arr)
	for i in range(0, n, CHUNK_SIZE):
		lower = i
		upper = i + CHUNK_SIZE
		arr[lower:upper] = variable[lower:upper]
		progress_callback(i / n)
	return arr

def get_start_minute(in_file: Dataset):
	"""
	Determine the minute at which the dataset starts (0-59).
	This is done by checking the time_coverage_start attribute of the dataset.
	"""
	start_time = in_file.time_coverage_start
	# yyyy-MM-dd hh:mm:ss
	pattern = r"\d{4}-\d{1,2}-\d{1,2} \d{1,2}:(\d{1,2}):\d{1,2}"
	matches = re.findall(pattern, start_time)
	if len(matches) < 1:
		m = "Unable to parse start time; expected yyyy-MM-dd hh:mm:ss format, but was '%s'"
		raise ValueError(m % in_file.time_coverage_start)
	return int(matches[0])

def change_start_minute(datetime: str, minute: int) -> str:
	"""
	Change the minute value of a date/time string.
	"""
	if minute < 0 or minute > 59:
		m = "Unable to set start minute: minute value %d must be in range [0, 59]"
		raise ValueError(m % minute)

	pattern = r"(\d{4}-\d{1,2}-\d{1,2} \d{1,2}:)\d{1,2}(:\d{1,2})"
	matches = re.findall(pattern, datetime)
	if len(matches) < 2:
		m = "Invalid datetime format; expected yyyy-MM-dd hh:mm:ss format, but was '%s'"
		raise ValueError(m % datetime)
	return "%s%d%s" % (matches[0], minute, matches[1])

def is_start_of_year(date: datetime.datetime) -> bool:
	"""
	Check if a given datetime object represents the start of a year.
	"""
	return date.second == 0 \
	and date.minute == 0 \
	and date.hour == 0 \
	and date.day == 1 \
	and date.month == 1

def get_next_year(start_date: datetime.datetime) -> datetime.datetime:
	"""
	Get a date representing the first valid date time in the first year
	on or after the given date. So if start_date lies on the exact start
	of a year, start_date will be returned. Otherwise the first day of
	the next year will be returned.
	"""
	if is_start_of_year(start_date):
		m = "get_next_year(): Start date %s lies on start of year"
		log_debug(m % start_date)
		return start_date
	return datetime.datetime(start_date.year + 1, 1, 1, 0, 0, 0)

def trim_to_start_year(in_file: Dataset, timestep: int
	, data: list[float]) -> list[float]:
	"""
	Trim the given data to the start of the next year.
	"""
	start_date = ozflux_common.parse_date(in_file.time_coverage_start)
	if (start_date.minute == 30):
		# temporal_aggregation() trims the first value if it lies on the
		# half-hour boundary. We need to do the same here.
		log_debug("Data starts on 30m boundary: 1st value is ignored")
		start_date = start_date + datetime.timedelta(minutes = 30)
	next_year = get_next_year(start_date)

	delta = next_year - start_date
	hours_per_timestep = timestep / MINUTES_PER_HOUR
	seconds_per_timestep = SECONDS_PER_HOUR * hours_per_timestep
	n_trim = int(delta.total_seconds() / seconds_per_timestep)
	m = "Trimming %d values to get to start of %d"
	log_debug(m % (n_trim, next_year.year))
	return data[n_trim:]

def remove_nans(data: list[float]
	, progress_cb: Callable[[float], None]) -> list[float]:
	"""
	Replace any NaN in the list with a mean of nearby values.
	"""
	n = len(data)
	N_NEIGHBOUR = 5
	for i in range(n):
		if data[i].mask:
			# Use mean of 5 closest values if value is nan.
			x = ozflux_common.neighbouring_mean(data, i, N_NEIGHBOUR)
			m = "Replacing NaN at %d with mean %.2f (n = %d)"
			log_debug(m % (i, x, N_NEIGHBOUR))
			data[i] = x
		if i % PROGRESS_CHUNK_SIZE == 0:
			progress_cb(i / n)
	return data

def bounds_checks(data: list[float], xmin: float, xmax: float) -> list[float]:
	"""
	Bounds checking. Any values in the list which exceed theses bounds will be
	set to the boundary value.

	@param data: The data to be checked.
	@param xmin: Lower boundary.
	@param xmax: Upper boundary.
	"""
	for i in range(len(data)):
		if data[i] < xmin:
			m = "Value %.2f in row %d exceeds lower bound of %.2f"
			log_debug(m % (data[i], i, xmin))
			data[i] = xmin
		elif data[i] > xmax:
			m = "Value %.2f in row %d exceeds upper bound of %.2f"
			log_debug(m % (data[i], i, xmax))
			data[i] = xmax
	return data

def get_data(in_file: Dataset, forcing: Forcing \
	, output_timestep: int \
	, progress_cb: Callable[[float], None]) -> list[float]:
		"""
		Get all data from the input file and convert it to a format
		suitable for the output file.
		"""
		# 0. Get variable ID from variable name.
		in_name = indata_forcing_names[forcing]
		out_name = outdata_forcing_names[forcing]

		# Some variables are not translated directly into the output
		# file and can be ignored (ie filled with zeroes).
		if in_name == "zero":
			log_diagnostic("Using zeroes for %s" % out_name)
			input_timestep = int(in_file.time_step)
			timestep_ratio = output_timestep // input_timestep
			n = in_file.variables["time"].size // timestep_ratio
			data = zeroes(n)
			return trim_to_start_year(in_file, output_timestep, data)
		if not in_name in in_file.variables:
			m = "Variable %s does not exist in input file"
			raise ValueError(m % in_name)

		var_id = in_file.variables[in_name]
		in_units = get_units(var_id)
		out_units = forcing_units[forcing]
		matching_units = units_match(in_units, out_units)

		# About 1/3 of time to fix units (if units need fixing).
		units_time_proportion = 0 if matching_units else 1 / 3

		# About 2/3 of remaining time to read the data from input file.
		read_time_proportion = 0.05

		# 5% of time to remove NaNs. Need to check this.
		fixnan_time_proportion = 0.05

		# Rest of time is spent aggregating over the timestep.
		aggregation_time_proportion = 1 \
			- read_time_proportion \
			- units_time_proportion \
			- fixnan_time_proportion

		# 1. Read data from netcdf file.
		data = read_data(var_id, lambda p:progress_cb(read_time_proportion * p))
		log_debug("Successfully read %d values from netcdf" % len(data))

		# 2. Replace NaN with a mean of nearby values.
		log_diagnostic("Removing NaNs")
		step_start = read_time_proportion
		step_size = fixnan_time_proportion
		data = remove_nans(data
		, lambda p: progress_cb(step_start + step_size * p))
		step_start += fixnan_time_proportion

		# 2. Change timestep to something suitable for lpj-guess.
		log_diagnostic("Aggregating %s to hourly timestep" % out_name)
		data = temporal_aggregation(in_file, data, forcing, output_timestep
		, lambda p: progress_cb(step_start + aggregation_time_proportion * p))
		step_start += aggregation_time_proportion

		# 3. Trim data to the start of a year.
		data = trim_to_start_year(in_file, output_timestep, data)

		# 4. Ensure units are correct.
		if not matching_units:
			log_diagnostic("Converting %s from %s to %s" % \
				(out_name, in_units, out_units))
			data = fix_units(data, in_units, out_units, output_timestep, \
				lambda p: progress_cb( \
					step_start + units_time_proportion * p))
		else:
			log_diagnostic("No units conversion required for %s" % out_name)

		data = bounds_checks(data, *forcing_bounds[forcing])

		# Done!
		return data

def process_file(in_file: str, out_file: str, timestep: int, \
	progress_callback: Callable[[float], None]):
	"""
	Read the input file and generate an output file at the specified
	path in LPJ-Guess (lsminput) format.

	@param in_file: Input file path
	@param out_file: Output file path
	@param timestep: Output timestep length in minutes.
	"""
	log_information("Processing %s..." % in_file)
	log_debug("Opening output file %s for writing" % out_file)
	with Dataset(out_file, "w", format=ozflux_common.NC_FORMAT) as nc_out:
		log_debug("Opening input file %s for reading" % in_file)
		with Dataset(in_file, "r", format=ozflux_common.NC_FORMAT) as nc_in:
			# Quick sanity check of time step.
			instep = int(nc_in.time_step)
			if instep > timestep:
				m = "Invalid input timestep (%d). Must be <= output (%d)"
				raise ValueError(m % instep, timestep)
			if timestep % instep != 0:
				m = "Invalid input timestep: %d. Must be an integer multiple of output timestep (%d)"
				raise ValueError(m % (instep, timestep))

			# 0. Create dimensions in output file.
			log_debug("Creating output dimensions")
			nc_out.createDimension(OUT_DIM_NAME_FORCINGS, Forcing.NFORCINGS)
			nc_out.createDimension(OUT_DIM_NAME_TIME, None)

			# 1. Create variables in output file.
			log_debug("Creating output variable")
			out_variable = nc_out.createVariable( \
				OUT_VARIABLE_FORCING_NAME \
				, OUT_VARIABLE_FORCING_FORMAT \
				, (OUT_DIM_NAME_TIME,OUT_DIM_NAME_FORCINGS))

			# 2. Copy data into this variable.
			log_debug("Migrating data")
			copy_data(nc_in, out_variable, timestep, progress_callback)

			# 3. Metadata for output file.
			nc_out.time_step = str(timestep) # in minutes.
			start_date = get_next_year(
				ozflux_common.parse_date(nc_in.time_coverage_start))
			nc_out.time_coverage_start = start_date.strftime(
				ozflux_common.DATE_FORMAT)
			nc_out.time_coverage_end = nc_in.time_coverage_end
			nc_out.latitude = nc_in.latitude
			nc_out.longitude = nc_in.longitude
			site_name = os.path.basename(in_file)
			site_name = os.path.splitext(site_name)[0]
			nc_out.site_name = site_name
			for forcing in Forcing:
				if forcing == Forcing.NFORCINGS:
					break
				oname = outdata_forcing_names[forcing]
				units = forcing_units[forcing]
				attr_name = "col_%d_%s_units" % (forcing, oname)
				setattr(nc_out, attr_name, units)

def parse_args(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Formatting ozflux data into a format suitable for consumption by LPJ-Guess")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default 3)", nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.INFORMATION)
	parser.add_argument("files", nargs = "+", help = "Input .nc files to be processed")
	parser.add_argument("-o", "--out-dir", required = True, help = "Path to the output directory. Processed files will be saved with the same file name into this directory")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	parser.add_argument("-P", "--parallel", action = "store_true", help = "Process files in parallel (highly experimental, use at own risk)")
	parser.add_argument("-t", "--timestep", type = int, required = True, help = "Output timestep in minutes")
	parser.add_argument("--version", action = "version", version = "%(prog)s " + VERSION)

	parsed = parser.parse_args(argv[1:])
	return Options(parsed.verbosity, parsed.files, parsed.out_dir
	, parsed.show_progress, parsed.parallel, parsed.timestep)

def main(opts: Options):
	"""
	Main CLI entrypoint function.

	@param opts: Object containing parsed CLI arguments.
	"""
	# Create output directory if it doesn't exist.
	if not os.path.exists(opts.out_dir):
		os.makedirs(opts.out_dir)

	# overall_progress holds the progress [0, 1] of processing each
	# file.
	global overall_progress
	overall_progress = [0.0] * len(opts.files)

	# Relative time spent in each file (based on file size).
	file_weightings = [os.path.getsize(f) for f in opts.files]
	total_weight = sum(file_weightings)
	file_weightings = [x / total_weight for x in file_weightings]

	# These are only required in parallel mode.
	threads = []
	mutex = multiprocessing.BoundedSemaphore(1)

	def progress_reporter(progress: float, file_idx: int):
		"""
		Locally-scoped function which is called periodically by in order
		to report the progress of processing a particular file.

		@param progress: Progress of the file processing in range [0, 1].
		@param file_idx: Index of the file being processed.
		"""
		# Shortcut to avoid unnecessary computations.
		if not opts.report_progress:
			return

		global overall_progress

		weighted_progress = file_weightings[file_idx] * progress

		# No need to check if in parallel mode, as the mutex is easily
		# obtained when running in serial mode.
		with mutex:
			overall_progress[file_idx] = weighted_progress
			aggregate_progress = sum(overall_progress) / sum(file_weightings)
			log_progress(aggregate_progress)

	class ProcessingThread(threading.Thread):
		def __init__(self, infile: str, outfile: str, idx: int):
			threading.Thread.__init__(self)
			self.index = idx
			self.infile = infile
			self.outfile = outfile
		def run(self):
			process_file(self.infile, self.outfile, lambda p: progress_reporter(p, self.index))

	for i in range(len(opts.files)):
		infile = opts.files[i]
		outfile = os.path.join(opts.out_dir, get_output_filename(infile))

		if opts.parallel:
			# Run task in a background thread.
			t = ProcessingThread(infile, outfile, i)
			t.start()
			threads.append(t)
		else:
			# Run task in this thread.
			process_file(infile, outfile, opts.timestep
			, lambda p: progress_reporter(p, i))

	# Wait for threads to exit.
	for t in threads:
		t.join()

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(argv)

	set_log_level(opts.log_level)
	set_show_progress(opts.report_progress)

	try:
		# Actual logic is in main().
		main(opts)
	except BaseException as error:
		# Basic error handling.
		log_error(traceback.format_exc())
		exit(1)
