#!/usr/bin/env python
from argparse import ArgumentParser
import os, re, time, pandas
import ozflux_dave, ozflux_netcdf, ozflux_parallel
import traceback
from ozflux_logging import *
from netCDF4 import Dataset, date2num
from sys import argv
from typing import Callable
from ozflux_netcdf import *
from ozflux_common import *
from ozflux_dave import get_dailygrass_vars, get_ameriflux_vars
import ozflux_lsm

class Options:
	"""
	Class for storing CLI arguments from the user.

	@param log: Log level.
	@param files: Input files.
	@param odir: Output directory.
	@param prog: True to write progress messages, 0 otherwise.
	@param parallel: True to process files in parallel.
	@param timestep: Desired output timestep, in minutes.
	@param compression_level: Compression quality for output file [0, 9]. 0 = none, 1 = fastest compression, largest filesize, 9 = slowest compression, smallest filesize.
	@param compression_type: Compression algorithm to be used (default 'zlib').
	@param single_outfile: Iff true, inputs will be written to a single output file.
	@param ameriflux: True to treat input files as ameriflux files (e.g. for checking variable names).
	"""
	def __init__(self, log : LogLevel, files: list[str], odir: str, \
		prog: bool, parallel: bool, timestep: int, out_type: LpjVersion,
		compression_level: int, compression_type: str, single_outfile: bool,
		ameriflux: bool):
		self.log_level = log
		self.files = files
		self.out_dir = odir
		self.report_progress = prog
		self.parallel = parallel
		self.timestep = timestep
		self.compression_level = compression_level
		self.compression_type = compression_type
		self.out_type = LpjVersion(out_type)
		self.single_outfile = single_outfile
		self.ameriflux = ameriflux

class MetProcessingTask(ozflux_parallel.Task):
	def __init__(self, infile: str, outfile: str, timestep: int
	      , out_type: LpjVersion, compression_level: int
		  , compression_type: str, ngridcell: int
		  , first_date: datetime.datetime):
		self.infile = infile
		self.outfile = outfile
		self.timestep = timestep
		self.out_type = out_type
		self.compression_level = compression_level
		self.compression_type = compression_type
		self.ngridcell = ngridcell
		self.first_date = first_date

	def exec(self, pcb: Callable[[float], None]):
		try:
			process_file(self.infile, self.outfile, self.timestep
			, self.out_type, self.compression_level, self.compression_type
			, self.ngridcell, self.first_date, pcb)
		except BaseException as err:
			raise ValueError(f"Failed to process {self.infile}") from err

def six_digit_string(x: float) -> str:
	"""
	Return a number, encoded as a 6 digit string.
	"""
	# Round x to 2 digits.
	res = str(abs(int(round(x, 2) * 1000)))
	pad = "0" * (6 - len(res))
	return "%s%s" % (pad, res)

def get_output_filename(infile: str) -> str:
	"""
	Get the expected output file name for the given input .nc file.
	"""
	with Dataset(infile, "r", format=NC_FORMAT) as nc:
		# Read latitude/longitude from input .nc file.
		lon = float(nc.longitude)
		lat = float(nc.latitude)

		return get_met_filename(lon, lat)

def write_data_dg(nc_out: Dataset, var_name: str, data: list[float], lon: float
	, lat: float, toffset: int, progress_cb: Callable[[float], None]):
	"""
	Write data to the output file (dailygrass mode).

	@param variable: Variable in the output file to which data will be written.
	@param data: A timeseries of data for a variable for a gridcell.
	@param lon: Longitude of the data.
	@param lat: Latitude of the data.
	@param toffset: Offset of the data on the time dimension.
	@param progress_cb: Called to report progress.
	"""
	n = len(data)
	chunk_size = get_steps_per_year(opts.timestep)

	(ilon, ilat) = get_coord_indices(nc_out, lon, lat)

	try:
		for i in range(0, n, chunk_size):
			upper = min(n, i + chunk_size)
			# Dimension order is lat,lon,time
			tlow = i + toffset
			thigh = upper + toffset
			nc_out.variables[var_name][ilat, ilon, tlow:thigh] = data[i:upper]
			progress_cb(upper / n)
	except BaseException as err:
		raise ValueError(f"Failed to proces {var_name}") from err

def change_start_minute(datetime: str, minute: int) -> str:
	"""
	Change the minute value of a date/time string.

	@param datetime: Date/time string in format yyyy-MM-dd hh:mm:ss
	@param minute: The new value of the minute field.
	"""
	if minute < 0 or minute > 59:
		m = "Unable to set start minute: minute value %d must be in range [0, 59]"
		raise ValueError(m % minute)

	pattern = r"(\d{4}-\d{1,2}-\d{1,2} \d{1,2}:)\d{1,2}(:\d{1,2})"
	matches = re.findall(pattern, datetime)
	if len(matches) < 2:
		m = "Invalid datetime format; expected yyyy-MM-dd hh:mm:ss format, but was '%s'"
		raise ValueError(m % datetime)
	return "%s%d%s" % (matches[0], minute, matches[1])

def get_vars(version: LpjVersion, timestep: int) -> list[ForcingVariable]:
	"""
	Get the list of variables to be copied to the output file.

	@param version: The target lpj-guess version.
	@param timestep: Output timestep in minutes.
	"""
	if version == LpjVersion.LSM:
		return ozflux_lsm.get_lsm_vars()
	if version == LpjVersion.DAILY_GRASS:
		if (opts.ameriflux):
			return get_ameriflux_vars(timestep)
		return get_dailygrass_vars(timestep)
	raise ValueError("Unknown output mode: %d" % version)

def copy_data_lsm(in_file: Dataset, out_file: Dataset, timestep: int, \
	pcb: Callable[[float], None]):
	"""
	Copy all data from the input file to the output variable.

	@param input file: Opened .nc file.
	@param variable: The newly-created and initialised forcing variable.
	@param timestep: Desired output timestep in minutes.
	@param pcb: Progress reporting function.
	"""
	# 0. Get variable IDs.
	i = 0
	vars = ozflux_lsm.get_lsm_vars()
	nvar = len(vars)
	n = len(vars)
	for var in vars:
		log_diagnostic("----- %s -----" % var.out_name)
		log_diagnostic("Reading %s data" % var.out_name)
		read_proportion = 0.66
		write_proportion = 1 - read_proportion

		data = get_data(in_file, var, timestep, False, True, lambda p: \
			pcb((i + p * read_proportion) / nvar))

		# 3. Write data to out file.
		start = i + read_proportion
		step = write_proportion
		log_diagnostic("Writing %s data" % var.out_name)
		reporter = lambda p: pcb((start + step * p) / n)
		ozflux_lsm.write_data(out_file, data, var, reporter)
		i += 1

def get_coord(nc: Dataset, attr: str, std_name: str):
	if hasattr(nc, attr):
		return float(getattr(nc, attr))
	dim = get_dim_from_std_name(nc, std_name)
	if dim.size > 1:
		raise ValueError(f"Input file has multiple ({len(dim)}) {std_name} values")
	if dim.size < 1:
		raise ValueError(f"Input file has 0 {std_name} values")
	if not dim.name in nc.variables:
		raise ValueError(f"Variable not found: {dim.name}")
	return nc.variables[dim.name][0]

def get_longitude(nc: Dataset) -> float:
	return get_coord(nc, "longitude", STD_LON)

def get_latitude(nc: Dataset) -> float:
	return get_coord(nc, "latitude", STD_LAT)

def write_data(nc_in: Dataset, nc_out: Dataset, data: list[float] \
	, var: ForcingVariable, version: LpjVersion, toffset: int
	, pcb: Callable[[float], None]):
	"""
	Write the data to the output file.

	@param nc: The output netcdf file.
	@param data: The data to be written.
	@param var: The variable being copied.
	@param version: The lpj-guess version selected by the user.
	@param toffset: Offset of the data on the time dimension.
	@param pcb: Progress callback function.
	"""
	if version == LpjVersion.LSM:
		ozflux_lsm.write_data(nc_out, data, var, pcb)
	elif version == LpjVersion.DAILY_GRASS:
		lon = get_longitude(nc_in)
		lat = get_latitude(nc_in)
		write_data_dg(nc_out, var.out_name, data, lon, lat, toffset, pcb)
	else:
		raise ValueError("Unknown lpj version %d" % version)

def copy_data(nc_in: Dataset, nc_out: Dataset, timestep: int
	, version: LpjVersion, first_date: datetime.datetime
	, pcb: Callable[[float], None]):
	"""
	Copy data to the output file in dailygrass format.

	@param nc_in: Input NetCDF file.
	@param nc_out: Output NetCDF file.
	@param timestep: Output timestep length in minutes.
	@param version: Output lpj-guess version selected by the user.
	@param first_date: Date of first datum in output file.
	@param progress_cb: Progress callback function.
	"""
	i = 0

	# Variables to be added to output file.
	variables = get_vars(version, timestep)

	# 95% of time to read
	global global_read_prop, global_write_prop

	nvar = len(variables)

	# Determine offset along the time dimension in the output file.
	tstart = get_first_date(nc_in)
	toffset = (tstart - first_date).days * HOURS_PER_DAY

	n = 0
	for var in variables:
		if opts.parallel:
			global cd_tp_lock
			cd_tp_lock.acquire()
		read_prop = global_read_prop
		write_prop = global_write_prop
		if opts.parallel:
			cd_tp_lock.release()

		log_diagnostic("----- %s -----" % var.in_name)
		log_diagnostic("Reading %s data" % var.in_name)

		reporter = lambda p: pcb( (i + p * read_prop) / nvar)
		read_start = time.time()
		data = get_data(nc_in, var, timestep, False, True, reporter)
		read_time = time.time() - read_start

		n = max(n, len(data))

		log_diagnostic("Writing %s data" % var.in_name)
		reporter = lambda p: pcb( (i + read_prop + write_prop * p) / nvar)
		write_start = time.time()
		write_data(nc_in, nc_out, data, var, version, toffset, reporter)
		write_time = time.time() - write_start

		# Bookkeeping.
		total_time = read_time + write_time
		if opts.parallel:
			cd_tp_lock.acquire()
		global_read_prop = read_time / total_time
		global_write_prop = write_time / total_time
		log_debug("Read: %.2f%%, Write: = %.2f%%" % \
			(global_read_prop, global_write_prop))
		if opts.parallel:
			cd_tp_lock.release()

		# Copy a few attributes.f
		in_var = nc_in.variables[var.in_name]
		out_var = nc_out.variables[var.out_name]

		out_var.units = var.out_units
		if 'standard_name' in in_var.ncattrs():
			out_var.standard_name = in_var.standard_name
		else:
			out_var.standard_name = var.in_name
		out_var.long_name = in_var.long_name

		i += 1

	# Copy data into time variable. This is # of hours since start date.
	# So it starts at the offset from the baseline date, and increments by
	# timestep width (in hours). Data is written in chunks to improve speed.

	# First we need to get the offset since the baseline date. For that, we need
	# to know the start date of the output data.
	start_date = get_first_date(nc_in)
	if (start_date.minute == 30):
		# temporal_aggregation() trims the first value if it lies on the
		# half-hour boundary. We need to do the same here.
		log_debug("Data starts on 30m boundary: 1st value is ignored")
		start_date = start_date + datetime.timedelta(minutes = 30)
	start_date = get_next_year(start_date)

	step_width = opts.timestep / MINUTES_PER_HOUR
	dlt = datetime.timedelta(hours = step_width)
	dates = pandas.date_range(start_date, periods = n, freq = dlt)
	dates = [d.to_pydatetime() for d in dates]

	# TODO: this probably won't work correctly with --single-outfile option, but
	# using date2num() really is the only rational way of handling this.
	var_time = nc_out.variables[VAR_TIME]
	var_time.units = f"hours since {DATE_BASELINE}"
	var_time.calendar = CALENDAR_GREGORIAN
	values = date2num(dates, var_time.units, var_time.calendar)

	for i in range(toffset, n, CHUNK_SIZE):
		up = min(n, i + CHUNK_SIZE)
		tlow = i + toffset
		thigh = up + toffset
		var_time[tlow:thigh] = values[i:up]

def create_dimensions(nc: Dataset, version: LpjVersion, ngridcell: int):
	"""
	Create the required dimensions in the output file.
	"""
	log_debug("Creating output dimensions")
	if (version == LpjVersion.LSM):
		ozflux_lsm.create_dimensions(nc)
	else:
		ozflux_dave.create_dimensions(nc, ngridcell)

	# Time dimension is needed in both LSM and daily_grass mode.
	create_dim_if_not_exists(nc, DIM_TIME)

def create_variables(nc: Dataset, version: LpjVersion, timestep: int
	, clevel: int, ctype: str):
	"""
	Create the required variables in the output file.

	@param nc: The output .nc file.
	@param version: The lpj-guses version selected by the user.
	@param timestep: Output timestep in minutes.
	@param clevel: Compression level [1-9], 9 = slowest/smallest.
	@param ctype: Compression algorithm to be used (or None).
	"""
	log_debug("Creating output variables")
	if (version == LpjVersion.LSM):
		ozflux_lsm.create_variables(nc)
	else:
		# I've created a separate function for the dailygrass variables to
		# improve readability in this function.
		vars = get_vars(version, timestep)
		ozflux_dave.create_dailygrass_variables(nc, timestep, vars, clevel, ctype)

def prepare_outfile(nc: Dataset, version: LpjVersion, ngridcell: int
	, timestep: int, clevel: int, ctype: str):
	"""
	Create the variables in the output NetCDF file. The variables created
	will depend to some degree on the output type.

	@param nc: Output .nc file.
	@param version: Output lpj-guess version selected by the user.
	@param ngridcell: The number of gridcells to be stored in this output file.
	@param timestep: Output timestep in minutes.
	@param clevel: Compression level [1-9], 9 = slowest/smallest.
	@param ctype: Compression algorithm to be used (or None).
	"""
	create_dimensions(nc, version, ngridcell)
	create_variables(nc, version, timestep, clevel, ctype)

def open_outfile(file: str) -> Dataset:
	log_debug("Opening output file %s for writing" % file)
	return Dataset(file, "r+", format = NC_FORMAT)

def process_files(in_files: list[str], out_file: str, timestep: int, out_type: \
	LpjVersion, pcb: Callable[[float], None]):
	"""
	Read the input files and write all data to the specified output file.

	@param in_files: Input files.
	@param out_file: Output file path.
	@param timestep: Output timestep in minutes.
	@param out_type: Target lpj-guess branch (LSM or daily-grass).
	@param pcb: Progress reporting function.
	"""
	read_prop = 0.9
	write_prop = 1 - read_prop
	with open_outfile(out_file) as nc_out:
		...

def write_metadata(nc: Dataset, timestep: int, version: LpjVersion):
	"""
	Write version-specific metadata.

	@param nc: Output .nc file.
	@param timestep: Output timestep in minutes.
	@param version: Output version selected by the user.
	"""
	ozflux_netcdf.write_common_metadata(nc, timestep)

	if version == LpjVersion.DAILY_GRASS:
		ozflux_dave.write_metadata(nc)
	elif version == LpjVersion.LSM:
		ozflux_lsm.write_metadata(nc)
	else:
		raise ValueError("Unknown output version %d" % version)

def get_timestep(nc: Dataset) -> int:
	"""
	Get the timestep of the specified file in minutes.

	@param nc: The netcdf file.
	"""
	# The ozflux method.
	ATTR_TIMESTEP = "time_ste"
	if hasattr(nc, ATTR_TIMESTEP):
		return getattr(nc, ATTR_TIMESTEP)

	# The fallback.
	time = nc.variables[VAR_TIME]
	times = time[0:2]
	dates = num2date(times, time.units, time.calendar
		, only_use_cftime_datetimes = False, only_use_python_datetimes = True)
	timestep: datetime.timedelta = dates[1] - dates[0]
	timestep.min
	return timestep.total_seconds() // 60

def process_file(in_file: str, out_file: str, timestep: int \
	, version: LpjVersion, clevel: int, ctype: str, ngridcell
	, first_date: datetime.datetime
	, pcb: Callable[[float], None]):
	"""
	Read the input file and generate an output file at the specified
	path in LPJ-Guess (lsminput) format.

	@param in_file: Input file path.
	@param out_file: Output file path.
	@param timestep: Output timestep length in minutes.
	@param version: Output version selected by the user.
	@param clevel: Compression level [1-9], 9 = slowest/smallest.
	@param ctype: Compression algorithm to be used (or None).
	@param ngridcell: Total number of gridcells to be processed.
	@param first_date: Date of first datum in output file.
	@param pcb: Progress callback function.
	"""
	log_information("Processing %s..." % in_file)
	with open_outfile(out_file) as nc_out:
		log_debug("Opening input file %s for reading" % in_file)
		with Dataset(in_file, "r", format=NC_FORMAT) as nc_in:
			# Quick sanity check of time step.
			instep = get_timestep(nc_in)
			if instep > timestep:
				m = "Invalid input timestep (%d). Must be <= output (%d)"
				raise ValueError(m % instep, timestep)
			if timestep % instep != 0:
				m = "Invalid input timestep: %d. Must be an integer multiple of output timestep (%d)"
				raise ValueError(m % (instep, timestep))

			prepare_outfile(nc_out, version, ngridcell, timestep, clevel, ctype)

			# Copy data into this variable.
			log_debug("Migrating data")
			copy_data(nc_in, nc_out, timestep, version, first_date, pcb)

			# Write site metadata.
			start_date = get_next_year(get_first_date(nc_in))
			attr_tstart = "time_coverage_start"
			attr_tend = "time_coverage_end"
			attr_lon = "longitude"
			attr_lat = "latitude"
			if opts.single_outfile:
				site_short = get_site_name_from_filename(in_file)
				attr_tstart = "%s_%s" % (attr_tstart, site_short)
				attr_tend = "%s_%s" % (attr_tend, site_short)
				attr_lon = "%s_%s" % (attr_lon, site_short)
				attr_lat = "%s_%s" % (attr_lat, site_short)

			setattr(nc_out, attr_tstart, start_date.strftime(DATE_FORMAT))
			setattr(nc_out, attr_tend, get_last_date(nc_out).strftime(DATE_FORMAT))
			setattr(nc_out, attr_lat, get_latitude(nc_in))
			setattr(nc_out, attr_lon, get_longitude(nc_out))

			if not opts.single_outfile:
				site_name = os.path.basename(in_file)
				site_name = os.path.splitext(site_name)[0]
				nc_out.site_name = site_name

			# Write additional global/common metadata.
			write_metadata(nc_out, timestep, opts.out_type)

def get_outfile_name(infile: str, opts: Options):
	"""
	Get the name of the output file corresponding to the given input file.
	@param infile: Name of the input file for which we need an output filename.
	@param opts: CLI options.
	"""
	if opts.single_outfile:
		return "combined.nc"
	if opts.out_type == LpjVersion.LSM:
		return get_output_filename(infile)
	return os.path.basename(infile)

def parse_args(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Formatting ozflux data into a format suitable for consumption by LPJ-Guess")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default 3)", nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.INFORMATION)
	parser.add_argument("files", nargs = "+", help = "Input .nc files to be processed")
	parser.add_argument("-o", "--out-dir", required = True, help = "Path to the output directory. Processed files will be saved with the same file name into this directory")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	parser.add_argument("-P", "--parallel", action = "store_true", help = "Process files in parallel")
	parser.add_argument("-t", "--timestep", type = int, required = True, help = "Output timestep in minutes")
	parser.add_argument("-c", "--output-compatibility", type = int, default = 0, help = "Output compatibility type; ie the LPJ-Guess input module to be used (0 = dave (ie NC input module), 1 = LSM) (default: 0)")
	parser.add_argument("--compression-level", type = int, nargs = "?", default = 4, help = "Compression quality for output file [0, 9]. 0 = none, 1 = fastest compression, largest filesize, 9 = slowest compression, smallest filesize (default 4)")
	parser.add_argument("--compression-type", default = "zlib", help = "Compression algorithm to be used (default 'zlib')")
	parser.add_argument("-s", "--single-outfile", action = "store_true", help = "Combine all inputs into a single output file")
	parser.add_argument("-a", "--ameriflux", action = "store_true", help = "Treat input files as ameriflux files (e.g. for checking variable names).")
	parser.add_argument("--version", action = "version", version = "%(prog)s " + VERSION)

	parsed = parser.parse_args(argv[1:])
	return Options(parsed.verbosity, parsed.files, parsed.out_dir
	, parsed.show_progress, parsed.parallel, parsed.timestep
	, parsed.output_compatibility, parsed.compression_level
	, parsed.compression_type, parsed.single_outfile, parsed.ameriflux)

def read_first_date(nc: Dataset) -> datetime.datetime:
	"""
	Read the first date within the netcdf file and return it as a python
	datetime object.

	@param nc: The netcdf file.
	"""
	# Assuming the name of the time variable is time...
	# Assuming the variable is one-dimensional...
	time = nc.variables[VAR_TIME]
	return num2date(time[0], time.units, time.calendar, only_use_cftime_datetimes = False, only_use_python_datetimes = True)

def get_first_date(nc: Dataset) -> datetime.datetime:
	"""
	Determine the first date of the output data which will be produced from
	the given input file.

	@param nc: Opened netcdf dataset.
	"""
	# Ozflux method.
	ATTR_TIME_START = "time_coverage_start"
	start_date: datetime
	if hasattr(nc, ATTR_TIME_START):
		# Ozflux sites provide an attribute. It would be better to not rely on
		# this but I'm not going to refactor this at this point.
		start_date = parse_date(getattr(nc, ATTR_TIME_START))
	else:
		# The fallback is to read the first time value from the input file.
		start_date = read_first_date(nc)

	if (start_date.minute == 30):
		# temporal_aggregation() trims the first value if it lies on the
		# half-hour boundary. We need to do the same here.
		log_debug("Data starts on 30m boundary: 1st value is ignored")
		start_date = start_date + datetime.timedelta(minutes = 30)
	start_date = get_next_year(start_date)
	return start_date

def get_last_date(nc: Dataset) -> datetime.datetime:
	"""
	Get the last date in the given file.
	"""
	time = nc.variables[VAR_TIME]
	value = time[len(time) - 1]
	return num2date(value, time.units, time.calendar
		, only_use_cftime_datetimes = False, only_use_python_datetimes = True)

def get_file_first_date(ncfile: str):
	"""
	Determine the first date of the output data which will be produced from
	the given input file.

	@param ncfile: Input netcdf file path.
	"""
	with Dataset(ncfile, "r", format=NC_FORMAT) as nc_in:
		return get_first_date(nc_in)

# @profile
def main(opts: Options):
	"""
	Main CLI entrypoint function.

	@param opts: Object containing parsed CLI arguments.
	"""
	# Create output directory if it doesn't exist.
	if not os.path.exists(opts.out_dir):
		os.makedirs(opts.out_dir)

	job_manager = ozflux_parallel.JobManager()

	first_date = datetime.datetime.max
	if opts.single_outfile:
		for infile in opts.files:
			first_date = min(first_date, get_file_first_date(infile))

	ngridcell = len(opts.files) if opts.single_outfile else 1
	for infile in opts.files:
		if not opts.single_outfile:
			first_date = get_file_first_date(infile)

		out_file_name = get_outfile_name(infile, opts)

		outfile = os.path.join(opts.out_dir, out_file_name)
		if os.path.exists(outfile):
			os.remove(outfile)

		job = MetProcessingTask(infile, outfile, opts.timestep, opts.out_type
			  , opts.compression_level, opts.compression_type, ngridcell
			  , first_date)

		job_manager.add_job(job, os.path.getsize(infile))

	if opts.parallel:
		job_manager.run_parallel()
	else:
		job_manager.run_single_threaded()

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(argv)

	set_log_level(opts.log_level)
	set_show_progress(opts.report_progress)

	try:
		# Actual logic is in main().
		main(opts)
	except BaseException as error:
		# Basic error handling.
		log_error(traceback.format_exc())
		exit(1)
