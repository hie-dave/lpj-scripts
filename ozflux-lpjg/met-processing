#!/usr/bin/env python
from argparse import ArgumentParser
import datetime, numpy, os, ozflux_common, re, threading, time
import multiprocessing, multiprocessing.connection
import traceback, sys
from ozflux_logging import *
from netCDF4 import Dataset, Variable
from enum import IntEnum
from ozflux_common import Forcing
from sys import argv
from typing import Callable

# Semantic version number.
VERSION = "1.0"

# Progress updates inside long loops are written every N iterations.
PROGRESS_CHUNK_SIZE = 1024

# Inputs/outputs will be read/written in chunks of this size.
# Increasing this will improve performance but increase memory usage.
CHUNK_SIZE = 16384

# Whether to aggregate timesteps in paralllel. This doesn't save a huge amount
# of time in the files that I've tested (which are fairly small), but it doesn't
# hurt performance, and should really help with larger files. I don't really see
# any reason to set this to false, which is why this isn't a CLI option.
PARALLEL_AGGREGATION = True

# If this is true, the latitude/longitude dimensions will be created unlimited
# in size. This is not recommended, as it slows down reading of the NetCDF. When
# this is false, the dimensions will have exactly the length required, which is
# the number of input files. (This assumes 1 grid point per input file.)
UNLIMITED_DIMS = False

#######################################################################
# Constants related to .nc format/processing
#######################################################################

# Name of the single variable created in the output file.
OUT_VARIABLE_LSM = "forcing_data"

# Data format in output file.
FORMAT_FLOAT = "f8"

# Name of the forcings dimension in the output file.
OUT_DIM_NAME_FORCINGS = "forcings"

# Name of the time dimension in the output file.
OUT_DIM_NAME_TIME = "time"

# Name of the longitude dimension in the output file.
OUT_DIM_NAME_LON = "longitude"

# Name of the latitude dimension in the output file.
OUT_DIM_NAME_LAT = "latitude"

# Name of the mean temperature variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_TEMP = "tav"

# Name of the insol variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_INSOL = "insol"

# Name of the prec variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_PREC = "prec"

# Name of the co2 variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_CO2 = "co2"

# Name of the VPD variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_VPD = "vpd"

# Name of the tmax variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_TMAX = "tmax"

# Name of the tmin variable created in the dailygrass output file.
DG_OUT_VARIABLE_NAME_TMIN = "tmin"

# Variable names for the output file.
outdata_forcing_names = {
	Forcing.SWDOWN: "swdown",
	Forcing.PARDF: "pardf",
	Forcing.PARDR: "pardr",
	Forcing.LWDOWN: "lwdown",
	Forcing.PRECLS: "precls",
	Forcing.PRECCO: "precco",
	Forcing.TAIR: "tair",
	Forcing.UAIR: "uair",
	Forcing.VAIR: "vair",
	Forcing.QAIR: "qair",
	Forcing.PSURF: "psurf"
}

# Variable names for the ozflux input file.
indata_forcing_names = {
	Forcing.SWDOWN: "Fsd",
	Forcing.PARDF: "zero",
	Forcing.PARDR: "zero",
	Forcing.LWDOWN: "Fld",
	Forcing.PRECLS: "Precip",
	Forcing.PRECCO: "zero",
	Forcing.TAIR: "Ta",
	Forcing.UAIR: "Ws",
	Forcing.VAIR: "zero",
	Forcing.QAIR: "SH",
	Forcing.PSURF: "ps"
}

# From DMB:
# > Radiation (SWDOWN, PARDF, PARDR, LWDOWN)-> W/m^2
# > Precipitation (PRECLS, PRECCO) -> kg/m^2/s
# > Air temperature (TAIR) -> K
# > Wind speed (UAIR, VAIR) -> m/s (u = eastward, v = northward)
# > Air specific humidity (QAIR) -> kg/kg (unitless)
# > Pressure (PSURF) -> Pa
forcing_units = {
	Forcing.SWDOWN: "W/m2",
	Forcing.PARDF: "W/m2",
	Forcing.PARDR: "W/m2",
	Forcing.LWDOWN: "W/m2",
	Forcing.PRECLS: "kg/m2/s",
	Forcing.PRECCO: "kg/m2/s",
	Forcing.TAIR: "K",
	Forcing.UAIR: "m/s",
	Forcing.VAIR: "m/s",
	Forcing.QAIR: "",
	Forcing.PSURF: "Pa"
}

# Min and max values for each forcing variable, in output units.
forcing_bounds = {
	Forcing.SWDOWN: (0, 1e5),
	Forcing.PARDF: (0, 1e5),
	Forcing.PARDR: (0, 1e5),
	Forcing.LWDOWN: (0, 1e5),
	Forcing.PRECLS: (0, 1e4),
	Forcing.PRECCO: (0, 1e3), # Always zero, not used.
	Forcing.TAIR: (200, 373), # Kelvins
	Forcing.UAIR: (0, 100),
	Forcing.VAIR: (0, 1), # Always zero, not used.
	Forcing.QAIR: (0, 0.2),
	Forcing.PSURF: (3e4, 1.5e5)
}

# Common alternative names for various units which we may reasonably encounter.
units_synonyms = [
	["W/m2", "W/m^2", "Wm^-2"],
	["kg/m2/s", "kg/m^2/s", "kgm^-2s^-1"],
	["K", "k"],
	["m/s", "ms^-1"],
	["Pa", "pa"],
	["kg/kg", "", "mm/mm", "m/m"], # debatable
	["ppm", "umol/mol"],
	["degC", "°C", "degrees C"]
]

# These are the functions used to aggregate data temporally, when converting
# from one timestep to another.
indata_aggregators = {
	Forcing.SWDOWN: numpy.mean,
	Forcing.PARDF: numpy.mean,
	Forcing.PARDR: numpy.mean,
	Forcing.LWDOWN: numpy.mean,
	Forcing.PRECLS: numpy.sum, # precip in mm
	Forcing.PRECCO: numpy.sum, # precip in mm
	Forcing.TAIR: numpy.mean,
	Forcing.UAIR: numpy.mean,
	Forcing.VAIR: numpy.mean,
	Forcing.QAIR: numpy.mean,
	Forcing.PSURF: numpy.mean,
}

# g to kg. This is not really used except as an example.
G_TO_KG = 1e-3

# KPa to Pa
KPA_TO_PA = 1e3

# °C to K
DEG_C_TO_K = 273.15

# Number of seconds per minute.
SECONDS_PER_MINUTE = 60

# Number of minutes per hour.
MINUTES_PER_HOUR = 60

# Number of seconds per hour.
SECONDS_PER_HOUR = SECONDS_PER_MINUTE * MINUTES_PER_HOUR

# Number of hours per day.
HOURS_PER_DAY = 24

# Number of days per year.
DAYS_PER_YEAR = 365 # ha

# Recipes for unit conversions.
#
# The keys are a tuple of two strings:
#
# 1. The input units
# 2. The output units.
#
# The values are functions which convert a value from the input units into the
# output units. These functions take two arguments:
#
# x: The input value in input units.
# t: The timestep length (in seconds).
#
# Note that units are changed after timestep aggregation occurs. So the input
# variable will already be in the output timestep at this point in time.
units_conversions = {
	("g", "kg"): lambda x, _: x * G_TO_KG, # (as an example)
	("mm", "kg/m2/s"): lambda x, t: x / t, # Assuming mm means mm per timestep
	("kg/m2/s", "mm"): lambda x, t: t * x,
	("degC", "K"): lambda x, _: x + DEG_C_TO_K,
	("K", "degC"): lambda x, _: x - DEG_C_TO_K,
	("kPa", "Pa"): lambda x, _: x * KPA_TO_PA
}

#######################################################################
# End of .nc constants
#######################################################################

# Global estimates of how much time it takes to perform various tasks (as a
# proportion of total time spent in get_data() function [0, 1]). These get
# updated during program execution and are used for progress reporting.
get_data_read_prop = 0.5
get_data_fixnan_prop = 0.05
get_data_t_agg_prop = 0.35
get_data_unit_prop = 0.05
get_data_bounds_prop = 0.05

# Global time estimates for reading vs writing data, as a proportion of time
# spent reading (which includes aggregation, unit conversion, etc) vs writing.
global_read_prop = 0.95
global_write_prop = 0.05

# Global mutex
mutex = multiprocessing.BoundedSemaphore(1)

# get_data() time proportion lock. Used to synchronise access to the
# get_data_*_prop variables.
gd_tp_lock = multiprocessing.BoundedSemaphore(1)

# copy_data_*() time proportion lock. Used to synchronise access to the
# global_*_prop variables.
cd_tp_lock = multiprocessing.BoundedSemaphore(1)

class Options:
	"""
	Class for storing CLI arguments from the user.

	@param log: Log level.
	@param files: Input files.
	@param odir: Output directory.
	@param prog: True to write progress messages, 0 otherwise.
	@param parallel: True to process files in parallel.
	@param timestep: Desired output timestep, in minutes.
	@param compression_level: Compression quality for output file [0, 9]. 0 = none, 1 = fastest compression, largest filesize, 9 = slowest compression, smallest filesize.
	@param compression_type: Compression algorithm to be used (default 'zlib').
	"""
	def __init__(self, log : LogLevel, files: list[str], odir: str, \
		prog: bool, parallel: bool, timestep: int, out_type: OutputType,
		compression_level: int, compression_type: str):
		self.log_level = log
		self.files = files
		self.out_dir = odir
		self.report_progress = prog
		self.parallel = parallel
		self.timestep = timestep
		self.compression_level = compression_level
		self.compression_type = compression_type
		self.out_type = OutputType(out_type)

class ForcingVariable():
	def __init__(self, in_name: str, out_name: str, out_units: str
		, aggregator: Callable[[list[float]], float], lbound: float
		, ubound: float):
		self.in_name = in_name
		self.out_name = out_name
		self.out_units = out_units
		self.aggregator = aggregator
		self.lbound = lbound
		self.ubound = ubound
	def new(forcing: Forcing, out_name: str, out_units: str):
		return ForcingVariable(indata_forcing_names[forcing]
			, out_name
			, out_units
			, indata_aggregators[forcing]
			, forcing_bounds[forcing][0]
			, forcing_bounds[forcing][1])

def six_digit_string(x: float) -> str:
	"""
	Return a number, encoded as a 6 digit string.
	"""
	# Round x to 2 digits.
	res = str(abs(int(round(x, 2) * 1000)))
	pad = "0" * (6 - len(res))
	return "%s%s" % (pad, res)

def get_output_filename(infile: str) -> str:
	"""
	Get the expected output file name for the given input .nc file.
	"""
	with Dataset(infile, "r", format=ozflux_common.NC_FORMAT) as nc:
		# Read latitude/longitude from input .nc file.
		lon = float(nc.longitude)
		lat = float(nc.latitude)

		return ozflux_common.get_met_filename(lon, lat)

def get_units(var_id: Variable) -> str:
	"""
	Get the units for the specified variable in the .nc file.
	"""
	return var_id.units

def write_data_lsm(variable: Variable, data: list[float], forcing: Forcing, \
	progress_callback: Callable[[float], None]):
	"""
	Write data to the output file (LSM mode).

	@param variable: Variable in the output file, to which data will be written.
	@param data: A column of data to be written.
	@param forcing: Column of the variable to which the data will be written.
	@progress_callback: Called to report progress.
	"""
	n = len(data)

	# Always use at least this many chunks.
	MIN_NUM_CHUNKS = 10

	chunk_size = min(CHUNK_SIZE, n / MIN_NUM_CHUNKS)
	for i in range(0, n, chunk_size):
		row = i
		upper = min(n, i + chunk_size)
		col = forcing
		variable[row:upper, col] = data[i:upper]
		progress_callback(i / n)

	log_debug("Successfully wrote %d items of %s" % (n, forcing))

def floats_equal(x: float, y: float) -> bool:
	"""
	Check if two floating point numbers are equal.

	@param x: The first number.
	@param y: The second number.
	"""
	EPSILON = 1e-10
	return abs(x - y) < EPSILON

def index_of(xarr: list[float], x: float) -> int:
	"""
	Return the index of x in the given list, or -1 if not found.

	@param xarr: A list of floats.
	@param x: The value for which to search.
	"""
	for i in range(0, len(xarr)):
		if floats_equal(x, xarr[i]):
			return i
	return -1

def get_coord_indices(nc_out: Dataset, lon: float, lat: float) \
	-> tuple[float, float]:
	"""
	Get the indices of the longitude and latitude, adding them to the file if
	they do not already exist. The return value is a tuple of
	(longitud_index, latitude_index).

	@param nc_out: The output NetCDF file.
	@param lon: Longitude.
	@param lat: Latitude.
	"""
	var_lon = nc_out.variables[OUT_DIM_NAME_LON]
	var_lat = nc_out.variables[OUT_DIM_NAME_LAT]
	index_lon = index_of(var_lon, lon)
	if index_lon == -1:
		if nc_out.dimensions[OUT_DIM_NAME_LON].isunlimited():
			index_lon = len(var_lon)
		else:
			index_lon = numpy.ma.flatnotmasked_edges(var_lon)[0]
		var_lon[index_lon] = lon
	index_lat = index_of(var_lat, lat)
	if index_lat == -1:
		if nc_out.dimensions[OUT_DIM_NAME_LAT].isunlimited():
			index_lat = len(var_lat)
		else:
			index_lat = numpy.ma.flatnotmasked_edges(var_lat)[0]
		var_lat[index_lat] = lat
	return (index_lon, index_lat)

def write_data_dg(nc_out: Dataset, var_name: str, data: list[float], lon: float
	, lat: float, progress_cb: Callable[[float], None]):
	"""
	Write data to the output file (dailygrass mode).

	@param variable: Variable in the output file to which data will be written.
	@param data: A timeseries of data for a variable for a gridcell.
	@param lon: Longitude of the data.
	@param lat: Latitude of the data.
	@param progress_cb: Called to report progress.
	"""
	n = len(data)
	chunk_size = get_steps_per_year(opts.timestep)
	for i in range(0, n, chunk_size):
		upper = min(n, i + chunk_size)
		(ilon, ilat) = get_coord_indices(nc_out, lon, lat)
		# Dimension order is time,lat,lon
		nc_out.variables[var_name][i:upper, ilat, ilon] = data[i:upper]
		progress_cb((i + chunk_size) / n)

def zeroes(n: int) -> list[float]:
	"""
	Create an array of zeroes of the specified length.
	"""
	return [0.0] * n

def aggregate(data: list[float], start: int, stop: int, chunk_size: int \
	, aggregator: Callable[[list[float]], float] \
	, progress_cb: Callable[[float], None]) -> list[float]:
	"""
	Aggregate the values in the specified dataset between the start and
	stop indices, using the specified aggregator function.

	@param data: The data.
	@param start: Aggregate only values after this index.
	@param stop: Aggregate only values before this index.
	@param chunk_size: Aggregate values in chunks of this size.
	@param aggregator: A function to aggregate a chunk of values.
	@param progress_cb: A function which handles progress reporting.
	"""
	n = len(data)
	if start < 0:
		raise ValueError("start cannot be less than 0 (is %d)" % start)
	if stop > n:
		m = "stop cannot be past end of list (is %d and n = %d)"
		raise ValueError(m % (stop, n))
	if start >= stop:
		m = "start cannot be greater than stop (start = %d, stop = %d)"
		raise ValueError(m % (start, stop))

	niter = ((stop - start) // chunk_size)
	result = [0.0] * niter
	for i in range(niter):
		lower = start + i * chunk_size
		upper = min(n, start + (i + 1) * chunk_size)
		result[i] = aggregator(data[lower:upper])
		if i % PROGRESS_CHUNK_SIZE == 0:
			progress_cb(i / niter)
	return result

def temporal_aggregation(in_file: Dataset, data: list[float] \
	, output_timestep: float, aggregator: Callable[[list[float]], float]
	, progress_callback: Callable[[float], None]) -> list[float]:
	"""
	Aggregate the input data to destination timestep.

	@param in_file: Input netcdf file.
	@param data: Data to be aggregated.
	@param forcing: The variable represented by the data.
	@param output_timestep: The desired output timestep.
	@param aggregator: The function used to aggregate values (typically mean or sum).
	@param progress_callback: Function used for progress reporting.
	"""
	# todo: support other timesteps (both source and target)?
	input_timestep = int(in_file.time_step)
	if input_timestep == output_timestep:
		return data
	if output_timestep < input_timestep:
		m = "Invalid output timestep (%d); must be >= input timestep (%d)"
		raise ValueError(m % (output_timestep, input_timestep))
	if output_timestep % input_timestep != 0:
		m = "Invalid timestep; output (%d) must be an integer multiple of input\
timestep (%d)"
		raise ValueError(m % (output_timestep, input_timestep))

	# Record the amount of data at the start of the function. This is
	# written as a diagnostic later.
	n_start = len(data)

	start_minute = get_start_minute(in_file)
	if start_minute == 30 and input_timestep == 30:
		# Don't include the first timestep value if it lies on the half-
		# hour boundary. trim_to_start_year() relies on this assumption,
		# so it will need to be updated if we change this and vice versa.
		m = "Input data starts on 30m boundary. First value will be removed."
		log_debug(m)
		data = data[1:]
	elif start_minute != 0:
		m = "Strange start time: '%s'. Data must start on the hour or half-hour"
		raise ValueError(m % in_file.time_coverage_start)

	# Guaranteed to be an integer division, given the above error checks.
	timestep_ratio = output_timestep // input_timestep

	out: list[float]
	out = []

	if PARALLEL_AGGREGATION:
		# This can be quite slow for large datasets, so I've parallelised it.

		# Get number of CPUs.
		ncpu = multiprocessing.cpu_count()

		# Number of timesteps to be processed by each thread.
		chunk_size = len(data) // ncpu

		# As we're aggregating every 2 timesteps together, we need to ensure
		# that each thread is processing an even number of values.
		if chunk_size % 2 == 1:
			chunk_size += 1

		global thread_progress
		thread_progress = [0.0] * ncpu
		def update_progress(progress: float, tid: int):
			global thread_progress
			global mutex
			with mutex:
				thread_progress[tid] = progress
				progress_callback(numpy.mean(thread_progress))

		class Aggregator(threading.Thread):
			def __init__(self, data: list[float], start: int, stop: int
				, chunk_size: int, tid: int
				, aggregation_func: Callable[[list[float]], float]):
				threading.Thread.__init__(self)
				self.data = data
				self.istart = start
				self.stop = stop
				self.chunk_size = chunk_size
				self.thread_id = tid
				self.aggregation_func = aggregation_func
			def run(self):
				self.result = aggregate(self.data, self.istart, self.stop
				, self.chunk_size, self.aggregation_func
				, lambda p: update_progress(p, self.thread_id))

		# Create an array to hold the thread objects.
		threads = []
		for i in range(ncpu):
			# Calculate start and stop indices for this thread. Each thread
			# is operating on the same input data array, but they operate
			# on different parts (slices) of the array.
			start = i * chunk_size
			stop = len(data) if i == ncpu - 1 else start + chunk_size

			# Create the thread object.
			t = Aggregator(data, start, stop, timestep_ratio, i, aggregator)

			# Start the thread and store the object reference for later.
			t.start()
			threads.append(t)

		# Now we wait for all of the threads to finish, and retrieve their
		# results.
		for t in threads:
			t.join()
			out.extend(t.result)
	else:
		out = aggregate(data, 0, len(data), timestep_ratio, aggregator
		, progress_callback)

	n_end = len(out)
	m = "Temporal aggregation reduced array length by %d values"
	log_debug(m % (n_start - n_end))

	return out

def find_units_conversion(current_units: str, desired_units: str) \
	-> Callable[[float], float]:
	"""
	Find a conversion between two different units. Throw if not found.
	The return value is a function which takes and returns a float.
	"""
	# units_conversions is a dict mapping unit combos to a conversion.
	# units_conversions: dict[tuple[str, str], Callable[[float],float]]
	combination = (current_units, desired_units)
	if combination in units_conversions:
		return units_conversions[combination]
	m = "No unit conversion exists from '%s' to '%s'"
	raise ValueError(m % (current_units, desired_units))

def fix_units(data: list[float], current_units: str, desired_units: str, \
	timestep: int, progress_callback: Callable[[float], None]) -> list[float]:
	"""
	Convert data to the units required for the output file.
	This will modify the existing array.

	@param data: Input data.
	@param current_units: The input units.
	@param desired_units: The output units.
	@param timestep: The input timestep length in minutes.
	@param progress_callback: Function for progress reporting.
	"""
	conversion = find_units_conversion(current_units, desired_units)
	n = len(data)
	timestep *= SECONDS_PER_MINUTE
	for i in range(n):
		data[i] = conversion(data[i], timestep)
		if i % PROGRESS_CHUNK_SIZE == 0:
			progress_callback(i / n)
	return data

def units_match(unit0: str, unit1: str) -> str:
	"""
	Check if the two units are equivalent.
	E.g. m/s and ms^-1 would return true, but not m and kg.
	"""
	if unit0 == unit1:
		return True

	for case in units_synonyms:
		if unit0 in case and unit1 in case:
			return True
	return False

def read_data(variable: Variable, progress_callback: Callable[[float], None]) \
	-> list[float]:
	"""
	Read all data for a variable from the .nc input file.
	"""
	arr = [float] * variable.size
	n = len(arr)
	for i in range(0, n, CHUNK_SIZE):
		lower = i
		upper = i + CHUNK_SIZE
		arr[lower:upper] = variable[lower:upper]
		progress_callback(i / n)
	return arr

def get_start_minute(in_file: Dataset):
	"""
	Determine the minute at which the dataset starts (0-59).
	This is done by checking the time_coverage_start attribute of the dataset.
	"""
	start_time = in_file.time_coverage_start
	# yyyy-MM-dd hh:mm:ss
	pattern = r"\d{4}-\d{1,2}-\d{1,2} \d{1,2}:(\d{1,2}):\d{1,2}"
	matches = re.findall(pattern, start_time)
	if len(matches) < 1:
		m = "Unable to parse start time; expected yyyy-MM-dd hh:mm:ss format, but was '%s'"
		raise ValueError(m % in_file.time_coverage_start)
	return int(matches[0])

def change_start_minute(datetime: str, minute: int) -> str:
	"""
	Change the minute value of a date/time string.
	"""
	if minute < 0 or minute > 59:
		m = "Unable to set start minute: minute value %d must be in range [0, 59]"
		raise ValueError(m % minute)

	pattern = r"(\d{4}-\d{1,2}-\d{1,2} \d{1,2}:)\d{1,2}(:\d{1,2})"
	matches = re.findall(pattern, datetime)
	if len(matches) < 2:
		m = "Invalid datetime format; expected yyyy-MM-dd hh:mm:ss format, but was '%s'"
		raise ValueError(m % datetime)
	return "%s%d%s" % (matches[0], minute, matches[1])

def is_start_of_year(date: datetime.datetime) -> bool:
	"""
	Check if a given datetime object represents the start of a year.
	"""
	return date.second == 0 \
	and date.minute == 0 \
	and date.hour == 0 \
	and date.day == 1 \
	and date.month == 1

def get_next_year(start_date: datetime.datetime) -> datetime.datetime:
	"""
	Get a date representing the first valid date time in the first year
	on or after the given date. So if start_date lies on the exact start
	of a year, start_date will be returned. Otherwise the first day of
	the next year will be returned.
	"""
	if is_start_of_year(start_date):
		m = "get_next_year(): Start date %s lies on start of year"
		log_debug(m % start_date)
		return start_date
	return datetime.datetime(start_date.year + 1, 1, 1, 0, 0, 0)

def trim_to_start_year(in_file: Dataset, timestep: int
	, data: list[float]) -> list[float]:
	"""
	Trim the given data to the start of the next year.
	"""
	start_date = ozflux_common.parse_date(in_file.time_coverage_start)
	if (start_date.minute == 30):
		# temporal_aggregation() trims the first value if it lies on the
		# half-hour boundary. We need to do the same here.
		log_debug("Data starts on 30m boundary: 1st value is ignored")
		start_date = start_date + datetime.timedelta(minutes = 30)
	next_year = get_next_year(start_date)

	delta = next_year - start_date
	hours_per_timestep = timestep / MINUTES_PER_HOUR
	seconds_per_timestep = SECONDS_PER_HOUR * hours_per_timestep
	n_trim = int(delta.total_seconds() / seconds_per_timestep)
	m = "Trimming %d values to get to start of %d"
	log_debug(m % (n_trim, next_year.year))
	return data[n_trim:]

def remove_nans(data: list[float]
	, progress_cb: Callable[[float], None]) -> list[float]:
	"""
	Replace any NaN in the list with a mean of nearby values.
	"""
	n = len(data)
	N_NEIGHBOUR = 5
	for i in range(n):
		if data[i].mask:
			# Use mean of 5 closest values if value is nan.
			x = ozflux_common.neighbouring_mean(data, i, N_NEIGHBOUR)
			m = "Replacing NaN at %d with mean %.2f (n = %d)"
			log_debug(m % (i, x, N_NEIGHBOUR))
			data[i] = x
		if i % PROGRESS_CHUNK_SIZE == 0:
			progress_cb(i / n)
	return data

def bounds_checks(data: list[float], xmin: float, xmax: float
	, progress_cb: Callable[[float], None]) -> list[float]:
	"""
	Bounds checking. Any values in the list which exceed theses bounds will be
	set to the boundary value.

	@param data: The data to be checked.
	@param xmin: Lower boundary.
	@param xmax: Upper boundary.
	@param progress_cb: Progress reporting function.
	"""
	n = len(data)
	for i in range(n):
		if data[i] < xmin:
			m = "Value %.2f in row %d exceeds lower bound of %.2f"
			log_debug(m % (data[i], i, xmin))
			data[i] = xmin
		elif data[i] > xmax:
			m = "Value %.2f in row %d exceeds upper bound of %.2f"
			log_debug(m % (data[i], i, xmax))
			data[i] = xmax
		if i % PROGRESS_CHUNK_SIZE == 0:
			progress_cb(i / n)
	return data

def get_data(in_file: Dataset \
	, in_name: str
	, out_name: str \
	, output_timestep: int \
	, in_units: str \
	, out_units: str \
	, aggregator: Callable[[list[float]], float] \
	, lower_bound: float \
	, upper_bound: float \
	, progress_cb: Callable[[float], None]) -> list[float]:
		"""
		Get all data from the input file and convert it to a format
		suitable for the output file.

		@param in_name: Name of the variable in the input file.
		@param out_name: Name of the variable in the output file.
		@param output_timestep: Output timestep width in minutes.
		"""
		# Some variables are not translated directly into the output
		# file and can be ignored (ie filled with zeroes).
		if in_name == "zero":
			log_diagnostic("Using zeroes for %s" % out_name)
			input_timestep = int(in_file.time_step)
			timestep_ratio = output_timestep // input_timestep
			n = in_file.variables["time"].size // timestep_ratio
			data = zeroes(n)
			return trim_to_start_year(in_file, output_timestep, data)
		if not in_name in in_file.variables:
			m = "Variable %s does not exist in input file"
			raise ValueError(m % in_name)

		var_id = in_file.variables[in_name]
		# in_units = get_units(var_id)
		# out_units = forcing_units[forcing]
		matching_units = units_match(in_units, out_units)

		global get_data_read_prop, get_data_fixnan_prop, get_data_t_agg_prop
		global get_data_unit_prop, get_data_bounds_prop

		# About 1/3 of time to fix units (if units need fixing).
		units_time_proportion = 0 if matching_units else get_data_unit_prop

		prop_tot = get_data_read_prop + get_data_fixnan_prop + \
			get_data_t_agg_prop + units_time_proportion + get_data_bounds_prop

		# About 2/3 of remaining time to read the data from input file.
		read_time_proportion = get_data_read_prop / prop_tot

		# 5% of time to remove NaNs. Need to check this.
		fixnan_time_proportion = get_data_fixnan_prop / prop_tot

		# Rest of time is spent aggregating over the timestep.
		aggregation_time_proportion = get_data_t_agg_prop / prop_tot

		bounds_time_prop = get_data_bounds_prop / prop_tot

		# Read data from netcdf file.
		read_start = time.time()
		data = read_data(var_id, lambda p:progress_cb(read_time_proportion * p))
		read_tot = time.time() - read_start
		log_debug("Successfully read %d values from netcdf" % len(data))

		# Replace NaN with a mean of nearby values.
		log_diagnostic("Removing NaNs")
		step_start = read_time_proportion
		step_size = fixnan_time_proportion
		fixnan_start = time.time()
		data = remove_nans(data
		, lambda p: progress_cb(step_start + step_size * p))
		step_start += fixnan_time_proportion
		fixnan_tot = time.time() - fixnan_start

		# Change timestep to something suitable for lpj-guess.
		log_diagnostic("Aggregating %s to hourly timestep" % out_name)
		t_agg_start = time.time()
		data = temporal_aggregation(in_file, data, output_timestep, aggregator
		, lambda p: progress_cb(step_start + aggregation_time_proportion * p))
		step_start += aggregation_time_proportion
		t_agg_tot = time.time() - t_agg_start

		# Trim data to the start of a year.
		data = trim_to_start_year(in_file, output_timestep, data)

		# Ensure units are correct.
		unit_start = time.time()
		if not matching_units:
			log_diagnostic("Converting %s from %s to %s" % \
				(out_name, in_units, out_units))
			data = fix_units(data, in_units, out_units, output_timestep, \
				lambda p: progress_cb( \
					step_start + units_time_proportion * p))
		else:
			log_diagnostic("No units conversion required for %s" % out_name)
		unit_tot = time.time() - unit_start

		step_start += units_time_proportion

		bounds_start = time.time()
		data = bounds_checks(data, lower_bound, upper_bound, lambda p: \
			progress_cb(step_start + bounds_time_prop * p))
		bounds_tot = time.time() - bounds_start

		time_tot = read_tot + fixnan_tot + t_agg_tot + bounds_tot + unit_tot
		read_prop = 100.0 * read_tot / time_tot
		fixnan_prop = 100.0 * fixnan_tot / time_tot
		t_agg_prop = 100.0 * t_agg_tot / time_tot
		bounds_prop = 100.0 * bounds_tot / time_tot
		unit_prop = 100.0 * unit_tot / time_tot

		# Update global time estimates of all activities.
		if opts.parallel:
			global gd_tp_lock
			gd_tp_lock.acquire()
		get_data_read_prop = read_prop
		get_data_fixnan_prop = fixnan_prop
		get_data_t_agg_prop = t_agg_prop
		get_data_bounds_prop = bounds_prop
		if not matching_units:
			get_data_unit_prop = unit_prop
		if opts.parallel:
			# global gd_tp_lock
			gd_tp_lock.release()

		# Done!
		return data

def copy_data_lsm(in_file: Dataset, variable: Variable, timestep: int, \
	progress_callback: Callable[[float], None]):
	"""
	Copy all data from the input file to the output variable.

	@param input file: Opened .nc file.
	@param variable: The newly-created and initialised forcing variable.
	@param timestep: Desired output timestep in minutes.
	@param progress_callback: Progress reporting function.
	"""
	# 0. Get variable IDs.
	i = 0
	for forcing in Forcing:
		if forcing == Forcing.NFORCINGS:
			break
		out_name = outdata_forcing_names[forcing]
		log_diagnostic("----- %s -----" % out_name)
		log_diagnostic("Reading %s data" % out_name)
		read_proportion = 0.66
		write_proportion = 1 - read_proportion
		
		data = get_data(in_file, indata_forcing_names[forcing]
		, outdata_forcing_names[forcing]
		, timestep
		, get_units(in_file.variables[indata_forcing_names[forcing]])
		, forcing_units[forcing]
		, indata_aggregators[forcing]
		*forcing_bounds[forcing]
		, lambda p: \
			progress_callback((i + p * read_proportion) / Forcing.NFORCINGS))

		# 3. Write data to out file.
		log_diagnostic("Writing %s data" % out_name)
		write_data_lsm(variable, data, forcing, lambda p: progress_callback( \
			(i + read_proportion + write_proportion * p) / Forcing.NFORCINGS))
		i += 1

def copy_data_dailygrass(nc_in: Dataset, nc_out: Dataset, timestep: int
	, progress_cb: Callable[[float], None]):
	"""
	Copy data to the output file in dailygrass format.

	@param nc_in: Input NetCDF file.
	@param nc_out: Output NetCDF file.
	@param timestep: Output timestep length in minutes.
	@param progress_cb: Progress callback function.
	"""
	i = 0

	# Output units for temperature variables.
	temp_out_units = "degC"

	# Lower and upper bounds for temperature variables.
	temp_min_degc = -100
	temp_max_degc = 100

	# Variables to be added to output file.
	variables = [
	ForcingVariable(indata_forcing_names[Forcing.TAIR]
		, DG_OUT_VARIABLE_NAME_TEMP, temp_out_units, numpy.mean, temp_min_degc
		, temp_max_degc)
	, ForcingVariable.new(Forcing.SWDOWN, DG_OUT_VARIABLE_NAME_INSOL, "W/m2")
	, ForcingVariable.new(Forcing.PRECLS, DG_OUT_VARIABLE_NAME_PREC, "mm")
	, ForcingVariable("CO2", DG_OUT_VARIABLE_NAME_CO2, "ppm", numpy.mean, 250
		, 900)
	, ForcingVariable("VPD", DG_OUT_VARIABLE_NAME_VPD, "kPa", numpy.mean, 0, 100)
	]

	step_width = opts.timestep / MINUTES_PER_HOUR

	# If generating a daily file, need to include tmax/tmin variables in output.
	if step_width == 24:
		name_tav = indata_forcing_names[Forcing.TAIR]
		variables.append(ForcingVariable(name_tav, DG_OUT_VARIABLE_NAME_TMAX, temp_out_units
			, numpy.amax, temp_min_degc, temp_max_degc))
		variables.append(ForcingVariable(name_tav, DG_OUT_VARIABLE_NAME_TMIN, temp_out_units
			, numpy.amin, temp_min_degc, temp_max_degc))

	# 95% of time to read
	global global_read_prop, global_write_prop

	lon = float(nc_in.longitude)
	lat = float(nc_in.latitude)

	n = 0
	for var in variables:
		if opts.parallel:
			global cd_tp_lock
			cd_tp_lock.acquire()
		read_prop = global_read_prop
		write_prop = global_write_prop
		if opts.parallel:
			cd_tp_lock.release()

		log_diagnostic("----- %s -----" % var.in_name)
		log_diagnostic("Reading %s data")
		in_units = get_units(nc_in.variables[var.in_name])

		read_start = time.time()
		data = get_data(nc_in, var.in_name, var.out_name, timestep, in_units \
		, var.out_units, var.aggregator, var.lbound, var.ubound \
		, lambda p: progress_cb( (i + p * read_prop) / len(variables)))
		read_time = time.time() - read_start

		n = max(n, len(data))

		log_diagnostic("Writing %s data")
		write_start = time.time()
		write_data_dg(nc_out, var.out_name, data, lon, lat, lambda p: \
		progress_cb( (i + read_prop + write_prop * p) / len(variables)))
		write_time = time.time() - write_start

		# Bookkeeping.
		total_time = read_time + write_time
		if opts.parallel:
			cd_tp_lock.acquire()
		global_read_prop = read_time / total_time
		global_write_prop = write_time / total_time
		log_debug("Read: %.2f%%, Write: = %.2f%%" % \
			(global_read_prop, global_write_prop))
		if opts.parallel:
			cd_tp_lock.release()

		# Copy a few attributes.f
		in_var = nc_in.variables[var.in_name]
		out_var = nc_out.variables[var.out_name]

		out_var.units = var.out_units
		if 'standard_name' in in_var.ncattrs():
			out_var.standard_name = in_var.standard_name
		else:
			out_var.standard_name = var.in_name
		out_var.long_name = in_var.long_name

		i += 1

	# Copy data into time variable. This is # of hours since start date.
	# So it starts at 0 and increments by timestep width (in hours).
	# Write data in chunks to speed things up a bit.
	timesteps = [i * step_width for i in range(n)]
	for i in range(0, n, CHUNK_SIZE):
		up = min(n, i + CHUNK_SIZE)
		nc_out.variables[OUT_DIM_NAME_TIME][i:up] = timesteps[i:up]

def get_steps_per_year(timestep: int) -> int:
	"""
	Calculate the number of timesteps in a year.
	"""
	step_width_hour = opts.timestep / MINUTES_PER_HOUR
	return int(HOURS_PER_DAY / step_width_hour * DAYS_PER_YEAR)

def create_dailygrass_variables(nc: Dataset):
	"""
	Create the variables required for dailygrass mode.

	@param nc: The output netcdf file.
	"""
	dims = (OUT_DIM_NAME_TIME, OUT_DIM_NAME_LAT, OUT_DIM_NAME_LON)
	chunksizes = (get_steps_per_year(opts.timestep), 1, 1)
	format = FORMAT_FLOAT

	create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_TEMP, format, dims, chunksizes)
	create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_INSOL, format, dims, chunksizes)
	create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_PREC, format, dims, chunksizes)
	create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_CO2, format, dims, chunksizes)
	create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_VPD, format, dims, chunksizes)

	# When generating daily outputs, we need to include tmax/tmin.
	if opts.timestep == 1440:
		create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_TMAX, format, dims, chunksizes)
		create_var_if_not_exists(nc, DG_OUT_VARIABLE_NAME_TMIN, format, dims, chunksizes)

def create_dim_if_not_exists(nc: Dataset, name: str, size: int = 0):
	"""
	Create an unlimited dimension in the NetCDF file if it does not already
	exist.

	@param nc: The NetCDF file.
	@param name: Name of the dimension.
	"""
	if not name in nc.dimensions:
		nc.createDimension(name, size)

def create_var_if_not_exists(nc: Dataset, name: str, format: str
	, dims: tuple[str], chunksizes: tuple[int] = None):
	"""
	Create a variable in the NetCDF file if it does not already exist.

	@param nc: The NetCDF file.
	@param name: Name of the variable.
	@param format: Format of the variable.
	@param dims: Dimensions of the variable.
	"""
	compression = None if opts.compression_level == 0 else opts.compression_type
	log_diagnostic("Compression = %s L%d"%(compression, opts.compression_level))
	if not name in nc.variables:
		nc.createVariable(name, format, dims, compression = compression
		, complevel = opts.compression_level
		, chunksizes = chunksizes)

def prepare_outfile(nc: Dataset, out_type: OutputType):
	"""
	Create the variables in the output NetCDF file. The variables created
	will depend to some degree on the output type.
	"""
	# 0. Create dimensions in output file.
	log_debug("Creating output dimensions")
	if (out_type == OutputType.LSM):
		# LSM mode needs a forcings dimension.
		nc.createDimension(OUT_DIM_NAME_FORCINGS, Forcing.NFORCINGS)
	else:
		# Dailygrass mode needs lon/lat dimensions (as it's gridded).
		# Chunk size for geographic (ie lat/lon) variables.
		geo_chunk_size = (1,)
		dim_size = 0 if UNLIMITED_DIMS else len(opts.files)
		create_dim_if_not_exists(nc, OUT_DIM_NAME_LON, dim_size)
		create_dim_if_not_exists(nc, OUT_DIM_NAME_LAT, dim_size)
		create_var_if_not_exists(nc, OUT_DIM_NAME_LON
			, FORMAT_FLOAT, (OUT_DIM_NAME_LON), geo_chunk_size)
		create_var_if_not_exists(nc, OUT_DIM_NAME_LAT
			, FORMAT_FLOAT, (OUT_DIM_NAME_LAT), geo_chunk_size)
		var_lon = nc.variables[OUT_DIM_NAME_LON]
		var_lat = nc.variables[OUT_DIM_NAME_LAT]
		var_lon.long_name = "longitude"
		var_lat.long_name = "latitude"
		var_lon.standard_name = "longitude"
		var_lat.standard_name = "latitude"
		var_lon.units = "degree_east"
		var_lat.units = "degree_north"

	# Time dimension is needed in both LSM and daily_grass mode.
	create_dim_if_not_exists(nc, OUT_DIM_NAME_TIME)

	if out_type == OutputType.DAILY_GRASS:
		create_var_if_not_exists(nc, OUT_DIM_NAME_TIME, FORMAT_FLOAT
			, (OUT_DIM_NAME_TIME), (get_steps_per_year(opts.timestep),))

	# 1. Create variables in output file.
	log_debug("Creating output variables")
	if (out_type == OutputType.LSM):
		# LSM mode only requires a single variable.
		nc.createVariable(OUT_VARIABLE_LSM \
		, FORMAT_FLOAT \
		, (OUT_DIM_NAME_TIME,OUT_DIM_NAME_FORCINGS), compression = 'zlib')
	else:
		# I've created a separate function for the dailygrass variables to
		# improve readability in this function.
		create_dailygrass_variables(nc)

def process_file(in_file: str, out_file: str, timestep: int \
	, out_type: OutputType, progress_callback: Callable[[float], None]):
	"""
	Read the input file and generate an output file at the specified
	path in LPJ-Guess (lsminput) format.

	@param in_file: Input file path
	@param out_file: Output file path
	@param timestep: Output timestep length in minutes.
	"""
	log_information("Processing %s..." % in_file)
	log_debug("Opening output file %s for writing" % out_file)
	with Dataset(out_file, "r+", format=ozflux_common.NC_FORMAT) as nc_out:
		log_debug("Opening input file %s for reading" % in_file)
		with Dataset(in_file, "r", format=ozflux_common.NC_FORMAT) as nc_in:
			# Quick sanity check of time step.
			instep = int(nc_in.time_step)
			if instep > timestep:
				m = "Invalid input timestep (%d). Must be <= output (%d)"
				raise ValueError(m % instep, timestep)
			if timestep % instep != 0:
				m = "Invalid input timestep: %d. Must be an integer multiple of output timestep (%d)"
				raise ValueError(m % (instep, timestep))

			prepare_outfile(nc_out, out_type)

			# 2. Copy data into this variable.
			log_debug("Migrating data")
			if out_type == OutputType.LSM:
				copy_data_lsm(nc_in, nc_out.variables[OUT_VARIABLE_LSM]
					, timestep, progress_callback)
			else:
				copy_data_dailygrass(nc_in, nc_out, timestep, progress_callback)

			# 3. Metadata for output file.
			nc_out.time_step = str(timestep) # in minutes.
			start_date = get_next_year(
				ozflux_common.parse_date(nc_in.time_coverage_start))
			nc_out.time_coverage_start = start_date.strftime(
				ozflux_common.DATE_FORMAT)
			nc_out.time_coverage_end = nc_in.time_coverage_end
			nc_out.latitude = nc_in.latitude
			nc_out.longitude = nc_in.longitude
			site_name = os.path.basename(in_file)
			site_name = os.path.splitext(site_name)[0]
			nc_out.site_name = site_name
			if opts.out_type == OutputType.DAILY_GRASS:
				var_time = nc_out.variables[OUT_DIM_NAME_TIME]
				var_time.calendar = "gregorian"
				var_time.long_name = OUT_DIM_NAME_TIME
				var_time.standard_name = OUT_DIM_NAME_TIME
				var_time.units = "hours since %s" % nc_out.time_coverage_start
			for forcing in Forcing:
				if forcing == Forcing.NFORCINGS:
					break
				oname = outdata_forcing_names[forcing]
				units = forcing_units[forcing]
				attr_name = "col_%d_%s_units" % (forcing, oname)
				setattr(nc_out, attr_name, units)

def parse_args(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Formatting ozflux data into a format suitable for consumption by LPJ-Guess")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default 3)", nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.INFORMATION)
	parser.add_argument("files", nargs = "+", help = "Input .nc files to be processed")
	parser.add_argument("-o", "--out-dir", required = True, help = "Path to the output directory. Processed files will be saved with the same file name into this directory")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	parser.add_argument("-P", "--parallel", action = "store_true", help = "Process files in parallel")
	parser.add_argument("-t", "--timestep", type = int, required = True, help = "Output timestep in minutes")
	parser.add_argument("-c", "--output-compatibility", type = int, required = True, help = "Output compatibility type (0 = daily_grass, 1 = LSM)")
	parser.add_argument("--compression-level", type = int, nargs = "?", default = 4, help = "Compression quality for output file [0, 9]. 0 = none, 1 = fastest compression, largest filesize, 9 = slowest compression, smallest filesize (default 4)")
	parser.add_argument("--compression-type", default = "zlib", help = "Compression algorithm to be used (default 'zlib')")
	parser.add_argument("--version", action = "version", version = "%(prog)s " + VERSION)

	parsed = parser.parse_args(argv[1:])
	return Options(parsed.verbosity, parsed.files, parsed.out_dir
	, parsed.show_progress, parsed.parallel, parsed.timestep
	, parsed.output_compatibility, parsed.compression_level
	, parsed.compression_type)

def main(opts: Options):
	"""
	Main CLI entrypoint function.

	@param opts: Object containing parsed CLI arguments.
	"""
	# Create output directory if it doesn't exist.
	if not os.path.exists(opts.out_dir):
		os.makedirs(opts.out_dir)

	# overall_progress holds the progress [0, 1] of processing each
	# file.
	global overall_progress
	overall_progress = [0.0] * len(opts.files)

	# Relative time spent in each file (based on file size).
	file_weightings = [os.path.getsize(f) for f in opts.files]
	total_weight = sum(file_weightings)
	file_weightings = [x / total_weight for x in file_weightings]

	# These are only required in parallel mode.
	processes = []
	mutex = multiprocessing.BoundedSemaphore(1)

	def progress_reporter(progress: float, file_idx: int):
		"""
		Locally-scoped function which is called periodically by in order
		to report the progress of processing a particular file.

		@param progress: Progress of the file processing in range [0, 1].
		@param file_idx: Index of the file being processed.
		"""
		# Shortcut to avoid unnecessary computations.
		if not opts.report_progress:
			return

		global overall_progress

		weighted_progress = file_weightings[file_idx] * progress

		# No need to check if in parallel mode, as the mutex is easily
		# obtained when running in serial mode.
		with mutex:
			overall_progress[file_idx] = weighted_progress
			aggregate_progress = sum(overall_progress) / sum(file_weightings)
			log_progress(aggregate_progress)

	class ProcessingTask(multiprocessing.Process):
		"""
		A sub-process which will process a single file, with process reporting
		via a pipe back to the main process.

		Using multiple processes because the hdf5 library is not threadsafe.
		"""
		def __init__(self, infile: str, outfile: str, idx: int, timestep: int
		, output_type: OutputType
		, progress_writer: multiprocessing.connection.Connection):
			multiprocessing.Process.__init__(self)
			self.index = idx
			self.infile = infile
			self.outfile = outfile
			self.timestep = timestep
			self.output_type = output_type
			self.progress_writer = progress_writer
		def run(self):
			# Do main processing.
			process_file(self.infile, self.outfile, self.timestep
			, self.output_type
			, lambda p: self.progress_writer.send((p, self.index)))

			# Close progress reporter pipe.
			self.progress_writer.close()

	readers: list[multiprocessing.connection.Connection] = []

	for i in range(len(opts.files)):
		infile = opts.files[i]
		out_file_name = os.path.basename(infile)
		if opts.out_type == OutputType.LSM:
			out_file_name = get_output_filename(infile)

		outfile = os.path.join(opts.out_dir, out_file_name)
		if os.path.exists(outfile):
			os.remove(outfile)

		if opts.parallel:
			# Run task in a background process (hdf5/netcdf is not thread-safe).

			# Create a pipe for 1-way communication (progress reporting).
			reader, writer = multiprocessing.connection.Pipe(duplex = False)
			# Put the read pipe handle into the readers list.
			readers.append(reader)

			# Start the process.
			p = ProcessingTask(infile, outfile, i, opts.timestep, opts.out_type
				, writer)
			p.start()

			# Close the writable end of the pipe now, to be sure that p is the
			# only process which owns a handle for it. This ensures that when p
			# closes its handle for the writable end, wait() will promptly
			# report the readable end as being ready.
			writer.close()

			# Store the process handle for later use.
			processes.append(p)
		else:
			# Run task in the current thread.
			process_file(infile, outfile, opts.timestep, opts.out_type
			, lambda p: progress_reporter(p, i))

	if opts.parallel:
		while readers:
			for reader in readers:
				try:
					msg = None
					if reader.poll():
						msg = reader.recv()
				except EOFError:
					readers.remove(reader)
				else:
					if msg is not None:
						(progress, process_index) = msg
						progress_reporter(progress, process_index)

		# Wait for processes to exit (actually, they should have all finished by
		# the time we get to here).
		for p in processes:
			p.join()

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(argv)

	set_log_level(opts.log_level)
	set_show_progress(opts.report_progress)

	try:
		# Actual logic is in main().
		main(opts)
	except BaseException as error:
		# Basic error handling.
		log_error(traceback.format_exc())
		exit(1)
