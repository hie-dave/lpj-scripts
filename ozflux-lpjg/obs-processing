#!/usr/bin/env python3
#
# This script reads the raw .nc files and produces csv files with observations
# used in benchmarking.
#

#%pip install openpyxl numpy pandas

import math
import biom_processing, datetime, glob, numpy, pandas, time, traceback
import ozflux_common

from argparse import ArgumentParser
from ozflux_logging import *
from ozflux_netcdf import *
from sys import argv
from netCDF4 import Dataset
from typing import Callable

_ATTR_HEIGHT = "height"

# Total soil profile depth in m.
_SOIL_DEPTH = 1.5

# Name of the soil water column in the output file.
_NAME_SW = "swmm"

# Name of the "site name" column in the LAI data file.
_LAI_COL_ID = "ID"

# Name of the date column in the LAI data file.
_LAI_COL_DATE = "Date"

# Name of the LAI column in the LAI data file.
_LAI_COL_LAI = "MYD15A2H_006_Lai_500m"

# Name of the default sheet name in the LAI data file.
_SHEET_LAI = "OzFlux-sites-LAI-MYD15A2H-006-r"

# Standard variables in the ozflux files which will be used for benchmarking.
_standard_variables = [
	# GPP shouldn't get this high, but this is mainly for error checking anyway.
	ForcingVariable("GPP_LT", "gpp", "kgC/m2/day", numpy.mean, 0, 1e6),

	# todo: are these bounds sensible?
	ForcingVariable("ET", "et", "kg/m2/s", numpy.mean, -10, 10),
	ForcingVariable("ER_LT", "resp", "kgC/m2/day", numpy.mean, -10, 10),
	ForcingVariable("NEE_LT", "nee", "kgC/m2/day", numpy.mean, -10, 10),
]

# Map of site names to a list of names of soil water layer variables in the flux
# data file.
_layer_variable_map = {
	# fixme - this is only 5cm!
	"AdelaideRiver": ["Sws"],
	# fixme - no data for 30-60cm or 80-100cm!
	"AliceSpringsMulga": ["Sws_mulga", "Sws_10cm_mulga_2b", "Sws_60cm_mulga_2", "Sws_100cm_mulga_2"],
	# todo: what is the difference between Sws_05cma_N, Sws_05cma_S, Sws_05cmb_N, Sws_05cmb_S???
	"Boyagin": ["Sws_05cma_N", "Sws_08cm_main", "Sws_10cma_N", "Sws_15cm_main", "Sws_20cma_N", "Sws_40cm_N", "Sws_80cm_N", "Sws_110cm_N"],
	# Note: Otway only has top 15cm.
	"Otway": ["Sws"],
	# Note: Samford only has top 10cm.
	"Samford": ["Sws"],
	"SturtPlains": ["Sws", "Sws_50cm"],
	"Yanco": ["Sws_3cm", "Sws", "Sws_15cm", "Sws_45cm", "Sws_75cm"],
}

class Observation:
	"""
	An observation made on a particular date.
	"""
	def __init__(self, date: datetime.datetime, value: float):
		self.date = date
		self.value = value

class Options:
	"""
	Class for storing CLI arguments from the user.

	@param log: Log level.
	@param files: Input files.
	@param odir: Output directory.
	@param prog: True to write progress messages, 0 otherwise.
	@param parallel: True to process files in parallel.
	@param timestep: Desired output timestep, in hours.
	@param biomass_file: Optional file containing biomass readings
	@param biomass_searchpath: Search path for biomass readings. If provided, will search under here for a file containing biomass readings for the site
	@param lai_file: Path to file containing LAI observations.
	"""
	def __init__(self, log : LogLevel, files: list[str], odir: str, prog: bool,
		parallel: bool, timestep: int, biomass_file: str
		, biomass_searchpath: str, lai_file: str):
		self.log_level = log
		self.files = files
		self.out_dir = odir
		self.report_progress = prog
		self.parallel = parallel
		self.timestep = timestep
		self.biomass_file = biomass_file
		self.biomass_searchpath = biomass_searchpath
		self.lai_file = lai_file

def parse_args(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Formatting ozflux data into a format suitable for consumption by LPJ-Guess")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default 3)", nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.INFORMATION)
	parser.add_argument("files", nargs = "+", help = "Input .nc files to be processed")
	parser.add_argument("-o", "--out-dir", required = True, help = "Path to the output directory. Processed files will be saved with the same file name into this directory")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	parser.add_argument("-P", "--parallel", action = "store_true", help = "Process files in parallel")
	parser.add_argument("-t", "--timestep", type = int, required = True, help = "Output timestep in hours")
	parser.add_argument("--version", action = "version", version = "%(prog)s " + ozflux_common.VERSION)
	parser.add_argument("-l", "--lai-file", required = True, help = "Path to a file containing lai observations")

	biomass_group = parser.add_mutually_exclusive_group()
	biomass_group.add_argument("-b", "--biomass-readings", required = False, help = "Optional file containing biomass readings")
	biomass_group.add_argument("-s", "--biomass-searchpath", required = False, help = "Search path for biomass readings. If provided, will search under here for a file containing biomass readings for the site")

	p = parser.parse_args(argv[1:])

	return Options(p.verbosity, p.files, p.out_dir, p.show_progress, p.parallel,
		p.timestep, p.biomass_readings, p.biomass_searchpath, p.lai_file)

def write_csv(outfile: str, names: list[str], data: list[list[float]]
	, biomass_data: list[biom_processing.BiomassReading]
	, start_date: datetime.datetime, timestep: int \
	, lai_data: list[Observation], pcb: Callable[[float], None], delim = ","):
	"""
	Write the data to a csv file.

	@param outfile: Output file path.
	@param names: Output file column names.
	@param data: 2D data array where each element is a single column of data.
	@param start_date: First date in the data.
	@param timestep: Output timestep (in hours).
	@param lai_data: LAI observations. May be empty list if none available.
	"""
	if len(names) != len(data):
		raise ValueError("Number of column names (%d) doesn't match number of columns (%d)" % (len(names), len(data)))
	if len(names) == 0:
		log_warning("Writing 0 columns to output file")
		return

	nrow = len(data[0])
	for i in range(1, len(data)):
		if len(data[i]) != nrow:
			raise ValueError("Incorrect number of rows in column '%s'. Expected %d but was %d" % (names[i], nrow, len(data[i])))

	write_hours = timestep % 24 != 0

	delta = datetime.timedelta(hours = timestep)
	end_date = start_date + nrow * delta

	biomass_data = [b for b in biomass_data if b.date >= start_date and b.date <= end_date]
	biomass_data = sorted(biomass_data, key = lambda b: b.date)

	write_biomass = len(biomass_data) > 0
	write_lai = len(lai_data) > 0

	biomass_index = 0
	lai_index = next(i for i in range(len(lai_data)) if lai_data[i].date >= start_date)
	with open(outfile, "w") as csv:
		csv.write("year%sdoy%s" % (delim, delim))
		if write_hours:
			csv.write("hour%s" % delim)
		csv.write("%s" % str.join(delim, names))
		if write_biomass:
			csv.write(",live_biomass,dead_biomass")
		if write_lai:
			csv.write(",lai")
		csv.write("\n")
		date = start_date
		for i in range(nrow):
			if i % PROGRESS_CHUNK_SIZE == 0:
				pcb(i / nrow)
			# %j is day of year (1-366), but lpj-guess writes day of year as
			# (0-365), so we subtract one here to put it in the same "units".
			doy = str(int(date.strftime("%j")) - 1)
			csv.write("%s%s%s%s" % (date.year, delim, doy, delim))

			# Write the hour column.
			if write_hours:
				csv.write("%s%s" % (date.hour, delim))

			# Write the data columns.
			csv.write("%s" % str.join(delim, [str(column[i]) for column in data]))

			# Write the biomass column.
			if write_biomass:
				live_str = ""
				dead_str = ""
				if biomass_index < len(biomass_data):
					reading = biomass_data[biomass_index]
					if reading.date == date:
						biomass_index += 1
						if not math.isnan(reading.live):
							live_str = reading.live
						if not math.isnan(reading.dead):
							dead_str = reading.dead
				csv.write("%s%s%s%s" % (delim, live_str, delim, dead_str))

			if write_lai:
				lai_str = ""
				idx = [i for i in range(len(lai_data)) if lai_data[i].date == date]
				if lai_index < len(lai_data):
					reading = lai_data[lai_index]
					if reading.date == date:
						lai_index += 1
						if not math.isnan(reading.value):
							lai_str = reading.value
				csv.write("%s%s" % (delim, lai_str))

			csv.write("\n")
			date += delta
	pcb(1)

def get_biomass_data(nc_file: str, biom_file: str, biom_searchpath: str,
	pcb: Callable[[float], None]) \
	-> list[biom_processing.BiomassReading]:
	"""
	Attempt to read biomass data corresponding to the site specified by the .nc
	file name.

	@param nc_file: Input .nc file (may be used to determine site name).
	@param biom_file: -b CLI argument from user (or None if not given).
	@param biom_searchpath: -s CLI argument from user (or None if not given).
	"""
	if biom_file != None:
		raw = biom_processing.read_raw_data(biom_file)
		return biom_processing.read_biomass_data(raw, pcb)

	if biom_searchpath == None:
		log_warning("No biomass input data file was given. No biomass data will be included in output file.")
		return []

	# Determine site name, convert to lower case.
	site_name = get_site_name(nc_file)
	site_name = site_name.replace(" ", "_").lower()

	if "CumberlandPlain" in nc_file:
		site_name = "cumberland_plains"

	site_dir = "%s/%s" % (biom_searchpath, site_name)
	dir = site_dir if os.path.exists(site_dir) else biom_searchpath

	for file in glob.glob("%s/*.csv" % dir):
		log_diagnostic("Attempting to read biomass data from '%s'..." % file)
		try:
			raw = biom_processing.read_raw_data(file)
			return biom_processing.read_biomass_data(raw, pcb)
		except BaseException as error:
			# log_warning("Failed to read biomass data from file '%s'. This is not necessarily a problem." % file)
			# log_warning(error)
			continue
	# Cannot find a .csv file containing valid biomass readings.
	return []

def get_layer_variables(file: str) -> list[str]:
	"""
	Get the names of variables representing layers in this 

	@param nc: The input .nc file name.
	"""
	site = get_site_name_from_filename(file)
	if not site in _layer_variable_map:
		raise ValueError("Unknown layer variables for site '%s'" % site)
	return _layer_variable_map[site]

def get_layer_height(nc: Dataset, layer: str, next_layer: str) -> tuple[float, float]:
	"""
	Get the height of the specified layer in m. (ie distance between top and
	bottom of the layer.) The return value is a tuple containing the top of the
	layer and the layer height in that order.

	@param nc: Input .nc file.
	@param layer: Name of a layered soil water variable in the .nc file. Must
	              have a height attribute.
	@param next_layer: Name of the nc variable containing the next layer, or
	                   None if this is the bottom layer. Must have a height
					   attribute.
	"""
	height = getattr(nc.variables[layer], _ATTR_HEIGHT)
	depths = ozflux_common.get_lengths(height)

	layer_top = depths[0]

	if len(depths) == 2:
		# Height was of the form 0.1m - 0.2m.
		layer_bottom = depths[1]
		return (layer_top, layer_bottom - layer_top)

	if next_layer == None:
		# Bottom layer - assume this layer extends to bottom of profile.
		if layer_top > _SOIL_DEPTH:
			# If we get to here, the configured layer names in the lookup table
			# are probably incorrect for this site.
			m = "Bottom layer starts at %.2fm, which is below the hardcoded soil profile depth (%.2fm), and has unknown height"
			raise ValueError(m % (layer_top, _SOIL_DEPTH))
		
		dlt = _SOIL_DEPTH - layer_top
		m = "Site '%s': extending bottom soil layer by %.2fm"
		log_warning(m % (nc.site_name, dlt) )
		return (layer_top, dlt)

	height_next_layer = getattr(nc.variables[next_layer], _ATTR_HEIGHT)
	next_layer_depths = ozflux_common.get_lengths(height_next_layer)

	top_next_layer = next_layer_depths[0]

	if top_next_layer < layer_top:
		m = "Unknown height of layer '%s' in site '%s'; layer starts at %.2fm but the next layer down ('%s') starts at %.2fm. This probably indicates an issue in the layer name lookup table for this site."
		raise ValueError(m % (layer, nc.site_name, layer_top, next_layer, top_next_layer))

	return (layer_top, top_next_layer - layer_top)

def get_sw_data(file: str, timestep: int, pcb: Callable[[float], None]) \
-> list[float]:
	"""
	Read SW timeseries data from the given file name.

	@param file: Path to the input .nc file.
	@param timestep: Desired output timestep in hours.
	@param pcb: Progress callback function used for progress reporting.
	"""
	# Get output timestep in minutes.
	t_min = timestep * MINUTES_PER_HOUR

	# Get site name.
	site = get_site_name(file)

	# Cumulative depth up to current layer, used for error checking.
	cum_depth = 0

	# Create an array containing running total of SW for each timestep.
	total_mm = []

	with Dataset(file, "r", format=ozflux_common.NC_FORMAT) as nc:
		# Determine which layer variables we should read for this site.
		layer_variables = get_layer_variables(file)
		step_start = 0
		step_size = 1 / len(layer_variables)
		for i in range(len(layer_variables)):
			layer_name = layer_variables[i]

			# Read volumetric SW content for this layer.
			var = ForcingVariable(layer_name, layer_name, "m3/m3", numpy.mean
			 	, 0, 1)
			layer_vol = get_data(nc, var, t_min
				, lambda p: pcb(step_start + step_size * p))

			# Now determine layer width.
			nl = layer_variables[i + 1] if i + 1 < len(layer_variables)else None
			(top, height) = get_layer_height(nc, layer_name, nl)
			if less_than(top, cum_depth):
				m = "Double counting of layer '%s' in site '%s'. Cumulative depth before this layer is %.2fm, but this layer apparently starts at %.2fm"
				raise ValueError(m % (layer_name, site, cum_depth, top))

			if greater_than(top, _SOIL_DEPTH):
				m = "SW Layer '%s' in site '%s' starts at %.2fm, which is below max profile depth of %.2fm"
				raise ValueError(m % (layer_name, site, top, _SOIL_DEPTH))

			if greater_than(top, cum_depth):
				# This should really only happen in the first layer. The other
				# error checks should prevent this from occurring further down.
				delta = top - cum_depth
				top = cum_depth
				height += delta

			# If necessary, extend layer to either the next layer or the bottom
			# of the profile.
			if i + 1 < len(layer_variables):
				nl_height = getattr(nc.variables[nl], _ATTR_HEIGHT)
				nl_top = ozflux_common.get_length(nl_height)
				if top + height != nl_top:
					height = nl_top - top
			elif top + height != _SOIL_DEPTH:
				dlt = _SOIL_DEPTH - height - top
				height = _SOIL_DEPTH - top
				m = "Site '%s': extending bottom soil layer by %.2fm"
				log_warning(m % (site, dlt) )

			# Now we can convert to an actual amount for this layer.
			height_mm = height * MM_PER_M
			layer_mm = [x * height_mm for x in layer_vol]

			if i == 0:
				total_mm = layer_mm
			else:
				if len(layer_mm) != len(total_mm):
					m = "Layer '%s' contains %d elements which does not match expected number (%d)"
					raise ValueError(m % (layer_name, len(layer_mm), len(layer_mm)))

				# Now add these values element-wise to the running total.
				total_mm = [x + y for x, y in zip(total_mm, layer_mm)]

			cum_depth += height
			step_start += step_size
	pcb(1)

	return total_mm

def get_lai_data(data: pandas.DataFrame, site: str, timestep: int
		 , pcb: Callable[[float], None]
		 , sheet: str = _SHEET_LAI) -> list[Observation]:
	"""
	Read LAI observations for the specified site from the given input file.

	@param file: Path to the input (excel/.xlsx) file.
	@param timestep: Desired output timestep in hours.
	@param pcb: Progress reporting function.
	@param sheet: Name of the sheet in the input file to read.
	"""
	data = data[data[_LAI_COL_ID] == site]
	data = data.groupby([_LAI_COL_DATE]).mean(numeric_only = True)

	lai = [Observation(datetime.datetime.combine(date.date(), datetime.datetime.min.time()), float(lai)) for (date, lai) in data.iterrows()]
	return lai

def process_file(file: str, out_dir: str, timestep: int, biomass_file: str
	, biomass_searchpath: str, lai_data: pandas.DataFrame
	, pcb: Callable[[float], None]):
	"""
	Extract the standard variables from the input file, write them to a .csv
	file in the specified output directory in the specified timestep.

	@param file: Input file path.
	@param out_dir: Output directory.
	@param timestep: Output timestep in hours.
	@param pcb: Progress callback function.
	@param lai_file: Path to a file containing LAI observations.
	"""
	log_information("Processing '%s'..." % file)

	read_prop = 0.76
	write_prop = 0.03
	biom_prop = 0.0044
	sw_prop = 0.2
	lai_prop = 1 - write_prop - read_prop - biom_prop - sw_prop
	if lai_prop < 0:
		raise ValueError("Invalid default time proportions")

	variables = _standard_variables

	biom_start = time.time()
	biomass_data = get_biomass_data(file, biomass_file, biomass_searchpath
		, lambda p: pcb(biom_prop * p))
	biom_time = time.time() - biom_start

	step_start = biom_prop

	sw_start = time.time()
	sw_data = get_sw_data(file, timestep
		    , lambda p: pcb(step_start + sw_prop * p) )
	sw_time = time.time() - sw_start

	step_start += sw_prop

	lai_start = time.time()
	site = get_site_name(file)
	lai_readings = get_lai_data(lai_data, site, timestep
			, lambda p: pcb(step_start + lai_prop * p))
	lai_time = time.time() - lai_start

	log_debug("Opening input file '%s' for reading..." % file)

	step_start += lai_prop

	data = []
	read_start = time.time()
	timestep_minutes = timestep * MINUTES_PER_HOUR
	start_date: datetime
	with Dataset(file, "r", format=ozflux_common.NC_FORMAT) as nc:
		step_size = read_prop / len(variables)
		for var in variables:
			data.append(get_data(nc, var, timestep_minutes, lambda p: \
				pcb(step_start + step_size * p)))
			step_start += step_size
		start_date = get_next_year(ozflux_common.parse_date(nc.time_coverage_start))
	read_time = time.time() - read_start

	# step_start += read_prop

	data.append(sw_data)

	file = os.path.basename(file)
	filename = "%s.csv" % os.path.splitext(file)[0]
	outfile = os.path.join(out_dir, filename)

	if not os.path.exists(out_dir):
		os.makedirs(out_dir)

	write_start = time.time()
	names = ["%s" % v.out_name for v in variables]
	names.append(_NAME_SW)

	timestep_hours = timestep
	write_csv(outfile, names, data, biomass_data, start_date, timestep_hours
	   , lai_readings, lambda p: pcb(step_start + write_prop * p))
	write_time = time.time() - write_start

	total_time = read_time + write_time + biom_time + sw_time + lai_time

	read_prop = read_time / total_time
	write_prop = write_time / total_time
	biom_prop = biom_time / total_time
	sw_prop = sw_time / total_time
	lai_prop = lai_time / total_time

	log_diagnostic("Read time: %.2f%%; Write time: %.2f%%; Biomass time: %.2f%%; SW time: %.2f%%; lai time: %.2f%%"
	% (100 * read_prop, 100 * write_prop, 100 * biom_prop, 100 * sw_prop, 100 * lai_prop))

def main(opts: Options):
	"""
	Main function.

	@param opts: Parsed CLI options provided by the user..
	"""
	step = 1 / len(opts.files)
	start = 0

	log_information("Reading LAI data...")
	cols = [_LAI_COL_ID, _LAI_COL_DATE, _LAI_COL_LAI]
	lai_data = pandas.read_excel(opts.lai_file, _SHEET_LAI, usecols = cols, parse_dates = True)

	for file in opts.files:
		process_file(file, opts.out_dir, opts.timestep, opts.biomass_file
		, opts.biomass_searchpath, lai_data
		, lambda p: log_progress(start + step * p))
		start += step

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(argv)

	set_log_level(opts.log_level)
	set_show_progress(opts.report_progress)

	try:
		# Actual logic is in main().
		main(opts)
	except BaseException as error:
		# Basic error handling.
		log_error(traceback.format_exc())
		exit(1)
