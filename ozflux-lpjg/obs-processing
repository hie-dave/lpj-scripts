#!/usr/bin/env python3
#
# This script reads the raw .nc files and produces csv files with observations
# used in benchmarking.
#

import math
import biom_processing, datetime, glob, numpy, time, traceback
import ozflux_common, ozflux_netcdf

from argparse import ArgumentParser
from ozflux_logging import *
from ozflux_netcdf import *
from sys import argv
from netCDF4 import Dataset, Variable
from typing import Callable

# Standard variables in the ozflux files which will be used for benchmarking.
_standard_variables = [
	# GPP shouldn't get this high, but this is mainly for error checking anyway.
	ForcingVariable("GPP_LT", "gpp", "kgC/m2/day", numpy.mean, 0, 1e6),

	# todo: are these bounds sensible?
	ForcingVariable("ET", "et", "kg/m2/s", numpy.mean, -10, 10),
	ForcingVariable("ER_LT", "resp", "kgC/m2/day", numpy.mean, -10, 10, True),
	ForcingVariable("NEE_LT", "nee", "kgC/m2/day", numpy.mean, -10, 10)
]

class Options:
	"""
	Class for storing CLI arguments from the user.

	@param log: Log level.
	@param files: Input files.
	@param odir: Output directory.
	@param prog: True to write progress messages, 0 otherwise.
	@param parallel: True to process files in parallel.
	@param timestep: Desired output timestep, in hours.
	@param biomass_file: Optional file containing biomass readings
	@param biomass_searchpath: Search path for biomass readings. If provided, will search under here for a file containing biomass readings for the site
	"""
	def __init__(self, log : LogLevel, files: list[str], odir: str, prog: bool,
		parallel: bool, timestep: int, biomass_file: str
		, biomass_searchpath: str):
		self.log_level = log
		self.files = files
		self.out_dir = odir
		self.report_progress = prog
		self.parallel = parallel
		self.timestep = timestep
		self.biomass_file = biomass_file
		self.biomass_searchpath = biomass_searchpath

def parse_args(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Formatting ozflux data into a format suitable for consumption by LPJ-Guess")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default 3)", nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.INFORMATION)
	parser.add_argument("files", nargs = "+", help = "Input .nc files to be processed")
	parser.add_argument("-o", "--out-dir", required = True, help = "Path to the output directory. Processed files will be saved with the same file name into this directory")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	parser.add_argument("-P", "--parallel", action = "store_true", help = "Process files in parallel")
	parser.add_argument("-t", "--timestep", type = int, required = True, help = "Output timestep in hours")
	parser.add_argument("--version", action = "version", version = "%(prog)s " + ozflux_common.VERSION)

	biomass_group = parser.add_mutually_exclusive_group()
	biomass_group.add_argument("-b", "--biomass-readings", required = False, help = "Optional file containing biomass readings")
	biomass_group.add_argument("-s", "--biomass-searchpath", required = False, help = "Search path for biomass readings. If provided, will search under here for a file containing biomass readings for the site")

	p = parser.parse_args(argv[1:])

	return Options(p.verbosity, p.files, p.out_dir, p.show_progress, p.parallel,
		p.timestep, p.biomass_readings, p.biomass_searchpath)

def write_csv(outfile: str, names: list[str], data: list[list[float]]
	, biomass_data: list[biom_processing.BiomassReading]
	, start_date: datetime.datetime, timestep: int \
	, pcb: Callable[[float], None], delim = ","):
	"""
	Write the data to a csv file.

	@param outfile: Output file path.
	@param names: Output file column names.
	@param data: 2D data array where each element is a single column of data.
	@param start_date: First date in the data.
	@param timestep: Output timestep (in hours).
	"""
	if len(names) != len(data):
		raise ValueError("Number of column names (%d) doesn't match number of columns (%d)" % (len(names), len(data)))
	if len(names) == 0:
		log_warning("Writing 0 columns to output file")
		return

	nrow = len(data[0])
	for i in range(1, len(data)):
		if len(data[i]) != nrow:
			raise ValueError("Incorrect number of rows in column '%s'. Expected %d but was %d" % (names[i], nrow, len(data[i])))

	write_hours = timestep % 24 != 0

	delta = datetime.timedelta(hours = timestep)
	end_date = start_date + nrow * delta

	biomass_data = [b for b in biomass_data if b.date >= start_date and b.date <= end_date]
	biomass_data = sorted(biomass_data, key = lambda b: b.date)

	write_biomass = len(biomass_data) > 0

	biomass_index = 0
	with open(outfile, "w") as csv:
		csv.write("year%sdoy%s" % (delim, delim))
		if write_hours:
			csv.write("hour%s" % delim)
		csv.write("%s" % str.join(delim, names))
		if write_biomass:
			csv.write(",live_biomass,dead_biomass")
		csv.write("\n")
		date = start_date
		for i in range(nrow):
			if i % PROGRESS_CHUNK_SIZE == 0:
				pcb(i / nrow)
			# %j is day of year (1-366), but lpj-guess writes day of year as
			# (0-365), so we subtract one here to put it in the same "units".
			doy = str(int(date.strftime("%j")) - 1)
			csv.write("%s%s%s%s" % (date.year, delim, doy, delim))

			# Write the hour column.
			if write_hours:
				csv.write("%s%s" % (date.hour, delim))

			# Write the data columns.
			csv.write("%s" % str.join(delim, [str(column[i]) for column in data]))

			# Write the biomass column.
			if write_biomass:
				live_str = ""
				dead_str = ""
				if biomass_index < len(biomass_data):
					reading = biomass_data[biomass_index]
					if reading.date == date:
						biomass_index += 1
						if not math.isnan(reading.live):
							live_str = reading.live
						if not math.isnan(reading.dead):
							dead_str = reading.dead
				csv.write("%s%s%s%s" % (delim, live_str, delim, dead_str))
			csv.write("\n")
			date += delta
	pcb(1)

def get_biomass_data(nc_file: str, biom_file: str, biom_searchpath: str,
	pcb: Callable[[float], None]) \
	-> list[biom_processing.BiomassReading]:
	"""
	Attempt to read biomass data corresponding to the site specified by the .nc
	file name.

	@param nc_file: Input .nc file (may be used to determine site name).
	@param biom_file: -b CLI argument from user (or None if not given).
	@param biom_searchpath: -s CLI argument from user (or None if not given).
	"""
	if biom_file != None:
		raw = biom_processing.read_raw_data(biom_file)
		return biom_processing.read_biomass_data(raw, pcb)

	if biom_searchpath == None:
		log_warning("No biomass input data file was given. No biomass data will be included in output file.")
		return []

	# Determine site name, convert to lower case.
	site_name = ozflux_netcdf.get_site_name(nc_file)
	site_name = site_name.replace(" ", "_").lower()

	if "CumberlandPlain" in nc_file:
		site_name = "cumberland_plains"

	site_dir = "%s/%s" % (biom_searchpath, site_name)
	dir = site_dir if os.path.exists(site_dir) else biom_searchpath

	for file in glob.glob("%s/*.csv" % dir):
		log_diagnostic("Attempting to read biomass data from '%s'..." % file)
		try:
			raw = biom_processing.read_raw_data(file)
			return biom_processing.read_biomass_data(raw, pcb)
		except BaseException as error:
			log_warning("Failed to read biomass data from file '%s'. This is not necessarily a problem." % file)
			log_warning(error)
			continue
	# Cannot find a .csv file containing valid biomass readings.
	return []

def process_file(file: str, out_dir: str, timestep: int, biomass_file: str
	, biomass_searchpath: str, pcb: Callable[[float], None]):
	"""
	Extract the standard variables from the input file, write them to a .csv
	file in the specified output directory in the specified timestep.

	@param file: Input file path.
	@param out_dir: Output directory.
	@param timestep: Output timestep in hours.
	@param pcb: Progress callback function.
	"""
	log_information("Processing '%s'..." % file)

	read_prop = 0.85
	biom_prop = 0.14
	write_prop = 1 - read_prop - biom_prop

	variables = _standard_variables

	biom_start = time.time()
	biomass_data = get_biomass_data(file, biomass_file, biomass_searchpath
		, lambda p: pcb(biom_prop * p))
	biom_time = time.time() - biom_start

	log_debug("Opening input file '%s' for reading..." % file)

	data = []
	read_start = time.time()
	timestep_minutes = timestep * MINUTES_PER_HOUR
	start_date: datetime
	with Dataset(file, "r", format=ozflux_common.NC_FORMAT) as nc:
		step_size = read_prop / len(variables)
		step_start = biom_prop
		for var in variables:
			data.append(get_data(nc, var, timestep_minutes, lambda p: \
				pcb(step_start + step_size * p)))
			step_start += step_size
		start_date = get_next_year(ozflux_common.parse_date(nc.time_coverage_start))
	read_time = time.time() - read_start

	file = os.path.basename(file)
	filename = "%s.csv" % os.path.splitext(file)[0]
	outfile = os.path.join(out_dir, filename)

	if not os.path.exists(out_dir):
		os.makedirs(out_dir)

	write_start = time.time()
	names = ["%s" % v.out_name for v in variables]

	timestep_hours = timestep
	write_csv(outfile, names, data, biomass_data, start_date, timestep_hours, lambda p: \
		pcb(biom_prop + read_prop + write_prop * p))
	write_time = time.time() - write_start

	total_time = read_time + write_time + biom_time

	read_prop = read_time / total_time
	write_prop = write_time / total_time
	biom_prop = biom_time / total_time

	log_diagnostic("Time spent reading: %.2f%%; Time spent writing: %.2f%%; Time spenting reading biomass data: %.2f%%"
	% (100 * read_prop, 100 * write_prop, 100 * biom_prop))

def main(opts: Options):
	"""
	Main function.

	@param opts: Parsed CLI options provided by the user..
	"""
	step = 1 / len(opts.files)
	start = 0
	for file in opts.files:
		process_file(file, opts.out_dir, opts.timestep, opts.biomass_file
		, opts.biomass_searchpath, lambda p: log_progress(start + step * p))
		start += step

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(argv)

	set_log_level(opts.log_level)
	set_show_progress(opts.report_progress)

	try:
		# Actual logic is in main().
		main(opts)
	except BaseException as error:
		# Basic error handling.
		log_error(traceback.format_exc())
		exit(1)
