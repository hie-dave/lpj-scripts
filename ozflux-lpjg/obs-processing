#!/usr/bin/env python3
#
# This script reads the raw .nc files and produces csv files with observations
# used in benchmarking.
#

#%pip install openpyxl numpy pandas

import math
import biom_processing, datetime, glob, numpy, pandas, time, traceback
import ozflux_common

from argparse import ArgumentParser
from ozflux_logging import *
from ozflux_netcdf import *
from sys import argv
from netCDF4 import Dataset
from typing import Callable
from ozflux_parallel import JobManager

_ATTR_HEIGHT = "height"

# Total soil profile depth in m.
_SOIL_DEPTH = 1.5

# Name of the soil water column in the output file.
_NAME_SW = "swmm"

# Name of the LAI column in the output file.
_NAME_LAI = "lai"

# Name of the 90cm soil water column in the output file.
_NAME_SMIPS = "swmm_90"

# Name of the 90cm soil moisture index in the output file.
_NAME_SWINDEX = "swindex_90"

# Name of the live biomass column in the output file.
_NAME_LIVE = "live_biomass"

# Name of the dead biomass column in the output file.
_NAME_DEAD = "dead_biomass"

# Name of the "site name" column in the LAI data file.
_LAI_COL_ID = "ID"

# Name of the date column in the LAI data file.
_LAI_COL_DATE = "Date"

# Name of the LAI column in the LAI data file.
_LAI_COL_LAI = "MYD15A2H_006_Lai_500m"

# Name of the default sheet name in the LAI data file.
_SHEET_LAI = "OzFlux-sites-LAI-MYD15A2H-006-r"

# Name of the date column in smips input files.
_SMIPS_COL_DATE = "date"

# Name of the SW at 90cm column in smips input files.
_SMIPS_COL_SW = "totalbucket"

# Name of the soil moisture index column in smips input files.
_SMIPS_COL_SWINDEX = "SMindex"

# Standard variables in the ozflux files which will be used for benchmarking.
_standard_variables = [
	# GPP shouldn't get this high, but this is mainly for error checking anyway.
	ForcingVariable("GPP_LT", "gpp", "kgC/m2/day", numpy.mean, 0, 1e6),

	# todo: are these bounds sensible?
	ForcingVariable("ET", "et", "kg/m2/s", numpy.mean, -10, 10),
	ForcingVariable("ER_LT", "resp", "kgC/m2/day", numpy.mean, -10, 10),
	ForcingVariable("NEE_LT", "nee", "kgC/m2/day", numpy.mean, -10, 10),
]

# Map of site names to a list of names of soil water layer variables in the flux
# data file. The site name should be the .nc file name before the first '_'.
_layer_variable_map = {
	"AdelaideRiver": ["Sws"],
	"AliceSpringsMulga": ["Sws_mulga", "Sws_10cm_mulga_2b", "Sws_60cm_mulga_2", "Sws_100cm_mulga_2"],
	# todo: what is the difference between Sws_05cma_N, Sws_05cma_S, Sws_05cmb_N, Sws_05cmb_S???
	"Boyagin": ["Sws_05cma_N", "Sws_08cm_main", "Sws_10cma_N", "Sws_15cm_main", "Sws_20cma_N", "Sws_40cm_N", "Sws_80cm_N", "Sws_110cm_N"],
	"Calperum": ["Sws", "Sws_25cm", "Sws_50cm", "Sws_100cm"],
	"CapeTribulation": ["Sws", "Sws_75cma"],
	"Collie": ["Sws", "Sws_10cm", "Sws_30cm"],
	"CowBay": ["Sws", "Sws_75cma"],
	"CumberlandPlain": ["Sws", "Sws_20cm"],
	"DalyPasture": ["Sws", "Sws_50"],
	"DalyUncleared": ["Sws", "Sws_50cm"],
	"DryRiver": ["Sws", "Sws_50cm"],
	# Note: Emerald does have Sws, but it has no height attribute!!!!
	"Emerald": ["Sws_5cmd", "Sws_15cmd", "Sws_22p5cmd", "Sws_30cmd"],
	# Note FoggDam does have Sws, but it has no height attribute!!!!
	"FoggDam": ["Sws_10"],
	"Gingin": ["Sws", "Sws_20cm", "Sws_40cm", "Sws_80cm"],
	"GreatWesternWoodlands": ["Sws_5cmb", "Sws", "Sws_20cmb", "Sws_30cmd", "Sws_50cmb", "Sws_70cmb"],
	"HowardSprings": ["Sws", "Sws40cm", "Sws100cm", "Sws120cm", "Sws140cm"],
	"Litchfield": ["Sws", "Sws20cm", "Sws50cm", "Sws100cm", "Sws140cm"],
	"Longreach": ["Sws"],
	"Otway": ["Sws"],
	# Note RedDirt does have Sws, but it has no height attribute!!!!
	"RedDirtMelonFarm": ["Sws_5cm", "Sws_50cm"],
	"Ridgefield": ["Sws", "Sws_10cm", "Sws_20cm", "Sws_40cm", "Sws_80cm"],
	"RiggsCreek": ["Sws", "Sws_50cm"],
	"RobsonCreek": ["Sws", "Sws_75cma"],
	"Samford": ["Sws"],
	"SilverPlains": ["Sws"],
	"SturtPlains": ["Sws", "Sws_50cm"],
	# Note: TiTreeEast has Sws_grass, Sws_mulga, and Sws_spinifex
	"TiTreeEast": ["Sws"],
	"Tumbarumba": ["Sws"],
	"WallabyCreek": ["Sws", "Sws_15cma", "Sws_20cma", "Sws_40cma", "Sws_80cma"],
	"Warra": ["Sws", "Sws_20cmd", "Sws_40cmd", "Sws_80cmd", "Sws_1p2m"],
	"WombatStateForest": ["Sws", "Sws_35cma", "Sws_65cma", "Sws_95cma"],
	"Yanco": ["Sws_3cm", "Sws", "Sws_15cm", "Sws_45cm", "Sws_75cm"],
}

class Observation:
	"""
	An observation made on a particular date.
	"""
	def __init__(self, date: datetime.datetime, value: float):
		self.date = date
		self.value = value

class Observations:
	"""
	A collection of observations for a particular variable.
	"""
	def __init__(self, name: str, observations: list[Observation]):
		self.name = name
		self.data = sorted(observations, key = lambda b: b.date)

class Options:
	"""
	Class for storing CLI arguments from the user.

	@param log: Log level.
	@param files: Input files.
	@param odir: Output directory.
	@param prog: True to write progress messages, 0 otherwise.
	@param parallel: True to process files in parallel.
	@param timestep: Desired output timestep, in hours.
	@param smips_path: Path to file containing SW content at 90cm as obtained from SMIPS.
	@param biomass_file: Optional file containing biomass readings
	@param biomass_searchpath: Search path for biomass readings. If provided, will search under here for a file containing biomass readings for the site
	@param lai_file: Path to file containing LAI observations.
	"""
	def __init__(self, log : LogLevel, files: list[str], odir: str, prog: bool,
		parallel: bool, timestep: int, smips_path: str, smips_index_path: str
		, biomass_file: str, biomass_searchpath: str, lai_file: str):
		self.log_level = log
		self.files = files
		self.out_dir = odir
		self.report_progress = prog
		self.parallel = parallel
		self.timestep = timestep
		self.biomass_file = biomass_file
		self.biomass_searchpath = biomass_searchpath
		self.lai_file = lai_file
		self.smips_path = smips_path
		self.smips_index_path = smips_index_path

def parse_args(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Formatting ozflux data into a format suitable for consumption by LPJ-Guess")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default 3)", nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.INFORMATION)
	parser.add_argument("files", nargs = "+", help = "Input .nc files to be processed")
	parser.add_argument("-o", "--out-dir", required = True, help = "Path to the output directory. Processed files will be saved with the same file name into this directory")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	parser.add_argument("-P", "--parallel", action = "store_true", help = "Process files in parallel")
	parser.add_argument("-t", "--timestep", type = int, required = True, help = "Output timestep in hours")
	parser.add_argument("--version", action = "version", version = "%(prog)s " + ozflux_common.VERSION)
	parser.add_argument("-l", "--lai-file", required = True, help = "Path to a file containing lai observations")
	parser.add_argument("-s", "--smips-path", required = True, help = "Path to directory containing site-level SW content at 90cm as obtained from SMIPS. Each file in this directory should be named site.csv")
	parser.add_argument("--smips-index-path", help = "Optional path to directory containing site-level soil moisture index as obtained from smips. Each file should be named site.csv")

	biomass_group = parser.add_mutually_exclusive_group()
	biomass_group.add_argument("-b", "--biomass-readings", required = False, help = "Optional file containing biomass readings")
	biomass_group.add_argument("--biomass-searchpath", required = False, help = "Search path for biomass readings. If provided, will search under here for a file containing biomass readings for the site")

	p = parser.parse_args(argv[1:])

	return Options(p.verbosity, p.files, p.out_dir, p.show_progress, p.parallel,
		p.timestep, p.smips_path, p.smips_index_path, p.biomass_readings,
		p.biomass_searchpath, p.lai_file)

def multiply_timedelta(delta: datetime.timedelta, n: int) -> datetime.timedelta:
	"""
	Return a timedelta which represents N multiples of the original delta.

	@param delta: A time delta.
	@param n: Any real number.
	"""
	return datetime.timedelta(seconds = n * delta.total_seconds())

def _create_observations(name: str, data: list[float], start: datetime.datetime
		, timestep: datetime.timedelta):
	
	obs = [Observation(start + multiply_timedelta(timestep, i), data[i]) for (i, _) in enumerate(data)]
	return Observations(name, obs)

def write_csv(outfile: str, variables: list[Observations]
	#names: list[str], data: list[list[float]]
	#, biomass_data: list[biom_processing.BiomassReading]
	, start_date: datetime.datetime, end_date: datetime.datetime, timestep: int
	#, lai_data: list[Observation], sw90_data: list[Observation]
	, pcb: Callable[[float], None], delim = ","):
	"""
	Write the data to a csv file.

	@param outfile: Output file path.
	@param names: Output file column names.
	@param data: 2D data array where each element is a single column of data.
	@param start_date: First date in the data.
	@param timestep: Output timestep (in hours).
	@param lai_data: LAI observations. May be empty list if none available.
	"""

	write_hours = timestep % 24 != 0

	delta = datetime.timedelta(hours = timestep)
	nrow = int((end_date - start_date).total_seconds() // delta.total_seconds())

	variables = [v for v in variables if len(v.data) > 0]
	indices = [next(i for i in range(len(var.data)) if var.data[i].date >= start_date) for var in variables]

	with open(outfile, "w") as csv:
		csv.write("year%sdoy%s" % (delim, delim))
		if write_hours:
			csv.write("hour%s" % delim)
		csv.write("%s" % str.join(delim, [v.name for v in variables]))
		csv.write("\n")
		date = start_date
		for i in range(nrow):
			if i % PROGRESS_CHUNK_SIZE == 0:
				pcb(i / nrow)
			# %j is day of year (1-366), but lpj-guess writes day of year as
			# (0-365), so we subtract one here to put it in the same "units".
			doy = str(int(date.strftime("%j")) - 1)
			csv.write("%s%s%s%s" % (date.year, delim, doy, delim))

			# Write the hour column.
			if write_hours:
				csv.write("%s%s" % (date.hour, delim))

			for i in range(len(variables)):
				row = indices[i]
				var = variables[i]
				if row < len(var.data):
					reading = var.data[row]
					if reading.date == date:
						csv.write("%s" % reading.value)
						indices[i] += 1
					if i < (len(variables) - 1):
						csv.write(delim)
			csv.write("\n")
			date += delta
	pcb(1)

def get_biomass_data(nc_file: str, biom_file: str, biom_searchpath: str,
	pcb: Callable[[float], None]) \
	-> list[biom_processing.BiomassReading]:
	"""
	Attempt to read biomass data corresponding to the site specified by the .nc
	file name.

	@param nc_file: Input .nc file (may be used to determine site name).
	@param biom_file: -b CLI argument from user (or None if not given).
	@param biom_searchpath: -s CLI argument from user (or None if not given).
	"""
	if biom_file != None:
		raw = biom_processing.read_raw_data(biom_file)
		return biom_processing.read_biomass_data(raw, pcb)

	if biom_searchpath == None:
		log_warning("No biomass input data file was given. No biomass data will be included in output file.")
		return []

	# Determine site name, convert to lower case.
	site_name = get_site_name(nc_file)
	site_name = site_name.replace(" ", "_").lower()

	if "CumberlandPlain" in nc_file:
		site_name = "cumberland_plains"

	site_dir = "%s/%s" % (biom_searchpath, site_name)
	dir = site_dir if os.path.exists(site_dir) else biom_searchpath

	for file in glob.glob("%s/*.csv" % dir):
		log_diagnostic("Attempting to read biomass data from '%s'..." % file)
		try:
			raw = biom_processing.read_raw_data(file)
			return biom_processing.read_biomass_data(raw, pcb)
		except BaseException as error:
			# log_warning("Failed to read biomass data from file '%s'. This is not necessarily a problem." % file)
			# log_warning(error)
			continue
	# Cannot find a .csv file containing valid biomass readings.
	return []

def get_layer_variables(file: str) -> list[str]:
	"""
	Get the names of variables representing layers in this 

	@param nc: The input .nc file name.
	"""
	site = get_site_name_from_filename(file)
	if not site in _layer_variable_map:
		raise ValueError("Unknown layer variables for site '%s'" % site)
	return _layer_variable_map[site]

def get_layer_height(nc: Dataset, layer: str, next_layer: str) -> tuple[float, float]:
	"""
	Get the height of the specified layer in m. (ie distance between top and
	bottom of the layer.) The return value is a tuple containing the top of the
	layer and the layer height in that order.

	@param nc: Input .nc file.
	@param layer: Name of a layered soil water variable in the .nc file. Must
	              have a height attribute.
	@param next_layer: Name of the nc variable containing the next layer, or
	                   None if this is the bottom layer. Must have a height
					   attribute.
	"""
	height = getattr(nc.variables[layer], _ATTR_HEIGHT)
	depths = ozflux_common.get_lengths(height)

	layer_top = depths[0]

	if len(depths) == 3:
		# Some sites oh so helpfully have a height that looks like:
		# "-0.08m vertically,0.08 - 0.38 m vertically"
		if ozflux_common.floats_equal(depths[0], depths[1]):
			depths = [depths[0], depths[2]]
		else:
			raise ValueError("Unable to interpret layer depths from height string '%s'" % height)

	if len(depths) == 2 and depths[0] > depths[1]:
		tmp = depths[0]
		depths[0] = depths[1]
		depths[1] = tmp

	if len(depths) == 2 and ozflux_common.floats_equal(depths[0], depths[1]):
		depths = [depths[0]]

	if len(depths) == 2:
		# Height was of the form 0.1m - 0.2m.
		layer_bottom = depths[1]
		return (layer_top, layer_bottom - layer_top)

	if next_layer == None:
		# Bottom layer - assume this layer extends to bottom of profile.
		if layer_top > _SOIL_DEPTH:
			# If we get to here, the configured layer names in the lookup table
			# are probably incorrect for this site.
			m = "Bottom layer starts at %.2fm, which is below the hardcoded soil profile depth (%.2fm), and has unknown height"
			raise ValueError(m % (layer_top, _SOIL_DEPTH))

		dlt = _SOIL_DEPTH - layer_top
		m = "Site '%s': extending bottom soil layer by %.2fm"
		log_warning(m % (nc.site_name, dlt) )
		return (layer_top, dlt)

	height_next_layer = getattr(nc.variables[next_layer], _ATTR_HEIGHT)
	next_layer_depths = ozflux_common.get_lengths(height_next_layer)

	top_next_layer = next_layer_depths[0]

	if top_next_layer < layer_top:
		m = "Unknown height of layer '%s' in site '%s'; layer starts at %.2fm but the next layer down ('%s') starts at %.2fm. This probably indicates an issue in the layer name lookup table for this site."
		raise ValueError(m % (layer, nc.site_name, layer_top, next_layer, top_next_layer))

	return (layer_top, top_next_layer - layer_top)

def get_sw_data(file: str, timestep: int, pcb: Callable[[float], None]) \
-> list[Observation]:
	"""
	Read SW timeseries data from the given file name.

	@param file: Path to the input .nc file.
	@param timestep: Desired output timestep in hours.
	@param pcb: Progress callback function used for progress reporting.
	"""
	# Get output timestep in minutes.
	t_min = timestep * MINUTES_PER_HOUR

	# Get site name.
	site = get_site_name(file)

	# Cumulative depth up to current layer, used for error checking.
	cum_depth = 0

	# Create an array containing running total of SW for each timestep.
	total_mm = []

	with Dataset(file, "r", format=ozflux_common.NC_FORMAT) as nc:
		# Determine which layer variables we should read for this site.
		layer_variables = get_layer_variables(file)
		step_start = 0
		step_size = 1 / len(layer_variables)
		for i in range(len(layer_variables)):
			layer_name = layer_variables[i]

			# Read volumetric SW content for this layer.
			var = ForcingVariable(layer_name, layer_name, "m3/m3", numpy.mean
			 	, 0, 1)
			layer_vol = get_data(nc, var, t_min
				, lambda p: pcb(step_start + step_size * p))

			# Now determine layer width.
			nl = layer_variables[i + 1] if i + 1 < len(layer_variables)else None
			(top, height) = get_layer_height(nc, layer_name, nl)
			if less_than(top, cum_depth):
				m = "Double counting of layer '%s' in site '%s'. Cumulative depth before this layer is %.2fm, but this layer apparently starts at %.2fm"
				raise ValueError(m % (layer_name, site, cum_depth, top))

			if greater_than(top, _SOIL_DEPTH):
				m = "SW Layer '%s' in site '%s' starts at %.2fm, which is below max profile depth of %.2fm"
				raise ValueError(m % (layer_name, site, top, _SOIL_DEPTH))

			if greater_than(top, cum_depth):
				# This should really only happen in the first layer. The other
				# error checks should prevent this from occurring further down.
				delta = top - cum_depth
				top = cum_depth
				height += delta

			# If necessary, extend layer to either the next layer or the bottom
			# of the profile.
			if i + 1 < len(layer_variables):
				nl_height = getattr(nc.variables[nl], _ATTR_HEIGHT)
				nl_top = ozflux_common.get_length(nl_height)
				if top + height != nl_top:
					height = nl_top - top
			elif top + height != _SOIL_DEPTH:
				dlt = _SOIL_DEPTH - height - top
				height = _SOIL_DEPTH - top
				m = "Site '%s': extending bottom soil layer by %.2fm"
				log_warning(m % (site, dlt) )

			# Now we can convert to an actual amount for this layer.
			height_mm = height * MM_PER_M
			layer_mm = [x * height_mm for x in layer_vol]

			if i == 0:
				total_mm = layer_mm
			else:
				if len(layer_mm) != len(total_mm):
					m = "Layer '%s' contains %d elements which does not match expected number (%d)"
					raise ValueError(m % (layer_name, len(layer_mm), len(layer_mm)))

				# Now add these values element-wise to the running total.
				total_mm = [x + y for x, y in zip(total_mm, layer_mm)]

			cum_depth += height
			step_start += step_size
	pcb(1)

	return total_mm

def get_lai_data(data: pandas.DataFrame, site: str, timestep: int, pcb: Callable[[float], None]) -> list[Observation]:
	"""
	Read LAI observations for the specified site from the given input file.

	@param data: Pre-parsed LAI data. (This is parsed once because the input file contains data for all sites.)
	@param site: Name of the site.
	@param timestep: Desired output timestep in hours.
	@param pcb: Progress reporting function.
	"""
	data = data[data[_LAI_COL_ID] == site]
	data = data.groupby([_LAI_COL_DATE]).mean(numeric_only = True)

	lai = [Observation(datetime.datetime.combine(date.date(), datetime.datetime.min.time()), float(lai)) for (date, lai) in data.iterrows()]
	return lai

def get_smips_data_from_file(file: str, timestep: int, col: str, pcb: Callable[[float], None]) \
		-> list[Observation]:
	"""
	Read smips SW content at 90cm from the site file in the specified path.

	@param file: Input csv file.
	@param timestep: Output timestep in hours.
	@param pcb: Progress reporting function.
	"""
	data = pandas.read_csv(file, parse_dates = True)
	data.iloc[:,0] = [datetime.datetime.strptime(x, "%Y-%m-%d") for x in data.iloc[:,0]]
	parsed = [Observation(row[_SMIPS_COL_DATE], row[col]) for row in data.iloc]
	return parsed

def get_smips_data(site: str, path: str, timestep: int, col: str, pcb: Callable[[float], None]) \
		-> list[Observation]:
	"""
	Read smips SW content at 90cm from the site file in the specified path.

	@param site: Name of the site.
	@param path: Path to a directory containing site-level csv files containing timeseries of SW content at 90cm.
	@param timestep: Output timestep in hours.
	@param col: Column name of data to be read.
	@param pcb: Progress reporting function.
	"""
	file = os.path.join(path, "%s.csv" % site)
	return get_smips_data_from_file(file, timestep, col, pcb)

def process_file(file: str, out_dir: str, timestep: int, biomass_file: str
	, biomass_searchpath: str, lai_data: pandas.DataFrame, smips_path: str
	, smips_index_path: str, pcb: Callable[[float], None]):
	"""
	Extract the standard variables from the input file, write them to a .csv
	file in the specified output directory in the specified timestep.

	@param file: Input file path.
	@param out_dir: Output directory.
	@param timestep: Output timestep in hours.
	@param smips_path: Path to smips input files.
	@param smips_path: Path to smips SW index files.
	@param pcb: Progress callback function.
	@param lai_file: Path to a file containing LAI observations.
	"""
	log_information("Processing '%s'..." % file)

	read_prop = 0.76
	write_prop = 0.03
	biom_prop = 0.0024
	sw_prop = 0.2
	smips_prop = 0.002
	lai_prop = 1 - write_prop - read_prop - biom_prop - sw_prop - smips_prop
	if lai_prop < 0:
		raise ValueError("Invalid default time proportions")

	flux_vars = _standard_variables

	out_variables: list[Observations] = []

	step_start = 0

	lai_start = time.time()
	site = get_site_name(file)
	lai_data = get_lai_data(lai_data, site, timestep
			, lambda p: pcb(step_start + lai_prop * p))
	out_variables.append(Observations(_NAME_LAI, lai_data))
	lai_time = time.time() - lai_start

	log_debug("Opening input file '%s' for reading..." % file)

	step_start += lai_prop

	read_start = time.time()
	timestep_minutes = timestep * MINUTES_PER_HOUR
	start_date: datetime.datetime
	end_date: datetime.datetime
	delta = datetime.timedelta(hours = timestep)
	timeseries: list[datetime.datetime] = []
	with Dataset(file, "r", format=ozflux_common.NC_FORMAT) as nc:
		step_size = read_prop / len(flux_vars)
		start_date = get_next_year(ozflux_common.parse_date(nc.time_coverage_start))
		for var in flux_vars:
			raw = get_data(nc, var, timestep_minutes, lambda p: \
				pcb(step_start + step_size * p))
			if len(timeseries) == 0:
				timeseries = [start_date + multiply_timedelta(delta, i) for i in range(len(raw))]
				end_date = start_date + multiply_timedelta(delta, len(raw))
			parsed = [Observation(x, y) for (x, y) in zip(timeseries, raw)]
			out_variables.append(Observations(var.out_name, parsed))
			step_start += step_size
	read_time = time.time() - read_start

	sw_start = time.time()
	sw_data = get_sw_data(file, timestep
		    , lambda p: pcb(step_start + sw_prop * p) )
	out_variables.append(_create_observations(_NAME_SW, sw_data, start_date, delta))
	sw_time = time.time() - sw_start
	step_start += sw_prop

	smips_start = time.time()
	site_name = get_site_name_from_filename(file)
	sw90_data = get_smips_data(site_name, smips_path, timestep, _SMIPS_COL_SW
			    , lambda p: pcb(step_start + 0.5 * smips_prop * p))
	out_variables.append(Observations(_NAME_SMIPS, sw90_data))
	if os.path.exists(smips_index_path):
		swindex_data = get_smips_data(site_name, smips_index_path, timestep
				, _SMIPS_COL_SWINDEX
				, lambda p: pcb(step_start + smips_prop * (0.5 * p + 0.5)))
		out_variables.append(Observations(_NAME_SWINDEX, swindex_data))
	smips_time = time.time() - smips_start
	step_start += smips_prop

	biom_start = time.time()
	biomass_data = get_biomass_data(file, biomass_file, biomass_searchpath
		, lambda p: pcb(step_start + biom_prop * p))
	biomass_data = [b for b in biomass_data if b.date >= start_date and b.date <= end_date]
	biomass_data = sorted(biomass_data, key = lambda b: b.date)
	if len(biomass_data) > 0:
		live_data = Observations(_NAME_LIVE, [Observation(d.date, d.live) for d in biomass_data])
		dead_data = Observations(_NAME_DEAD, [Observation(d.date, d.dead) for d in biomass_data])
		out_variables.append(live_data)
		out_variables.append(dead_data)

	biom_time = time.time() - biom_start
	step_start += biom_prop

	# step_start += read_prop

	file = os.path.basename(file)
	filename = "%s.csv" % get_site_name_from_filename(file)
	outfile = os.path.join(out_dir, filename)

	if not os.path.exists(out_dir):
		os.makedirs(out_dir)

	write_start = time.time()
	names = ["%s" % v.out_name for v in flux_vars]
	names.append(_NAME_SW)

	timestep_hours = timestep
	write_csv(outfile, out_variables, start_date, end_date, timestep_hours
	   , lambda p: pcb(step_start + write_prop * p))
	write_time = time.time() - write_start

	total_time = read_time + write_time + biom_time + sw_time + lai_time + smips_time

	read_prop = read_time / total_time
	write_prop = write_time / total_time
	biom_prop = biom_time / total_time
	sw_prop = sw_time / total_time
	lai_prop = lai_time / total_time
	smips_prop = smips_time / total_time

	log_diagnostic("Read time: %.2f%%; Write time: %.2f%%; Biomass time: %.2f%%; SW time: %.2f%%; lai time: %.2f%%; smips time: %.2f%%"
	% (100 * read_prop, 100 * write_prop, 100 * biom_prop, 100 * sw_prop, 100 * lai_prop, 100 * smips_prop))

class Processor:
	def __init__(self, file: str, out_dir: str, timestep: int, biomass_file: str, biomass_searchpath: str
	      , lai_data: pandas.DataFrame, smips_path: str, smips_index_path: str):
		self.file = file
		self.out_dir = out_dir
		self.timestep = timestep
		self.biomass_file = biomass_file
		self.biomass_searchpath = biomass_searchpath
		self.lai_data = lai_data
		self.smips_path = smips_path
		self.smips_index_path = smips_index_path
	def exec(self, pcb: Callable[[float], None]):
		process_file(self.file, self.out_dir, self.timestep, self.biomass_file
	       , self.biomass_searchpath, self.lai_data, self.smips_path
		   , self.smips_index_path, pcb)

def main(opts: Options):
	"""
	Main function.

	@param opts: Parsed CLI options provided by the user..
	"""

	log_information("Reading LAI data...")
	cols = [_LAI_COL_ID, _LAI_COL_DATE, _LAI_COL_LAI]
	lai_data = pandas.read_excel(opts.lai_file, _SHEET_LAI, usecols = cols, parse_dates = True)

	job_manager = JobManager()

	for file in opts.files:
		p = Processor(file, opts.out_dir, opts.timestep, opts.biomass_file
				, opts.biomass_searchpath, lai_data, opts.smips_path
				, opts.smips_index_path)
		job_manager.add_job(p, os.path.getsize(file))

	if opts.parallel:
		job_manager.run_parallel()
	else:
		job_manager.run_single_threaded()

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(argv)

	set_log_level(opts.log_level)
	set_show_progress(opts.report_progress)

	try:
		# Actual logic is in main().
		main(opts)
	except BaseException as error:
		# Basic error handling.
		log_error(traceback.format_exc())
		exit(1)
