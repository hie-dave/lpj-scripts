#!/usr/bin/env python3
#
# This script is a faster cdo mergetime command. It merges input files which
# contain the same data over different time ranges.
#
import ozflux_common
import math
import traceback

from ozflux_logging import *
from ozflux_netcdf import *
from argparse import ArgumentParser
from sys import argv
from netCDF4 import Dataset, Dimension, num2date, date2num
from typing import Callable
from ozflux_parallel import JobManager
from os import path

class Options:
	"""
	Class for storing CLI arguments from the user.

	@param log: Log level.
	@param files: Input files.
	@param out: Output file.
	@param prog: True to write progress messages, 0 otherwise.
	@param parallel: True to process files in parallel.
	@param chunk_size: Minimum chunk size to be used when copying data.
	"""
	def __init__(self, log : LogLevel, files: list[str], out: str, prog: bool,
		parallel: bool, chunk_size: int, units: str):
		self.log_level = log
		self.files = files
		self.out_file = out
		self.report_progress = prog
		self.parallel = parallel
		self.chunk_size = chunk_size
		self.units = units

class MergeTime:
	def __init__(self, in_file: str, out_file: str, min_chunk_size: int):
		self.in_file = in_file
		self.out_file = out_file
		self.min_chunk_size = min_chunk_size
	def exec(self, pcb: Callable[[float], None]):
		log_information("Processing '%s'..." % self.in_file)
		with open_netcdf(self.out_file, True) as nc_out:
			with open_netcdf(self.in_file) as nc_in:
				merge_time(nc_in, nc_out, self.min_chunk_size, pcb)

def parse_args(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Formatting ozflux data into a format suitable for consumption by LPJ-Guess")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default 3)", nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.INFORMATION)
	parser.add_argument("files", nargs = "+", help = "Input .nc files to be processed")
	parser.add_argument("-o", "--out-file", required = True, help = "Path to the output file.")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	parser.add_argument("-P", "--parallel", action = "store_true", help = "Process files in parallel")
	parser.add_argument("-c", "--chunk-size", type = int, help = "Minimum chunk size to be used when copying data. If 0, the chunk size of the input data will be used. This does not affect the chunking of the variables in the output file. (default: 0)", default = 0)
	parser.add_argument("-u", "--units", help = "Output units (optional) (default: same as input units)", default = "")
	parser.add_argument("--version", action = "version", version = "%(prog)s " + ozflux_common.VERSION)

	p = parser.parse_args(argv[1:])

	return Options(p.verbosity, p.files, p.out_file, p.show_progress
		, p.parallel, p.chunk_size, p.units)

def get_nc_datatype(fmt: str) -> str:
	if fmt == "float64":
		return FORMAT_FLOAT
	if fmt == "float32":
		return FORMAT_SINGLE
	return fmt

def init_outfile(nc_out: Dataset, infiles: list[str], chunk_size: int, units: str):
	"""
	Initialise the output file by creating dimensions and variables as required.

	@param nc_out: The output file.
	@param first_infile: The first input file. All other input files must have
	identical variables and dimensionality to this file.
	@param chunk_size: Minimum chunk size to be used when copying data.
	@param units: Desired output units (empty string means same as input units).
	"""
	if len(infiles) < 1:
		raise ValueError("No input files")

	first_infile = infiles[0]
	with open_netcdf(first_infile) as nc_in:
		log_information("Creating dimensions in output file...")
		for name in nc_in.dimensions:
			dim = nc_in.dimensions[name]
			size = dim.size
			if name == DIM_TIME:
				# Make time an unlimited dimension
				size = 0
			create_dim_if_not_exists(nc_out, name, size)
		log_information("Creating variables in output file...")
		for name in nc_in.variables:
			var = nc_in.variables[name]
			dims = var.dimensions
			fmt = get_nc_datatype(var.datatype)
			filters = var.filters()
			# Compression level
			cl = filters['complevel']

			# Compression type
			ct = get_compression_type(filters)

			# Chunk size
			chunking = var.chunking()
			cs = None if chunking == "contiguous" else tuple(chunking)
			create_var_if_not_exists(nc_out, name, fmt, dims, cl, ct, cs)
			var_out = nc_out.variables[name]
			for attr in var.ncattrs():
				if not attr[0] == "_":
					value = getattr(var, attr)
					if attr == ATTR_UNITS and len(var.dimensions) == 3 and units != "":
						value = units
					setattr(var_out, attr, value)
		lat = get_var_from_std_name(nc_in, STD_LAT)
		copy_1d(nc_in, nc_out, lat.name, chunk_size, lambda p: ...)

		lon = get_var_from_std_name(nc_in, STD_LON)
		copy_1d(nc_in, nc_out, lon.name, chunk_size, lambda p: ...)

def append_timepoints(nc_in: Dataset, nc_out: Dataset, min_chunk_size: int, pcb: Callable[[float], None]):
	"""
	Append the contents of the time variable in the input file to the time
	variable in the output file.

	@param nc_in: Input netcdf file.
	@param nc_out: Output netcdf file.
	@param min_chunk_size: Minimum chunk size to be used for moving data.
	"""
	var_in = nc_in.variables[VAR_TIME]
	var_out = nc_out.variables[VAR_TIME]

	calendar_in = getattr(var_in, ATTR_CALENDAR)
	calendar_out = getattr(var_out, ATTR_CALENDAR)

	units_in = getattr(nc_in.variables[VAR_TIME], ATTR_UNITS)
	units_out = getattr(nc_out.variables[VAR_TIME], ATTR_UNITS)

	convert_units = units_in != units_out or calendar_in != calendar_out

	check_ndims(var_in, 1)
	check_ndims(var_out, 1)

	chunk_size = var_in.chunking()[0]
	chunk_size = max(chunk_size, min_chunk_size)

	n = len(var_in)
	nexist = len(var_out)
	for i in range(0, n, chunk_size):
		upper = min(n, i + chunk_size)
		chunk = var_in[i:upper]

		# The calendar or units in the input file don't match those in the
		# output file. Therefore, we will need to convert these numeric values
		# to their equivalents in the output file.
		if convert_units:
			# Convert these numeric values to dates using the input file's
			# calendar and units.
			times = num2date(chunk, units_in, calendar_in)

			# Convert these dates to numeric values using the output file's
			# calendar and units.
			chunk = date2num(times, units_out, calendar_out)
		var_out[i + nexist:upper + nexist] = chunk
		pcb(upper / n)

def merge_time(nc_in: Dataset, nc_out: Dataset, min_chunk_size: int, pcb: Callable[[float], None]):
	# This is a special append function designed for time variables. It behaves
	# the same as the other append_* functions but can convert time units.
	append_timepoints(nc_in, nc_out, min_chunk_size, lambda p: ...)

	nvar = len([name for name in nc_in.variables if not name in nc_in.dimensions])
	start = 0
	step = 1.0 / nvar
	for name in nc_in.variables:
		if name in nc_in.dimensions:
			continue

		prog = lambda p: pcb(start + step * p)

		# todo: rename this
		append_time(nc_in, nc_out, name, min_chunk_size, prog)

		start += step
		pcb(start)

def main(opts: Options):
	"""
	Main function.

	@param opts: Parsed CLI options provided by the user..
	"""

	if len(opts.files) < 1:
		return

	# Delete output file if it already exists.
	if os.path.exists(opts.out_file):
		os.remove(opts.out_file)

	job_manager = JobManager()

	with open_netcdf(opts.out_file, write = True) as nc_out:
		init_outfile(nc_out, opts.files, opts.chunk_size, opts.units)
	# todo: order by start time ascending?

	# Each MergeTime object will open and close the output file internally.
	# Failure to do this results in a memory leak.
	for file in opts.files:
		if not os.path.exists(file):
			raise ValueError(f"File not found: {file}")
		job_manager.add_job(MergeTime(file, opts.out_file, opts.chunk_size), path.getsize(file))
	job_manager.run_single_threaded()

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(argv)

	set_log_level(opts.log_level)
	set_show_progress(opts.report_progress)

	try:
		# Actual logic is in main().
		main(opts)
		print("\nFiles merged successfully!")
	except BaseException as error:
		# Basic error handling.
		log_error(traceback.format_exc())
		exit(1)
