#!/usr/bin/env python3
#
# This script is a faster cdo mergetime command. It merges input files which
# contain the same data over different time ranges.
#
import ozflux_common
import math, shutil, traceback, datetime, pandas, subprocess

from ozflux_logging import *
from ozflux_netcdf import *
from argparse import ArgumentParser
from sys import argv
from netCDF4 import Dataset, Dimension, num2date, date2num
from typing import Callable
from ozflux_parallel import JobManager
from os import path
from functools import reduce

# Exit code used when the walltime limit is reached.
EXIT_CODE_WALLTIME_LIMIT = 42

# Name of the Job ID environment variable provided by PBS.
ENV_PBS_JOBID = "PBS_JOBID"

# Name of a command which will rerun the current job.
CMD_RERUN = "qrerun"

# The amount of overhead allocated for moving the output file from the working
# directory to the output directory when a walltime limit is provided. When this
# happens, an estimate is made of how much time is required to move the file,
# and we allow for the actual move operation to exceed the estimate by this
# factor (a fraction of the estimate).
NOVERHEAD = 4

class Options:
	"""
	Class for storing CLI arguments from the user.

	@param log: Log level.
	@param files: Input files.
	@param out: Output file.
	@param prog: True to write progress messages, 0 otherwise.
	@param parallel: True to process files in parallel.
	@param chunk_size: Minimum chunk size to be used when copying data.
	@param workdir: Working directory. Input files will be copied to this location
	before being read. Output file will be written in this location before being moved
	to the ultimate output path specified by the -o parameter.
	@param restart_file: File containing checkpointing information which allows
						 this job to be restarted from a previous run.
	@param walltime_limit: Maximum amount of walltime allowed for this job. If
						   this time limit is reached, the job will terminate
						   and may be resumed later by using the restart_file
						   parameter.
	@param compression_level: Compression levels to be used in output files. -1
							  means same as input file.
	"""
	def __init__(self, log : LogLevel, files: list[str], out: str, prog: bool,
		parallel: bool, chunk_size: int, units: str, workdir: str,
		restart_file: str, walltime_limit: datetime.timedelta,
		compression_level: int, chunk_sizes = list[tuple[str, int]]):

		self.log_level = log
		self.files = files
		self.out_file = out
		self.report_progress = prog
		self.parallel = parallel
		self.min_chunk_size = chunk_size
		self.units = units
		self.work_dir = workdir
		self.restart_file = restart_file
		self.walltime_limit = walltime_limit
		# Rate of file copying between input and working directory, in bytes per second.
		self.copy_rate: float = 0
		self.compression_level = compression_level
		self.chunk_sizes = chunk_sizes

class MergeTime:
	def __init__(self, in_file: str, out_file: str, min_chunk_size: int
			  , work_dir: str, write_restart_file: bool):
		self.in_file = in_file
		self.out_file = out_file
		self.min_chunk_size = min_chunk_size
		self.work_dir = work_dir
		self.write_restart_file = write_restart_file
	def exec(self, pcb: Callable[[float], None]):
		in_file = self.in_file
		use_work_dir = False
		if self.work_dir is not None and self.work_dir != "":
			use_work_dir = True
			copy_start = datetime.datetime.now()
			in_file = copy_workdir(in_file, self.work_dir)
			copy_duration = datetime.datetime.now() - copy_start
			self.copy_rate = path.getsize(in_file) / copy_duration.total_seconds()
		log_information("Processing '%s'..." % in_file)
		with open_netcdf(self.out_file, True) as nc_out:
			with open_netcdf(in_file) as nc_in:
				merge_time(nc_in, nc_out, self.min_chunk_size, pcb)
		if use_work_dir:
			log_diagnostic(f"Deleting temp working file {in_file}...")
			os.remove(in_file)

def parse_args(argv: list[str]) -> Options:
	"""
	Parse CLI arguments, return a parsed options object.

	@param argv: Raw CLI arguments.

	@return Parsed Options object.
	"""
	parser = ArgumentParser(prog=argv[0], description = "Formatting ozflux data into a format suitable for consumption by LPJ-Guess")
	parser.add_argument("-v", "--verbosity", type = int, help = "Logging verbosity (1-5, default 3)", nargs = "?", const = LogLevel.INFORMATION, default = LogLevel.INFORMATION)
	parser.add_argument("files", nargs = "+", help = "Input .nc files to be processed")
	parser.add_argument("-o", "--out-file", required = True, help = "Path to the output file.")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	parser.add_argument("-P", "--parallel", action = "store_true", help = "Process files in parallel")
	parser.add_argument("-m", "--min-chunk-size", type = int, default = 0, help = "Number of chunks to read at a time (along each dimension) when copying data. If 1, the chunk size of the input data will be used. This does not affect the chunking of the variables in the output file. Higher values result in higher throughput at the cost of higher memory usage (default: 1)")
	parser.add_argument("-u", "--units", help = "Output units (optional) (default: same as input units)", default = "")
	parser.add_argument("-w", "--work-dir", default = None, help = "Working directory. Input files will be copied to this location before being read. Output file will be written in this location before being moved to the ultimate output path specified by the -o/--out-file parameter.")
	parser.add_argument("-W", "--walltime-limit", default = None, help = f"Walltime limit. If this limit is reached, the job will terminate (with exit code {EXIT_CODE_WALLTIME_LIMIT}) and may be resumed later by re-running the command with the same restart file. If the job is running in PBS, it will be resubmitted via the `qrerun` command. This requires that the job is restartable (ie submitted with -ry).")
	parser.add_argument("-r", "--restart-file", default = None, help = "Path to the restart file to be used. This allows execution to resume from a previous run of ncmergetime. This requires that the same arguments are used in all invocations of the command.")
	parser.add_argument("-c", "--chunk-sizes", default = "", help = "Chunk sizes for each dimension. This is optional, and if omitted, the values in the input files will be kept. This should be specified in the same format as for nco. E.g. lat/1,lon/1,time/365")
	parser.add_argument("--compression-level", type = int, default = -1, help = "Compression level 0-9. 0 means no compression. 9 means highest compression ratio but slowest performance. Default behaviour is to use the same compression level as in the input files.")
	parser.add_argument("--version", action = "version", version = "%(prog)s " + ozflux_common.VERSION)

	p = parser.parse_args(argv[1:])

	walltime_limit: datetime.timedelta = None
	if p.walltime_limit != None:
		walltime_limit = pandas.Timedelta(p.walltime_limit).to_pytimedelta()
	chunks = parse_chunksizes(p.chunk_sizes)

	if p.compression_level > 9:
		raise ValueError(f"Compression level must be between 0-9, but was: {p.compression_level}")

	return Options(p.verbosity, p.files, p.out_file, p.show_progress
		, p.parallel, p.min_chunk_size, p.units, p.work_dir, p.restart_file
		, walltime_limit, p.compression_level, chunks)

def copy_workdir(in_file: str, work_dir: str) -> str:
	"""
	Copy the specified file into the working directory and return the path to
	the file in the working directory.

	@param in_file: The file to be copied.
	@param work_dir: The working directory.
	"""

	if not os.path.exists(work_dir):
		raise ValueError(f"Working directory {work_dir} does not exist")

	work_file = os.path.join(work_dir, os.path.basename(in_file))
	log_diagnostic(f"Copying {in_file} to {work_file}...")

	copy_start = datetime.datetime.now()
	shutil.copy2(in_file, work_file)
	copy_duration = datetime.datetime.now() - copy_start
	log_diagnostic(f"Successfully copied {in_file} in {copy_duration}")
	return work_file

def get_ntime(file: str) -> int:
	"""
	Get the length of the time dimension in the specified file.
	"""
	with open_netcdf(file) as nc:
		return nc.dimensions[DIM_TIME].size

def init_outfile(nc_out: Dataset, infiles: list[str], chunk_size: int, units: str):
	"""
	Initialise the output file by creating dimensions and variables as required.

	@param nc_out: The output file.
	@param first_infile: The first input file. All other input files must have
	identical variables and dimensionality to this file.
	@param chunk_size: Minimum chunk size to be used when copying data.
	@param units: Desired output units (empty string means same as input units).
	"""
	if len(infiles) < 1:
		raise ValueError("No input files")

	log_information("Initialising output file")

	log_diagnostic(f"Opening input files to count total number of timesteps")
	ntime = sum([get_ntime(file) for file in infiles])
	log_diagnostic(f"Input files contain {ntime} timesteps")

	first_infile = infiles[0]
	with open_netcdf(first_infile) as nc_in:
		log_diagnostic("Creating dimensions in output file...")
		for name in nc_in.dimensions:
			dim = nc_in.dimensions[name]
			size = dim.size
			if name == DIM_TIME:
				size = ntime
			create_dim_if_not_exists(nc_out, name, size)
		log_diagnostic("Creating variables in output file...")
		for name in nc_in.variables:
			var = nc_in.variables[name]
			dims = var.dimensions
			fmt = get_nc_datatype(var.datatype.__str__())
			filters = var.filters()
			# Compression level
			cl = opts.compression_level
			if cl < 0:
				cl = filters['complevel']

			# Compression type
			ct = get_compression_type(filters)

			# Chunk sizes.
			cs = None
			if len(opts.chunk_sizes) > 0:
				cs = tuple([get_chunk_size(nc_in, nc_out, dim, opts.chunk_sizes) for dim in dims])
			else:
				# Chunking from input file.
				chunking = var.chunking()
				cs = None if chunking == "contiguous" else tuple(chunking)

			create_var_if_not_exists(nc_out, name, fmt, dims, cl, ct, cs)
			var_out = nc_out.variables[name]
			for attr in var.ncattrs():
				if not attr[0] == "_":
					value = getattr(var, attr)
					if attr == ATTR_UNITS and len(var.dimensions) == 3 and units != "":
						value = units
					setattr(var_out, attr, value)

		log_diagnostic(f"Copying spatial variables into output file...")

		log_debug(f"Copying latitude")
		lat = get_var_from_std_name(nc_in, STD_LAT)
		copy_1d(nc_in, nc_out, lat.name, chunk_size, lambda p: ...)

		log_debug(f"Copying longitude")
		lon = get_var_from_std_name(nc_in, STD_LON)
		copy_1d(nc_in, nc_out, lon.name, chunk_size, lambda p: ...)

def append_timepoints(nc_in: Dataset, nc_out: Dataset, min_chunk_size: int, pcb: Callable[[float], None]):
	"""
	Append the contents of the time variable in the input file to the time
	variable in the output file.

	@param nc_in: Input netcdf file.
	@param nc_out: Output netcdf file.
	@param min_chunk_size: Minimum chunk size to be used for moving data.
	"""
	var_in = nc_in.variables[VAR_TIME]
	var_out = nc_out.variables[VAR_TIME]

	calendar_in = getattr(var_in, ATTR_CALENDAR)
	calendar_out = getattr(var_out, ATTR_CALENDAR)

	units_in = getattr(nc_in.variables[VAR_TIME], ATTR_UNITS)
	units_out = getattr(nc_out.variables[VAR_TIME], ATTR_UNITS)

	convert_units = units_in != units_out or calendar_in != calendar_out

	check_ndims(var_in, 1)
	check_ndims(var_out, 1)

	chunk_size = var_in.chunking()[0]
	chunk_size = max(chunk_size * min_chunk_size, chunk_size)

	n = len(var_in)
	nexist = get_nexist(var_out)
	log_diagnostic(f"nexist = {nexist}")
	for i in range(0, n, chunk_size):
		upper = min(n, i + chunk_size)
		chunk = var_in[i:upper]

		# The calendar or units in the input file don't match those in the
		# output file. Therefore, we will need to convert these numeric values
		# to their equivalents in the output file.
		if convert_units:
			# Convert these numeric values to dates using the input file's
			# calendar and units.
			times = num2date(chunk, units_in, calendar_in)

			# Convert these dates to numeric values using the output file's
			# calendar and units.
			chunk = date2num(times, units_out, calendar_out)
		var_out[i + nexist:upper + nexist] = chunk
		pcb(upper / n)

def get_weight(nc: Dataset, name: str) -> int:
	"""
	Get the total 'weight' of a particular variable in a NetCDF file, where
	weight is defined as the product of the size of the variable's dimensions.

	@param name: Name of the variable.
	"""
	var = nc.variables[name]
	sizes = [nc.dimensions[dim].size for dim in var.dimensions]
	return reduce(lambda x, y: x * y, sizes, 1)

def merge_time(nc_in: Dataset, nc_out: Dataset, min_chunk_size: int, pcb: Callable[[float], None]):
	# This is a special append function designed for time variables. It behaves
	# the same as the other append_* functions but can convert time units.
	append_timepoints(nc_in, nc_out, min_chunk_size, lambda p: ...)

	vars = [name for name in nc_in.variables if not name in nc_in.dimensions]
	weights = [get_weight(nc_in, var) for var in vars]
	total_weight = sum(weights)
	start = 0
	for name in nc_in.variables:
		if name in nc_in.dimensions:
			continue
		step = get_weight(nc_in, name) / total_weight
		prog = lambda p: pcb(start + step * p)

		# todo: rename this
		append_time(nc_in, nc_out, name, min_chunk_size, prog)

		start += step
		pcb(start)

def parse_restart_file(restart_file: str) -> list[str]:
	"""
	Parse the restart file, and return a list of file names which have already
	been processed.
	"""
	# TODO: this doesn't support file names containing newlines!
	with open(restart_file) as file:
		return [line.strip("\n") for line in file.readlines()]

def walltime_abort():
	"""
	Abort the job due to reaching the walltime limit. This will resubmit the
	current PBS job if running in PBS.
	"""
	# Get PBS job ID.
	job_id = os.getenv(ENV_PBS_JOBID)

	# If this environment variable is set, we are probably running in PBS.
	if job_id != None and job_id != "":
		# Resubmit PBS job.
		log_information(f"We appear to be running in PBS with job ID {job_id}")

		# TODO: check if qrerun is on PATH?? Is this provided by other PBS
		# implementations?
		cmd = [CMD_RERUN, job_id]

		# TODO: capture and print the job ID of the new job?
		log_information(f"Executing {str.join(" ", cmd)}...")

		# This will throw if the command fails.
		subprocess.run(cmd, check = True)
		log_information(f"Successfully resubmitted job!")
	else:
		log_information("This does not appear to be running in PBS. The caller will need to handle resubmission of the job.")

	# Exit with the standard exit code.
	log_warning(f"Early termination due to walltime limit of {opts.walltime_limit} being reached.")
	sys.exit(EXIT_CODE_WALLTIME_LIMIT)

def main(opts: Options):
	"""
	Main function.

	@param opts: Parsed CLI options provided by the user..
	"""

	if len(opts.files) < 1:
		return

	if opts.walltime_limit is not None and opts.restart_file is None:
		raise ValueError(f"Walltime limit is set, but no restart file is being used. This doesn't really make sense - what do you plan to do with the resultant incomplete file?")

	out_file = opts.out_file

	use_work_dir = False
	if opts.work_dir is not None and opts.work_dir != "":
		if not os.path.exists(opts.work_dir):
			raise ValueError(f"Working directory {opts.work_dir} does not exist")
		use_work_dir = True
		out_file = os.path.join(opts.work_dir, os.path.basename(opts.out_file))
		log_diagnostic(f"Intermediate output file will be used: {out_file}")

	# True to resume from restart file. False otherwise.
	restart = False
	write_restart = False
	restart_files: list[str] = []
	if opts.restart_file is not None:
		log_diagnostic(f"Restart file will be written: '{opts.restart_file}'")
		write_restart = True
		if os.path.exists(opts.restart_file):
			log_diagnostic(f"Restart file will be read: '{opts.restart_file}'")
			restart = True
			restart_files = parse_restart_file(opts.restart_file)
			log_diagnostic(f"{len(restart_files)} files have already been processed")
			if use_work_dir and not os.path.exists(out_file) and os.path.exists(opts.out_file):
				log_diagnostic(f"Copying existing output file into working directory in order to resume from where we left off")
				shutil.copy2(opts.out_file, out_file)

	# Delete output file if it already exists.
	if os.path.exists(out_file) and not restart:
		log_diagnostic(f"Deleting existing output file '{out_file}'")
		os.remove(out_file)

	if not restart:
		with open_netcdf(out_file, write = True) as nc_out:
			init_outfile(nc_out, opts.files, opts.min_chunk_size, opts.units)
	# todo: order by start time ascending?

	init_walltime()

	# Each MergeTime object will open and close the output file internally.
	# Failure to do this results in a memory leak.
	in_files = [file for file in opts.files if not file in restart_files]
	step_start = 0
	total_weight = sum(path.getsize(file) for file in in_files)
	copy_rates: list[float] = []
	iteration_durations: list[float] = []
	for file in in_files:
		if not os.path.exists(file):
			raise ValueError(f"File not found: {file}")
		step_size = path.getsize(file) / total_weight
		job = MergeTime(file, out_file, opts.min_chunk_size, opts.work_dir, write_restart)
		iteration_start = datetime.datetime.now()
		job.exec(lambda p: log_progress(step_start + step_size * p))
		iteration_durations.append(datetime.datetime.now() - iteration_start)
		step_start += step_size

		# Append to restart file.
		if write_restart:
			with open(opts.restart_file, "a") as rfile:
				rfile.writelines([file, '\n'])

		more_files = file != in_files[len(in_files) - 1]
		if use_work_dir and opts.walltime_limit is not None and more_files:
			# Record the speed at which this job's input file was copied into
			# the working directory (in bytes per second).
			copy_rates.append(job.copy_rate)

			# Get the mean rate of copyin files from the input to the working
			# directory (bytes per second).
			mean_copy_rate = sum(copy_rates) / len(copy_rates)

			# Estimated the time required to copy the output file back to the
			# output directory.
			out_size = path.getsize(out_file)
			log_diagnostic(f"Output file size is currently {out_size}")
			copy_time = out_size / mean_copy_rate

			# Create a timedelta object to store this required time.
			est_outfile_copy_time = datetime.timedelta(seconds = copy_time)

			# Get the time remaining for this job.
			time_remaining = opts.walltime_limit - get_walltime()

			log_diagnostic(f"Mean copy rate is {mean_copy_rate} B/s")
			log_diagnostic(f"Estimated copy time for output file is  {est_outfile_copy_time}")
			log_diagnostic(f"Remaining time is {time_remaining}")

			# Check if there is enough time remaining to copy the output file
			# back into the output directory from the working directory.
			if time_remaining < NOVERHEAD * est_outfile_copy_time:
				log_warning(f"Copying output file to output directory. Estimated time required: {est_outfile_copy_time}. Time remaining: {time_remaining}")
				copy_start = datetime.datetime.now()

				# Copy the output file back to the output directory from the
				# working directory.
				shutil.move(out_file, opts.out_file)

				seconds = sum([x.total_seconds() for x in iteration_durations]) / len(iteration_durations)
				mean_iteration_duration = datetime.timedelta(seconds = seconds)
				log_diagnostic(f"Elapsed time is {get_walltime()} and estimated time required for another iteration is {mean_iteration_duration}")

				copy_duration = datetime.datetime.now() - copy_start
				log_information(f"Output file successfully moved to output directory in {copy_duration}. Exiting job...")
				walltime_abort()
		elif opts.walltime_limit is not None:
			# Mean duration of each iteration.
			seconds = sum([x.total_seconds() for x in iteration_durations]) / len(iteration_durations)
			mean_iteration_duration = datetime.timedelta(seconds = seconds)

			# Time remaining to the job.
			time_remaining = opts.walltime_limit - get_walltime()

			# If not enough time remaining, and more files must be appended,
			# terminate early.
			if time_remaining < NOVERHEAD * mean_iteration_duration and more_files:
				log_diagnostic(f"Elapsed time is {get_walltime()} and estimated time required for another iteration is {mean_iteration_duration}")
				walltime_abort()

	if use_work_dir:
		log_information(f"Moving temporary output file {out_file} to actual output path: {opts.out_file}")
		move_start = datetime.datetime.now()
		shutil.move(out_file, opts.out_file)
		move_duration = datetime.datetime.now() - move_start
		log_diagnostic(f"Successfully moved output file in {move_duration}")

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(argv)

	set_log_level(opts.log_level)
	set_show_progress(opts.report_progress)

	try:
		# Actual logic is in main().
		main(opts)
		print("\nFiles merged successfully!")
		print(f"Total duration: {get_walltime()}")
	except Exception as error:
		# Basic error handling.
		log_error(traceback.format_exc())
		exit(1)
