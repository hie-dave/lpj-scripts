#!/usr/bin/env python
#
# This script will read a netcdf file, duplicate the first year's data
# N times, and prepend the duplicate data to the file.
#

import abc, argparse, ozflux_logging, math, numpy, sys, traceback, warnings
from typing import Callable
from netCDF4 import Dataset, Variable

# Treat warnings as exceptions (as numpy is a bit trigger-happy).
warnings.simplefilter('error', UserWarning)

# leap years are a myth...actually, lpj-guess doesn't account for leap
# years, so we won't either.
DAYS_PER_YEAR = 365
MINUTES_PER_DAY = 24 * 60
VARIABLE_NAME = "forcing_data"
NUM_VARIABLES = 11
PROGRESS_INTERVAL = 512
CHUNK_SIZE = 65536

class DuplicateOptions:
	"""
	Class for storing CLI options.
	"""
	def __init__(self, log: ozflux_logging.LogLevel, files: list[str]
		, nyear: int, show_progress: bool):
		self.log_level = log
		self.files = files
		self.nyears = nyear
		self.show_progress = show_progress

class IWeatherGenerator(abc.ABC):
	"""
	Interface for a weather generator.
	"""
	@abc.abstractmethod
	def __init__(self, data: list[float], year_len: int):
		"""
		Initialise the weather generator.
		@param data: The base data to use for generation.
		@param year_len: Number of elements (timesteps) per year.
		"""
		...

	@abc.abstractmethod
	def generate_weather(self, nyear: int
		, progress_cb: Callable[[float], None]) -> list[float]:
		"""
		Generate N years' weather data.
		@param nyear: Number of years' data to generate. Output array
					  length should be nyear * year_len.
		"""
		...

class DuplicateGenerator(IWeatherGenerator):
	"""
	Weather generator implementation which just repeatedly duplicates
	the first years' data.
	"""
	def __init__(self, data: list[float], year_len: int):
		"""
		See base class doco for usage instructions.
		"""
		self.data = data
		self.year_len = year_len

	def generate_weather(self, nyear: int
		, progress_cb: Callable[[float], None]) -> list[float]:
		"""
		See base class doco for usage instructions.
		"""
		first_years_data = self.data[:self.year_len]
		return first_years_data * nyear

def is_masked(arr, i) -> bool:
	if not numpy.ma.is_masked(arr):
		return False
	return arr.mask[i]

class GaussianGenerator(IWeatherGenerator):
	"""
	Weather generator implementation which generates data from a normal
	distribution of sampled data.
	"""
	def __init__(self, data: list[float], year_len: int):
		"""
		See base class doco for usage instructions.
		"""
		self.year_len = year_len
		# No need to store the raw data (only store means/SDs).

		# Create an array in which each element is an array of all
		# values for that particular day of year, across all years.
		self.timesteps = [[] for _ in range(year_len)]
		for i in range(len(data)):
			doy = i % year_len
			if not is_masked(data, i) and not math.isnan(data[i]):
				self.timesteps[doy].append(data[i])
			else:
				# In principle this is not a problem, and we can
				# interpolate around this later if there is no data for
				# this doy, but in practice it may not be sensible to
				# use this generator for very sparse data.
				m = "Row %d contains a masked value"
				ozflux_logging.log_diagnostic(m % i)

		# Get mean/sd by day of year.
		self.means = [0.0] * year_len
		self.sds = [0.0] * year_len
		for i in range(year_len):
			timestep = self.timesteps[i]
			# If dataset is small, you really shouldn't be using this
			# generator. But we can try sampling nearby timesteps.
			if len(timestep) == 0:
				m = "No data available for timestep %d. Neighbouring timesteps will be sampled."
				ozflux_logging.log_warning(m % i)
				timestep = self.neighbouring_data(data, i, 2)

			# Calculate mean and standard deviation for this day of
			# year, across all years.
			self.means[i] = numpy.mean(timestep)
			self.sds[i] = numpy.std(timestep)

	def generate_weather(self, nyear: int
		, progress_cb: Callable[[float], None]) -> list[float]:
		"""
		See base class doco for usage instructions.
		"""
		n = self.year_len * nyear
		samples = [0.0] * n

		# Sample N times for each day of year.
		for i in range(self.year_len):
			sample = numpy.random.normal(self.means[i], self.sds[i], nyear)
			for j in range(nyear):
				index = i + self.year_len * j
				samples[index] = sample[j]
			if i % PROGRESS_INTERVAL == 0:
				progress_cb(i / self.year_len)

		return samples

	def neighbouring_indices(self, index, ndata, nindex) -> list[int]:
		"""
		Return N closest neighbouring indices for the given index.
		@param index: The start point.
		@param ndata: Length of the list.
		@param nindex: Number of indices to return.
		"""
		if nindex > ndata:
			raise IndexError("Attempted to interpolate with more values than exist in list")

		indices = [0] * nindex
		for i in range(nindex):
			sign = 1 if i % 2 == 0 else -1
			dlt = 1 + i // 2 * sign # 1, -1, 2, -2, ...
			idx = index + dlt

			# Given the error check at start of function, we should
			# eventualy find another neighbour. There is probably a
			# more efficient search algorithm than this, but this will
			# be called very rarely and I have bigger fish to fry.
			while idx < 0 or idx >= ndata or idx in indices:
				idx += dlt

			indices[i] = idx
		return indices

	def neighbouring_data(self, data: list[float], index: int, n: int) -> list[float]:
		"""
		Return data from neighbouring indices in a flat list.
		"""
		indices = self.neighbouring_indices(index, len(data), n)
		data: list[float]
		neighbours = []
		for i in indices:
			neighbours.extend(data[i])
		return neighbours

def prepend(data: list[list[float]], nc: Dataset):
	"""
	Prepend the 2d array to the specified .nc file.

	@param data: A list of list of floats. Each sub list should be the
	time series data for a single variable.
	"""
	if len(data) < 1:
		# Technically this is not a problem per se, but it's probably a
		# good indication that something has gone wrong.
		ozflux_logging.log_warning("Attempted to prepend empty array. Probably a bug.")
		return

	# The input data array is a list of lists, where each internal list
	# represents time series for 1 variable. Therefore, we need to
	# prepend the transpose of this array to the .nc file.
	return numpy.insert(
		nc.variables[VARIABLE_NAME],
		0,
		numpy.transpose(data),
		axis = 0
	)

def get_timesteps_per_year(nc_file: Dataset) -> int:
	"""
	Read the .nc file metadata and determine the number of timesteps per
	variable per year.
	"""
	timestep = int(nc_file.time_step) # in minutes
	timesteps_per_day = MINUTES_PER_DAY / timestep
	return int(timesteps_per_day * DAYS_PER_YEAR)

import time, gc

def generate_weather(in_file: str, nyear: int
	, progress_callback: Callable[[float], None]):
	"""
	Generate N years' data and prepend the result to the .nc file.
	"""
	ozflux_logging.log_information("----- Processing %s -----" % in_file)
	with Dataset(in_file, "r+", format="NETCDF4") as nc:
		# Count # of timesteps per year. This assumes the .nc file has
		# a global attribute called "time_step" which is in minutes.
		timesteps_per_year = get_timesteps_per_year(nc)

		# Proportion of time spent reading vs writing. Used for progress
		# reporting.
		write_weight = 7.5e-4 * nyear
		generate_weight = 0.75 * (1 - write_weight)
		read_weight = 1 - write_weight - generate_weight

		ozflux_logging.log_information("Reading...")
		read_start = time.time()
		generators: list[IWeatherGenerator]
		generators = []
		for i in range(NUM_VARIABLES):
			# Get i-th column, and create a weather generator instance
			# for this column.
			data = nc[VARIABLE_NAME][:, i]
			generators.append(GaussianGenerator(data, timesteps_per_year))
			progress_callback(read_weight * (i + 1) / NUM_VARIABLES)

		read_time = (time.time() - read_start) * 1000

		ozflux_logging.log_information("Generating...")
		generating_start = time.time()
		data = [ [] ] * NUM_VARIABLES
		step_size = generate_weight / NUM_VARIABLES

		# Generate data for each variable one by one. Each individual
		# weather generator will return a flat list of data, and each of
		# these flat lists are stored in the data list, which is a list
		# of lists (ie 2d list).
		for i in range(NUM_VARIABLES):
			step_start = read_weight + i * step_size
			data[i] = generators[i].generate_weather(nyear
			, lambda p: progress_callback(step_start + p * step_size))
		generating_time = (time.time() - generating_start) * 1000

		ozflux_logging.log_information("Writing...")
		data = prepend(data, nc)

		# Force garbage collection to free up some memory.
		gc.collect()

		write_start = time.time()
		nrow = data.shape[0]
		step_size = write_weight / nrow
		# Write data in chunks, to reduce memory usage (but increase
		# execution time). To make this faster, increase CHUNK_SIZE.
		for i in range(0, nrow, CHUNK_SIZE):
			step_start = read_weight + generate_weight
			upper = min(nrow, i + CHUNK_SIZE)
			nc.variables[VARIABLE_NAME][i:upper, :] = data[i:upper, :]
			if i % PROGRESS_INTERVAL == 0:
				progress_callback(step_start + step_size * (i + 1))
		write_time = (time.time() - write_start) * 1000

		total_time = read_time + generating_time + write_time
		read_weight = read_time / total_time
		read_perc = read_weight * 100
		write_weight = write_time / total_time
		write_perc = write_weight * 100
		gen_prop = generating_time / total_time
		gen_perc = gen_prop * 100
		ozflux_logging.log_diagnostic("Read : %.2fms/%.2f/%.2f%%" % (read_time, read_weight, read_perc))
		ozflux_logging.log_diagnostic("Write: %.2fms/%.2f/%.2f%%" % (write_time, write_weight, write_perc))
		ozflux_logging.log_diagnostic("Gener: %.2fms/%.2f/%.2f%%" % (generating_time, gen_prop, gen_perc))

def main(opts: DuplicateOptions):
	"""
	Main function; creates the benchmark configs.
	"""
	step_size = 1 / len(opts.files)
	for i in range(len(opts.files)):
		step_start = i / len(opts.files)
		generate_weather(opts.files[i], opts.nyears
			, lambda p: ozflux_logging.log_progress(step_start + step_size * p))

def parse_args(args: list[str]) -> DuplicateOptions:
	"""
	Parse CLI arguments.
	"""
	parser = argparse.ArgumentParser(args[0], description = "Generate met data in a netcdf file.")
	parser.add_argument("-v", "--verbosity", type = int, nargs = "?", default = ozflux_logging.LogLevel.INFORMATION, help = "Logging verbosity (1-5, default 3)")
	parser.add_argument("files", nargs = "+", help = "Input .nc files to be processed")
	parser.add_argument("-n", "--num-years", type = int, nargs = "?", default = "1", help = "Number of years of data to generate")
	parser.add_argument("-p", "--show-progress", action = "store_true", help = "Report progress")
	result = parser.parse_args(args[1:])
	return DuplicateOptions(result.verbosity, result.files
		, result.num_years, result.show_progress)

if __name__ == "__main__":
	# Parse CLI args
	opts = parse_args(sys.argv)

	# todo: rethink logging
	ozflux_logging.set_log_level(opts.log_level)
	ozflux_logging.set_show_progress(opts.show_progress)

	try:
		# Actual logic is in main().
		main(opts)
	except BaseException as error:
		# Basic error handling.
		ozflux_logging.log_error(traceback.format_exc())
		exit(1)


